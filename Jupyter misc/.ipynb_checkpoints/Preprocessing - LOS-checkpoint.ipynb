{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25fce576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.24.3\n",
      "Pandas version: 1.5.3\n",
      "Scikit version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "# External libraries for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "#To render graphs within notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib \n",
    "import os\n",
    "\n",
    "# Versions of libraries\n",
    "print(\"Numpy version: {}\".format(np.__version__))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Scikit version: {}\".format(sk.__version__))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d433a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Project/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5fb61",
   "metadata": {},
   "source": [
    "### Target variable calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93bc2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS based on admissions table (target dataframe)\n",
    "\n",
    "file = \"hosp/admissions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_admissions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f417b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions['dischtime'] = pd.to_datetime(df_admissions['dischtime'], format='%d/%m/%Y %H:%M')\n",
    "df_admissions['admittime'] = pd.to_datetime(df_admissions['admittime'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc4411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_los_hadm = pd.DataFrame()\n",
    "df_los_subject = pd.DataFrame()\n",
    "\n",
    "df_los_subject['subject_id'] = df_admissions['subject_id']\n",
    "df_los_hadm['hadm_id'] = df_admissions['hadm_id']\n",
    "df_los_hadm['los'] = df_admissions['dischtime']-df_admissions['admittime']\n",
    "df_los_subject['los'] = df_admissions['dischtime']-df_admissions['admittime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acfe74cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24181354</td>\n",
       "      <td>8 days 23:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25926192</td>\n",
       "      <td>7 days 20:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23983182</td>\n",
       "      <td>5 days 17:33:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22942076</td>\n",
       "      <td>1 days 17:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21606243</td>\n",
       "      <td>2 days 02:11:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>24745425</td>\n",
       "      <td>5 days 15:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>22168393</td>\n",
       "      <td>4 days 12:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>27708593</td>\n",
       "      <td>7 days 07:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>23251352</td>\n",
       "      <td>4 days 04:56:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>28108313</td>\n",
       "      <td>2 days 16:10:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hadm_id             los\n",
       "0    24181354 8 days 23:24:00\n",
       "1    25926192 7 days 20:12:00\n",
       "2    23983182 5 days 17:33:00\n",
       "3    22942076 1 days 17:41:00\n",
       "4    21606243 2 days 02:11:00\n",
       "..        ...             ...\n",
       "270  24745425 5 days 15:57:00\n",
       "271  22168393 4 days 12:18:00\n",
       "272  27708593 7 days 07:10:00\n",
       "273  23251352 4 days 04:56:00\n",
       "274  28108313 2 days 16:10:00\n",
       "\n",
       "[275 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_los_hadm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "534edc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average LOS for each subject_id\n",
    "df_los_subject = df_los_subject.groupby('subject_id').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6241c35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>1 days 10:40:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001217</td>\n",
       "      <td>6 days 08:30:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001725</td>\n",
       "      <td>2 days 23:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002428</td>\n",
       "      <td>5 days 14:54:34.285714285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10002495</td>\n",
       "      <td>6 days 21:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>10038999</td>\n",
       "      <td>9 days 02:41:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>10039708</td>\n",
       "      <td>7 days 11:56:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10039831</td>\n",
       "      <td>5 days 07:19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10039997</td>\n",
       "      <td>2 days 07:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10040025</td>\n",
       "      <td>6 days 14:04:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_id                       los\n",
       "0     10000032           1 days 10:40:00\n",
       "1     10001217           6 days 08:30:30\n",
       "2     10001725           2 days 23:52:00\n",
       "3     10002428 5 days 14:54:34.285714285\n",
       "4     10002495           6 days 21:24:00\n",
       "..         ...                       ...\n",
       "95    10038999           9 days 02:41:30\n",
       "96    10039708           7 days 11:56:06\n",
       "97    10039831           5 days 07:19:00\n",
       "98    10039997           2 days 07:47:00\n",
       "99    10040025           6 days 14:04:54\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_los_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "384fc5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admittime= pd.DataFrame()\n",
    "df_admittime['hadm_id'] = df_admissions['hadm_id']\n",
    "df_admittime['admittime'] = df_admissions['admittime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d48f1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_admittime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24687da7",
   "metadata": {},
   "source": [
    "### omr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcf442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional to do\n",
    "# Make a chartdate - admittime feature called days_since_admission \n",
    "# - Can't because hadm not provided here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f361d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/omr.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_omr = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c7f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine result_name and seq_num into the column name with result_value from the same row as its value \n",
    "\n",
    "# Function to combine values from columns into a new column \n",
    "def new_columns(row):\n",
    "    return row['result_name'] + '_' + str(row['seq_num'])\n",
    "\n",
    "new_names = df_omr.apply(new_columns, axis=1) # series of names of combinations \n",
    "\n",
    "\n",
    "def add_values(row, colName):\n",
    "    name = row['result_name'] + '_' + str(row['seq_num'])\n",
    "    if str(name) == colName:\n",
    "        return row['result_value']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "for i in range(len(new_names)):\n",
    "    df_omr[new_names[i]] = df_omr.apply(add_values, args=(new_names[i],), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e82dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop seq_num, result_name, result_value\n",
    "df_omr = df_omr.drop(columns=['seq_num', 'result_name', 'result_value'])\n",
    "# sequence number doesn't add any useful info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr['subject_id'].value_counts()\n",
    "\n",
    "# The patient with the most measurements has 391 so could make it 391 features for everyone but most will have lots of \n",
    "# zeroes\n",
    "# Fine as sparcity represents not taking many measurements which could also be a factor?\n",
    "# Could have number of measurements as an additional feature too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_omr[df_omr['subject_id'] == 10019003]\n",
    "filtered_df\n",
    "\n",
    "# Preserves every measurement made for each subject across all of their stays \n",
    "# Only one entry per row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = df_omr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d10073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering by date (so each patients measurements are chronological from top to bottom)\n",
    "\n",
    "df_omr = df_omr.sort_values(by=['subject_id', 'chartdate'])\n",
    "\n",
    "df_omr\n",
    "\n",
    "# This preserves for example, increase in weight over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74982da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop chartdate since the time shift is not consistent for each subject \n",
    "df_omr = df_omr.drop(columns=['chartdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_omr = df_omr.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_omr.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00573659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af367f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row for each subject, features for every measurement made on them \n",
    "\n",
    "colNames = df_omr.columns.tolist()\n",
    "colNames.remove('subject_id')\n",
    "\n",
    "x = 0\n",
    "prev_subject = 0\n",
    "\n",
    "\n",
    "for row in range(len(df_omr)):\n",
    "    current_subject = df_omr['subject_id'][row] \n",
    "    if current_subject != prev_subject:\n",
    "        x = 0 # reset x\n",
    "    for i in range(len(colNames)): # for each column\n",
    "        if df_omr.loc[row, colNames[i]] != 0:\n",
    "            if colNames[i] + '_0' not in df_omr_final.columns: # New column name added\n",
    "                x = 0 # reset x\n",
    "            new_name = colNames[i] + '_' + str(x)\n",
    "            if new_name in df_omr_final.columns and (current_subject == prev_subject): # Trying to add another of the same \n",
    "                # measurement for the same patient \n",
    "                x += 1\n",
    "                new_name = colNames[i] + '_' + str(x)\n",
    "            df_omr_final.loc[current_subject, new_name] = df_omr.loc[row, colNames[i]]\n",
    "            df_omr_final = df_omr_final.copy()\n",
    "            break # leave for loop as the rest of the columns will be 0 for this row\n",
    "    prev_subject = current_subject\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16107b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr_final.fillna(0, inplace=True)\n",
    "df_omr_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9fa323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df_omr_final.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all values to numbers \n",
    "\n",
    "df_omr_final = df_omr_final.astype(str)\n",
    "\n",
    "# Function to convert fraction string to decimal\n",
    "def fraction_to_decimal(fraction_str):\n",
    "    try:\n",
    "        numerator, denominator = map(int, fraction_str.split('/'))\n",
    "        return numerator / denominator\n",
    "    except ValueError:\n",
    "        return fraction_str  # Return unchanged if not a fraction\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "df_omr_final = df_omr_final.applymap(fraction_to_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr_final = df_omr_final.astype(float)\n",
    "# df_omr_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15327a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and convert it to a column\n",
    "df_omr_final.reset_index(inplace=True)\n",
    "df_omr_final.rename(columns={'index': 'subject_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames based on the ID column\n",
    "df_omr_final = df_omr_final.merge(df_los_subject, on='subject_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this show?\n",
    "# Each patient (subject_id is the index of the df) has measurements showing type_sequence_date\n",
    "# sequence starts from 1 and it is used when the same measurement was taken more than once in a single day\n",
    "# date starts from 0 and is used when the same measurement for the same patient was taken on a different day\n",
    "# Note that they were NOT taken on the same date for each patient but the bigger the date integer, the later the measurement\n",
    "# was taken, relative to that patient's admission  \n",
    "\n",
    "# Weight (Lbs)_1_0 is the first time the patient was weighed, Weight (Lbs)_3_0 is the third time they were weighed on that \n",
    "# same day as they were first weighed\n",
    "# Weight (Lbs)_1_1 is from a separate (later) date where the patient was weighed again, this is the first measurement \n",
    "# from this day \n",
    "# Any non applicable measurements are imputed with 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957f782",
   "metadata": {},
   "source": [
    "Decide which ones to keep all measurements of per patient and which to just take the average and keep as one record for patient (that aren’t likely to change):\n",
    "\n",
    "Remove blood pressure sitting, lying and standing as too few samples \n",
    "Take average for height \n",
    "\n",
    "Could probably drop a few of the features that are really empty ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1990ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop subject_id\n",
    "df_omr_final = df_omr_final.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr_final # target variable los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb358aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "# df_omr_final.to_csv('df_omr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeeac1c",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe8c28",
   "metadata": {},
   "source": [
    "Given a patients omr data (grouped by subject_id), predict their length of stay (take an average for multiple admissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcfd340",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_omr_final.drop(columns=['los'])\n",
    "target = df_omr_final['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "omr_data_train, omr_data_test, omr_label_train, omr_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", omr_data_train.shape, omr_label_train.shape)\n",
    "print(\"Testing set shape:\", omr_data_test.shape, omr_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaeb090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "# omr_data_train.to_csv('omr_data_train.csv', index=False)\n",
    "# omr_data_test.to_csv('omr_data_test.csv', index=False)\n",
    "\n",
    "# omr_label_train.to_csv('omr_label_train.csv', index=False)\n",
    "# omr_label_test.to_csv('omr_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de435ec",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31d6e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/jenni/OneDrive/Desktop/IP/\"\n",
    "file = \"omr_data_train.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "omr_data_train = pd.read_csv(full_path)\n",
    "\n",
    "file = \"omr_data_test.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "omr_data_test = pd.read_csv(full_path)\n",
    "\n",
    "file = \"omr_label_train.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "omr_label_train = pd.read_csv(full_path)\n",
    "\n",
    "file = \"omr_label_test.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "omr_label_test = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78555092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BMI (kg/m2)_1_0</th>\n",
       "      <th>Weight (Lbs)_1_0</th>\n",
       "      <th>Blood Pressure_1_0</th>\n",
       "      <th>BMI (kg/m2)_1_1</th>\n",
       "      <th>Weight (Lbs)_1_1</th>\n",
       "      <th>Height (Inches)_1_0</th>\n",
       "      <th>Weight (Lbs)_17_0</th>\n",
       "      <th>Weight (Lbs)_5_0</th>\n",
       "      <th>Weight (Lbs)_10_0</th>\n",
       "      <th>Weight (Lbs)_12_0</th>\n",
       "      <th>...</th>\n",
       "      <th>Weight (Lbs)_14_1</th>\n",
       "      <th>Weight (Lbs)_20_0</th>\n",
       "      <th>Weight (Lbs)_27_0</th>\n",
       "      <th>Weight (Lbs)_23_0</th>\n",
       "      <th>Weight (Lbs)_19_0</th>\n",
       "      <th>Weight (Lbs)_24_0</th>\n",
       "      <th>Weight (Lbs)_16_1</th>\n",
       "      <th>Weight (Lbs)_11_1</th>\n",
       "      <th>Weight (Lbs)_25_0</th>\n",
       "      <th>Weight (Lbs)_22_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.00</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>30.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 1007 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    BMI (kg/m2)_1_0  Weight (Lbs)_1_0  Blood Pressure_1_0  BMI (kg/m2)_1_1  \\\n",
       "0               0.0               0.0            0.000000             25.6   \n",
       "1               0.0             168.0            0.000000             25.5   \n",
       "2               0.0             167.0            0.000000             25.8   \n",
       "3               0.0               0.0            2.038462              0.0   \n",
       "4               0.0             138.0            0.000000              0.0   \n",
       "..              ...               ...                 ...              ...   \n",
       "58              0.0               0.0            0.000000             25.9   \n",
       "59              0.0               0.0            0.000000              0.0   \n",
       "60              0.0               0.0            0.000000             33.3   \n",
       "61              0.0               0.0            0.000000             30.5   \n",
       "62             30.9               0.0            0.000000              0.0   \n",
       "\n",
       "    Weight (Lbs)_1_1  Height (Inches)_1_0  Weight (Lbs)_17_0  \\\n",
       "0               0.00                 62.0                0.0   \n",
       "1               0.00                  0.0                0.0   \n",
       "2               0.00                  0.0                0.0   \n",
       "3               0.00                  0.0                0.0   \n",
       "4               0.00                  0.0                0.0   \n",
       "..               ...                  ...                ...   \n",
       "58              0.00                 73.0                0.0   \n",
       "59            165.00                 64.0                0.0   \n",
       "60              0.00                 62.0                0.0   \n",
       "61              0.00                 68.0                0.0   \n",
       "62            185.63                  0.0                0.0   \n",
       "\n",
       "    Weight (Lbs)_5_0  Weight (Lbs)_10_0  Weight (Lbs)_12_0  ...  \\\n",
       "0                0.0                0.0                0.0  ...   \n",
       "1                0.0                0.0                0.0  ...   \n",
       "2                0.0                0.0                0.0  ...   \n",
       "3                0.0                0.0                0.0  ...   \n",
       "4                0.0                0.0                0.0  ...   \n",
       "..               ...                ...                ...  ...   \n",
       "58               0.0                0.0                0.0  ...   \n",
       "59               0.0                0.0                0.0  ...   \n",
       "60               0.0                0.0                0.0  ...   \n",
       "61               0.0                0.0                0.0  ...   \n",
       "62               0.0                0.0                0.0  ...   \n",
       "\n",
       "    Weight (Lbs)_14_1  Weight (Lbs)_20_0  Weight (Lbs)_27_0  \\\n",
       "0                 0.0                0.0                0.0   \n",
       "1                 0.0                0.0                0.0   \n",
       "2                 0.0                0.0                0.0   \n",
       "3                 0.0                0.0                0.0   \n",
       "4                 0.0                0.0                0.0   \n",
       "..                ...                ...                ...   \n",
       "58                0.0                0.0                0.0   \n",
       "59                0.0                0.0                0.0   \n",
       "60                0.0                0.0                0.0   \n",
       "61                0.0                0.0                0.0   \n",
       "62                0.0                0.0                0.0   \n",
       "\n",
       "    Weight (Lbs)_23_0  Weight (Lbs)_19_0  Weight (Lbs)_24_0  \\\n",
       "0                 0.0                0.0                0.0   \n",
       "1                 0.0                0.0                0.0   \n",
       "2                 0.0                0.0                0.0   \n",
       "3                 0.0                0.0                0.0   \n",
       "4                 0.0                0.0                0.0   \n",
       "..                ...                ...                ...   \n",
       "58                0.0                0.0                0.0   \n",
       "59                0.0                0.0                0.0   \n",
       "60                0.0                0.0                0.0   \n",
       "61                0.0                0.0                0.0   \n",
       "62                0.0                0.0                0.0   \n",
       "\n",
       "    Weight (Lbs)_16_1  Weight (Lbs)_11_1  Weight (Lbs)_25_0  Weight (Lbs)_22_0  \n",
       "0                 0.0                0.0                0.0                0.0  \n",
       "1                 0.0                0.0                0.0                0.0  \n",
       "2                 0.0                0.0                0.0                0.0  \n",
       "3                 0.0                0.0                0.0                0.0  \n",
       "4                 0.0                0.0                0.0                0.0  \n",
       "..                ...                ...                ...                ...  \n",
       "58                0.0                0.0                0.0                0.0  \n",
       "59                0.0                0.0                0.0                0.0  \n",
       "60                0.0                0.0                0.0                0.0  \n",
       "61                0.0                0.0                0.0                0.0  \n",
       "62                0.0                0.0                0.0                0.0  \n",
       "\n",
       "[63 rows x 1007 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omr_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce to 12 features, this is an especially sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707dd8c",
   "metadata": {},
   "source": [
    "#### Truncated Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5dbc74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explained Variance Ratio:\n",
      "[0.33342246 0.10604019 0.08112751 0.04986414 0.0442775  0.03792258\n",
      " 0.03462321 0.03055395 0.02590118 0.02647111 0.02490924 0.02347857]\n",
      "\n",
      " Amount of original variance conserved: 0.8185916541599938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 12\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(omr_data_train)\n",
    "transformed_matrix = svd.transform(omr_data_train)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de9296",
   "metadata": {},
   "source": [
    "#### Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa142665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 12\n",
    "\n",
    "# Initialize NMF with the desired number of components\n",
    "nmf = NMF(n_components=n_components)\n",
    "\n",
    "# Fit the NMF model to the sparse matrix and transform the data\n",
    "W = nmf.fit_transform(omr_data_train)\n",
    "H = nmf.components_\n",
    "\n",
    "# Reconstruct the original matrix\n",
    "reconstructed_matrix = np.dot(W, H)\n",
    "\n",
    "# Print the factorized matrices (W and H) and the reconstructed matrix\n",
    "# print(\"Factorized Matrix W:\")\n",
    "# print(W)\n",
    "# print(\"\\nFactorized Matrix H:\")\n",
    "# print(H)\n",
    "# print(\"\\nReconstructed Matrix:\")\n",
    "# print(reconstructed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4fecad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Error (Frobenius norm): 2111.202976270996\n",
      "Original Matrix Norm (Frobenius norm): 4965.416241595012\n",
      "Percentage of Variance Conserved: 57.48185300991351\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Frobenius norm of the reconstruction error\n",
    "# which measures the element-wise difference between the original and reconstructed matrices\n",
    "reconstruction_error = np.linalg.norm(omr_data_train - reconstructed_matrix, 'fro')\n",
    "\n",
    "# Calculate the Frobenius norm of the original matrix\n",
    "original_norm = np.linalg.norm(omr_data_train, 'fro')\n",
    "\n",
    "# Calculate the percentage of variance conserved\n",
    "variance_conserved = (1 - reconstruction_error / original_norm) * 100\n",
    "\n",
    "print(\"Reconstruction Error (Frobenius norm):\", reconstruction_error)\n",
    "print(\"Original Matrix Norm (Frobenius norm):\", original_norm)\n",
    "print(\"Percentage of Variance Conserved:\", variance_conserved)\n",
    "\n",
    "# By evaluating the reconstruction error and the percentage of variance conserved, you can assess how well the NMF \n",
    "# factorization captures the structure of the original data. \n",
    "# Lower reconstruction error and higher percentage of variance conserved indicate a better quality of factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7ac18",
   "metadata": {},
   "source": [
    "#### Sparse PCA - takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f871d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "# Number of desired components\n",
    "n_components = 12\n",
    "\n",
    "# Initialize Sparse PCA with the desired number of components and sparsity level\n",
    "sparse_pca = SparsePCA(n_components=n_components, alpha=0.1)  # Adjust alpha as needed\n",
    "\n",
    "# Fit Sparse PCA to the sparse matrix and transform the data\n",
    "sparse_pca.fit(omr_data_train)\n",
    "transformed_matrix = sparse_pca.transform(omr_data_train)\n",
    "\n",
    "# Get the loading matrix (sparse components)\n",
    "loading_matrix = sparse_pca.components_\n",
    "\n",
    "# Print the transformed matrix and loading matrix\n",
    "print(\"Transformed Matrix:\")\n",
    "print(transformed_matrix)\n",
    "print(\"\\nLoading Matrix (Sparse Components):\")\n",
    "print(loading_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2d9a9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio of Component 1: 0.0000\n",
      "Explained Variance Ratio of Component 2: 0.0000\n",
      "Explained Variance Ratio of Component 3: 0.0000\n",
      "Explained Variance Ratio of Component 4: 0.0000\n",
      "Explained Variance Ratio of Component 5: 0.0000\n",
      "Explained Variance Ratio of Component 6: 0.0000\n",
      "Explained Variance Ratio of Component 7: 0.0000\n",
      "Explained Variance Ratio of Component 8: 0.0000\n",
      "Explained Variance Ratio of Component 9: 0.0000\n",
      "Explained Variance Ratio of Component 10: 0.0000\n",
      "Explained Variance Ratio of Component 11: 0.0000\n",
      "Explained Variance Ratio of Component 12: 0.0000\n",
      "\n",
      "Total Explained Variance Ratio: 4.867096141886648e-07\n"
     ]
    }
   ],
   "source": [
    "# Compute the squared norms of the components\n",
    "squared_norms = np.linalg.norm(loading_matrix, axis=1)**2\n",
    "\n",
    "# Calculate the proportion of explained variance for each component\n",
    "total_squared_norm = np.linalg.norm(omr_data_train)**2\n",
    "explained_variance_ratio = squared_norms / total_squared_norm\n",
    "\n",
    "# Print the explained variance ratio of each component\n",
    "for i, evr in enumerate(explained_variance_ratio):\n",
    "    print(f\"Explained Variance Ratio of Component {i + 1}: {evr:.4f}\")\n",
    "\n",
    "# Calculate the total explained variance ratio\n",
    "total_explained_variance_ratio = np.sum(explained_variance_ratio)\n",
    "print(\"\\nTotal Explained Variance Ratio:\", total_explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f70abe",
   "metadata": {},
   "source": [
    "#### Random Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0f61ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 12\n",
    "\n",
    "# Initialize Random Projection with the desired number of components\n",
    "random_projection = SparseRandomProjection(n_components=n_components)\n",
    "\n",
    "# Fit Random Projection to the sparse matrix and transform the data\n",
    "transformed_matrix = random_projection.fit_transform(omr_data_train)\n",
    "\n",
    "# Print the transformed matrix\n",
    "print(\"Transformed Matrix:\")\n",
    "print(transformed_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12668c0",
   "metadata": {},
   "source": [
    "Assess the quality of the dimensionality reduction performed by Random Projection by examining the pairwise distances between points in the original and projected spaces. A well-preserved variance will result in similar pairwise distances between points in both spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f1d601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Distortion: 287.1814575402386\n",
      "Average Distortion: 107.38740883522641\n"
     ]
    }
   ],
   "source": [
    "# Calculate the pairwise distances in the original space\n",
    "original_distances = np.linalg.norm(omr_data_train - omr_data_train.mean(axis=0), axis=1)\n",
    "\n",
    "# Calculate the pairwise distances in the projected space\n",
    "projected_distances = np.linalg.norm(transformed_matrix - transformed_matrix.mean(axis=0), axis=1)\n",
    "\n",
    "# Compute the distortion in pairwise distances\n",
    "distortion = np.abs(original_distances - projected_distances)\n",
    "\n",
    "# Calculate the maximum and average distortion\n",
    "max_distortion = np.max(distortion)\n",
    "average_distortion = np.mean(distortion)\n",
    "\n",
    "print(\"Maximum Distortion:\", max_distortion)\n",
    "print(\"Average Distortion:\", average_distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f96948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High distortion values suggest low preservation of variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "acd38e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
      "     ---------------------------------------- 0.0/90.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 90.9/90.9 kB 5.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from umap-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from umap-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from umap-learn) (1.3.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from umap-learn) (0.57.0)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Obtaining dependency information for pynndescent>=0.5 from https://files.pythonhosted.org/packages/4e/82/0b9851a2fd4da9b57d7931446f5ebab92a98f1f35d3dc0dae5f9ed50a462/pynndescent-0.5.11-py3-none-any.whl.metadata\n",
      "  Downloading pynndescent-0.5.11-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from umap-learn) (4.65.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn) (0.40.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tqdm->umap-learn) (0.4.6)\n",
      "Downloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 55.8/55.8 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: umap-learn\n",
      "  Building wheel for umap-learn (setup.py): started\n",
      "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86931 sha256=ad7e6226a5bbe5f7b61957262e3e28f264eb5d1632cb9fd63876afeb7c297c9e\n",
      "  Stored in directory: c:\\users\\jenni\\appdata\\local\\pip\\cache\\wheels\\de\\07\\2e\\814a6ee82e37528f2044a609a431028375b149bc31f03c0e27\n",
      "Successfully built umap-learn\n",
      "Installing collected packages: pynndescent, umap-learn\n",
      "Successfully installed pynndescent-0.5.11 umap-learn-0.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab4d878",
   "metadata": {},
   "source": [
    "#### Uniform Manifold Approximation and Projection (UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac3635fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'umap' has no attribute 'UMAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize UMAP with desired parameters\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m umap_model \u001b[38;5;241m=\u001b[39m umap\u001b[38;5;241m.\u001b[39mUMAP(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Fit UMAP to the data and transform it to the lower-dimensional space\u001b[39;00m\n\u001b[0;32m      8\u001b[0m umap_result \u001b[38;5;241m=\u001b[39m umap_model\u001b[38;5;241m.\u001b[39mfit_transform(omr_data_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'umap' has no attribute 'UMAP'"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize UMAP with desired parameters\n",
    "umap_model = umap.UMAP(n_neighbors=5, min_dist=0.3, n_components=2)\n",
    "\n",
    "# Fit UMAP to the data and transform it to the lower-dimensional space\n",
    "umap_result = umap_model.fit_transform(omr_data_train)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(umap_result[:, 0], umap_result[:, 1], c='b', marker='o')\n",
    "plt.title('UMAP Projection of Data')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c987849c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap\n",
      "  Downloading umap-0.1.1.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: umap\n",
      "  Building wheel for umap (setup.py): started\n",
      "  Building wheel for umap (setup.py): finished with status 'done'\n",
      "  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3550 sha256=d03b2e81ba4d6136e2a1a8575fb42f4e267be78ad66cb2494e6328c50f7c6bff\n",
      "  Stored in directory: c:\\users\\jenni\\appdata\\local\\pip\\cache\\wheels\\82\\d8\\73\\e9eb3334baaad795ff0278363ff1aca7568bdf2793e452a527\n",
      "Successfully built umap\n",
      "Installing collected packages: umap\n",
      "Successfully installed umap-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608348c3",
   "metadata": {},
   "source": [
    "### admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/admissions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_admissions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bb2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_admissions['subject_id'].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions.head() # admittime and dischtime for los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60fb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ea7b666",
   "metadata": {},
   "source": [
    "To drop: subject_id, admittime, dischtime, deathtime, hospital_expire_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an ed_duration feature for edouttime - edregtime (how long the patient stayed in the emergency department)\n",
    "\n",
    "# Convert to datetime\n",
    "df_admissions['edouttime'] = pd.to_datetime(df_admissions['edouttime'], format='%d/%m/%Y %H:%M')\n",
    "df_admissions['edregtime'] = pd.to_datetime(df_admissions['edregtime'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "df_admissions['ed_duration'] = df_admissions['edouttime'] - df_admissions['edregtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_admissions['ed_duration'] = df_admissions['ed_duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.drop(columns=['subject_id', 'admittime', 'dischtime', 'deathtime', 'hospital_expire_flag'\n",
    "                            , 'edregtime', 'edouttime', 'admit_provider_id','discharge_location'])\n",
    "\n",
    "# discharge_location is an outcome feature, should not be used to predict LOS as not known beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null with N/A and then one hot encode\n",
    "df_admissions['marital_status'] = df_admissions['marital_status'].fillna('N/A')\n",
    "df_admissions = pd.get_dummies(df_admissions, columns=['admission_type', 'admission_location', \n",
    "                                                      'insurance','language', 'marital_status','race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d837a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037928c9",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e42f01",
   "metadata": {},
   "source": [
    "Given patient data given at time of admission, predict their length of stay (dischtime - admittime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_admissions = df_admissions.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c793dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "# df_admissions.to_csv('df_admissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd1052",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d506b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_admissions.drop(columns=['los'])\n",
    "target = df_admissions['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "admissions_data_train, admissions_data_test, admissions_label_train, admissions_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", admissions_data_train.shape, admissions_label_train.shape)\n",
    "print(\"Testing set shape:\", admissions_data_test.shape, admissions_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa131f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "admissions_data_train.to_csv('admissions_data_train.csv', index=False)\n",
    "admissions_data_test.to_csv('admissions_data_test.csv', index=False)\n",
    "\n",
    "admissions_label_train.to_csv('admissions_label_train.csv', index=False)\n",
    "admissions_label_test.to_csv('admissions_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71208a89",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d829cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4828e569",
   "metadata": {},
   "source": [
    "### diagnoses - Do not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca392d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnoses are recorded upon discharge so don't use to predice LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21751d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/diagnoses_icd.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_diagnoses = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnoses.info()\n",
    "# Note that Icd_code and icd_version together relate to a particular diagnosis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnoses = df_diagnoses.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnoses = pd.get_dummies(df_diagnoses, columns=['icd_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a225671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnoses #Currently has too many features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4b0ca",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a18cb4",
   "metadata": {},
   "source": [
    "Given information about a billed diagnosis for a patient, predict their length of stay (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnoses = df_diagnoses.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_diagnoses = df_diagnoses.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "# df_diagnoses.to_csv('df_diagnoses.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958122b5",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514e2d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data = df_diagnoses.drop(columns=['los'])\n",
    "# target = df_diagnoses['los']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# diagnoses_data_train, diagnoses_data_test, diagnoses_label_train, diagnoses_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Print the shapes of the resulting training and testing sets\n",
    "# print(\"Training set shape:\", diagnoses_data_train.shape, diagnoses_label_train.shape)\n",
    "# print(\"Testing set shape:\", diagnoses_data_test.shape, diagnoses_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358097b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "# diagnoses_data_train.to_csv('diagnoses_data_train.csv', index=False)\n",
    "# diagnoses_data_test.to_csv('diagnoses_data_test.csv', index=False)\n",
    "\n",
    "# diagnoses_label_train.to_csv('diagnoses_label_train.csv', index=False)\n",
    "# diagnoses_label_test.to_csv('diagnoses_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d6953",
   "metadata": {},
   "source": [
    "### emar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddfd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records for 65 different patients \n",
    "# 181 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76008ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/emar.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_emar = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23cf656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar.info()\n",
    "# print(df_emar.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_emar['emar_seq'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10bca4",
   "metadata": {},
   "source": [
    "Impute with N/A and encode: enter_provider_id, medication\n",
    "\n",
    "Drop: subject_id, emar_id, poe_id, pharmacy_id, event_txt, storetime\n",
    "\n",
    "poe_id is an identifier which links administrations in emar to orders in poe and prescriptions\n",
    "storetime is when it was recorded in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature called delay using scheduletime - charttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_emar['scheduletime'] = pd.to_datetime(df_emar['scheduletime'], format='%Y/%m/%d %H:%M')\n",
    "df_emar['charttime'] = pd.to_datetime(df_emar['charttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_emar['delay'] = df_emar['charttime'] - df_emar['scheduletime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_emar['delay'] = df_emar['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9419c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar = df_emar.drop(columns=['subject_id','emar_id','poe_id','pharmacy_id',\n",
    "                               'event_txt','charttime','scheduletime','storetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efc81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null with N/A and then one hot encode\n",
    "df_emar['enter_provider_id'] = df_emar['enter_provider_id'].fillna('N/A')\n",
    "df_emar['medication'] = df_emar['medication'].fillna('N/A')\n",
    "df_emar = pd.get_dummies(df_emar, columns=['enter_provider_id', 'medication'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21e664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a774a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar['delay'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae59aa3",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f01fed",
   "metadata": {},
   "source": [
    "Relationship between particular medications, uniquely given by emar_id (and some other info regarding it) and length of stay (change hadm_id to LOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar = df_emar.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_emar = df_emar.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4749216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "df_emar.to_csv('df_emar.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebb578",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9a7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_emar.drop(columns=['los'])\n",
    "target = df_emar['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "emar_data_train, emar_data_test, emar_label_train, emar_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", emar_data_train.shape, emar_label_train.shape)\n",
    "print(\"Testing set shape:\", emar_data_test.shape, emar_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c51cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "emar_data_train.to_csv('emar_data_train.csv', index=False)\n",
    "emar_data_test.to_csv('emar_data_test.csv', index=False)\n",
    "\n",
    "emar_label_train.to_csv('emar_label_train.csv', index=False)\n",
    "emar_label_test.to_csv('emar_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1b3a5",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1db930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ccf94",
   "metadata": {},
   "source": [
    "### emar_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c981640",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/emar_detail.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_emar_detail = pd.read_csv(full_path,low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bea463",
   "metadata": {},
   "source": [
    "Fields that have lots of null values:\n",
    "reason_for_no_barcode: drop\n",
    "prior_infusion_rate: impute with zeroes\n",
    "infusion_rate: impute with zeroes\n",
    "infusion_rate_adjustment: impute with 'N/A', then one hot encoding\n",
    "infusion_rate_adjustment_amount: impute with zeroes\n",
    "infusion_rate_unit: impute with 'N/A', then one hot encoding\n",
    "infusion_complete: impute with 'N/A', then one hot encoding\n",
    "completion_interval: impute with 0, then ordinal encoding \n",
    "new_iv_bag_hung: impute with N, then binary encoding \n",
    "\n",
    "Text data to remove but maybe consider later:\n",
    "product_description, product_description_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.drop(columns=['reason_for_no_barcode']) # Too hard to encode, adds not much value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0s\n",
    "df_emar_detail['prior_infusion_rate'] = df_emar_detail['prior_infusion_rate'].fillna(0)\n",
    "df_emar_detail['infusion_rate'] = df_emar_detail['infusion_rate'].fillna(0)\n",
    "df_emar_detail['infusion_rate_adjustment_amount'] = df_emar_detail['infusion_rate_adjustment_amount'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_emar_detail['infusion_rate_adjustment'] = df_emar_detail['infusion_rate_adjustment'].fillna('N/A')\n",
    "df_emar_detail['infusion_rate_unit'] = df_emar_detail['infusion_rate_unit'].fillna('N/A')\n",
    "df_emar_detail['infusion_complete'] = df_emar_detail['infusion_complete'].fillna('N/A')\n",
    "df_emar_detail = pd.get_dummies(df_emar_detail, columns=['infusion_rate_adjustment','infusion_complete',\n",
    "                                                         'infusion_rate_unit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].fillna(0)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('PRN', 0)\n",
    "#Converting all the intervals to minutes\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 2 hours', 120)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 4 hours', 240)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 1 hour', 60)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 1.5 hours', 90)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 8 hours', 480)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 15 minutes', 15)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 12 hours', 720)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 30 minutes', 30)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 24 hours', 1140)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 1 minutes', 1)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 14 hours', 840)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 7 hours', 420)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 5 hours', 300)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 3 hours', 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ae7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['new_iv_bag_hung'] = df_emar_detail['new_iv_bag_hung'].fillna('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding\n",
    "df_emar_detail['new_iv_bag_hung'] = df_emar_detail['new_iv_bag_hung'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and one hot encode:\n",
    "# administration_type\n",
    "# barcode_type\n",
    "# complete_dose_not_given\n",
    "# dose_due_unit\n",
    "# dose_given_unit\n",
    "# will_remainder_of_dose_be_given\n",
    "# product_unit\n",
    "# product_code\n",
    "# route\n",
    "# side\n",
    "# site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['administration_type'] = df_emar_detail['administration_type'].fillna('N/A')\n",
    "df_emar_detail['barcode_type'] = df_emar_detail['barcode_type'].fillna('N/A')\n",
    "df_emar_detail['complete_dose_not_given'] = df_emar_detail['complete_dose_not_given'].fillna('N/A')\n",
    "df_emar_detail['dose_due_unit'] = df_emar_detail['dose_due_unit'].fillna('N/A')\n",
    "df_emar_detail['dose_given_unit'] = df_emar_detail['dose_given_unit'].fillna('N/A')\n",
    "df_emar_detail['will_remainder_of_dose_be_given'] = df_emar_detail['will_remainder_of_dose_be_given'].fillna('N/A')\n",
    "df_emar_detail['product_unit'] = df_emar_detail['product_unit'].fillna('N/A')\n",
    "df_emar_detail['product_code'] = df_emar_detail['product_code'].fillna('N/A')\n",
    "df_emar_detail['route'] = df_emar_detail['route'].fillna('N/A')\n",
    "df_emar_detail['side'] = df_emar_detail['side'].fillna('N/A')\n",
    "df_emar_detail['site'] = df_emar_detail['site'].fillna('N/A')\n",
    "df_emar_detail = pd.get_dummies(df_emar_detail, columns=['administration_type','barcode_type','complete_dose_not_given',\n",
    "                                                        'dose_due_unit','dose_given_unit',\n",
    "                                                        'will_remainder_of_dose_be_given','product_unit','product_code',\n",
    "                                                        'route','side','site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with zeroes:\n",
    "# dose_due and dose_given, but also need to deal with some of them being ranges\n",
    "# product_amount_given\n",
    "# restart_interval, then ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7594bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['product_amount_given'] = df_emar_detail['product_amount_given'].fillna(0)\n",
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].fillna(0)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].fillna(0)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a488171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].astype(str)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_middle_value(range_string):\n",
    "    if '-' in range_string:\n",
    "        start, end = map(float, range_string.split('-'))\n",
    "        return (start + end) / 2\n",
    "    else:\n",
    "        return range_string\n",
    "\n",
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].apply(find_middle_value)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].apply(find_middle_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('PRN', 0)\n",
    "#Converting all the intervals to minutes\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 2 hours', 120)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 4 hours', 240)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 1 hour', 60)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 30 minutes', 30)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 24 hours', 1140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N and map to binary encoding:\n",
    "# continued_infusion_in_other_location\n",
    "# non_formulary_visual_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['continued_infusion_in_other_location'] = df_emar_detail['continued_infusion_in_other_location'].fillna('N')\n",
    "df_emar_detail['non_formulary_visual_verification'] = df_emar_detail['non_formulary_visual_verification'].fillna('N')\n",
    "# Binary encoding\n",
    "df_emar_detail['continued_infusion_in_other_location'] = df_emar_detail['continued_infusion_in_other_location'].map({'Y': 1, 'N': 0})\n",
    "df_emar_detail['non_formulary_visual_verification'] = df_emar_detail['non_formulary_visual_verification'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.drop(columns=['pharmacy_id']) # Contains NaN values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c80f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.drop(columns=['emar_id']) # Practically unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01348878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace blanks with zero\n",
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].replace('___', 0)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].replace('___', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].astype(float)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b06e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A or 0\n",
    "# One hot encode the categorical features \n",
    "\n",
    "df_emar_detail['product_description'] = df_emar_detail['product_description'].fillna('N/A')\n",
    "df_emar_detail['product_description_other'] = df_emar_detail['product_description_other'].fillna('N/A')\n",
    "df_emar_detail['parent_field_ordinal'] = df_emar_detail['parent_field_ordinal'].fillna(0)\n",
    "df_emar_detail = pd.get_dummies(df_emar_detail, columns=['product_description_other','product_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb6ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70b37b",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943ead0",
   "metadata": {},
   "source": [
    "Given information about an administered medication, predict LOS (use subject_id mapped to average LOS frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a82ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.merge(df_los_subject, on='subject_id', how='left')\n",
    "df_emar_detail = df_emar_detail.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "# df_emar_detail.to_csv('df_emar_detail.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a629de",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748caaf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_emar_detail.drop(columns=['los'])\n",
    "target = df_emar_detail['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "emar_detail_data_train, emar_detail_data_test, emar_detail_label_train, emar_detail_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", emar_detail_data_train.shape, emar_detail_label_train.shape)\n",
    "print(\"Testing set shape:\", emar_detail_data_test.shape, emar_detail_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69723ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "# emar_detail_data_train.to_csv('emar_detail_data_train.csv', index=False)\n",
    "# emar_detail_data_test.to_csv('emar_detail_data_test.csv', index=False)\n",
    "\n",
    "# emar_detail_label_train.to_csv('emar_detail_label_train.csv', index=False)\n",
    "# emar_detail_label_test.to_csv('emar_detail_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c39217",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b11516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658b118",
   "metadata": {},
   "source": [
    "### hcpcsevents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c18d3e",
   "metadata": {},
   "source": [
    "Contains info for 18 different patients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_hcpcs has longer descriptions (connected by code) but no other useful info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/hcpcsevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_hcpcsevents = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient, admission, date, uniquely identifying billed code, sequence number, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents['seq_num'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794be5d",
   "metadata": {},
   "source": [
    "To drop: subject_id, chartdate, hcpcs_cd (code that links to longer description in d_hcpcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49faa7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature for days_since_admission using chartdate - admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_hcpcsevents['chartdate'] = pd.to_datetime(df_hcpcsevents['chartdate'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_hcpcsevents = df_hcpcsevents.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# Discard the time part and keep only the date\n",
    "df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_hcpcsevents['days_since_admission'] = df_hcpcsevents['chartdate'] - df_hcpcsevents['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_hcpcsevents['days_since_admission'] = df_hcpcsevents['days_since_admission'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents['days_since_admission'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = df_hcpcsevents.drop(columns=['subject_id','chartdate','hcpcs_cd'])\n",
    "# Not enough samples to include code as after encoding there would be a lot more features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae82538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = pd.get_dummies(df_hcpcsevents, columns=['short_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a339e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1fa82",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a331035",
   "metadata": {},
   "source": [
    "Given (brief) information about billed events, predict length of stay (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = df_hcpcsevents.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_hcpcsevents = df_hcpcsevents.drop(columns=['hadm_id', 'admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83517ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hcpcsevents.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "# df_hcpcsevents.to_csv('df_hcpcsevents.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5f1e1",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d107fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_hcpcsevents.drop(columns=['los'])\n",
    "target = df_hcpcsevents['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "hcpcsevents_data_train, hcpcsevents_data_test, hcpcsevents_label_train, hcpcsevents_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", hcpcsevents_data_train.shape, hcpcsevents_label_train.shape)\n",
    "print(\"Testing set shape:\", hcpcsevents_data_test.shape, hcpcsevents_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ffce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "hcpcsevents_data_train.to_csv('hcpcsevents_data_train.csv', index=False)\n",
    "hcpcsevents_data_test.to_csv('hcpcsevents_data_test.csv', index=False)\n",
    "\n",
    "hcpcsevents_label_train.to_csv('hcpcsevents_label_train.csv', index=False)\n",
    "hcpcsevents_label_test.to_csv('hcpcsevents_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f5c81",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 13 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/jenni/OneDrive/Desktop/IP/\"\n",
    "file = \"hcpcsevents_data_train.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "hcpcsevents_data_train = pd.read_csv(full_path)\n",
    "\n",
    "file = \"hcpcsevents_data_test.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "hcpcsevents_data_test = pd.read_csv(full_path)\n",
    "\n",
    "file = \"hcpcsevents_label_train.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "hcpcsevents_label_train = pd.read_csv(full_path)\n",
    "\n",
    "file = \"hcpcsevents_label_test.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "hcpcsevents_label_test = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8dd936",
   "metadata": {},
   "source": [
    "### labevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information regarding 252 different admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39087c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/labevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_labevents = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c203d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_labevents['hadm_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad57ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents['value'] = pd.to_numeric(df_labevents['value'], errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca829786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents['storetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60785952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature for days_since_admission using charttime - admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_labevents['charttime'] = pd.to_datetime(df_labevents['charttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_labevents = df_labevents.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_labevents['days_since_admission'] = df_labevents['charttime'] - df_labevents['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_labevents['days_since_admission'] = df_labevents['days_since_admission'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069686c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add storetime - charttime feature called delay\n",
    "\n",
    "# Convert to datetime\n",
    "df_labevents['storetime'] = pd.to_datetime(df_labevents['storetime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_labevents['delay'] = df_labevents['storetime'] - df_labevents['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_labevents['delay'] = df_labevents['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ecf60",
   "metadata": {},
   "source": [
    "Drop: labevent_id, subject_id, order_provider_id (too many Null), charttime, storetime, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03052b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = df_labevents.drop(columns=['labevent_id','subject_id','order_provider_id','charttime','storetime','comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For flag make abnormal = 1 and fill Null with 0\n",
    "df_labevents['flag'] = df_labevents['flag'].fillna(0)\n",
    "df_labevents['flag'] = df_labevents['flag'].replace('abnormal', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For priority fill Null with N/A and then one hot encode\n",
    "df_labevents['priority'] = df_labevents['priority'].fillna('N/A')\n",
    "df_labevents = pd.get_dummies(df_labevents, columns=['priority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7430ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = pd.get_dummies(df_labevents, columns=['valueuom','specimen_id','itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6dcf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with null values \n",
    "df_labevents = df_labevents.dropna()\n",
    "# Reduced from 107727 rows to 66660"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c4989",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076e71a",
   "metadata": {},
   "source": [
    "Given information about a laboratory event (from a patient specimen) - These include haematology measurements, blood gases, chemistry panels, and less common tests such as genetic assays.\n",
    "Predict that patient's eventual LOS (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = df_labevents.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_labevents = df_labevents.drop(columns=['hadm_id', 'admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1cbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_labevents.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "df_labevents.to_csv('df_labevents.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a6e2f",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c3e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_labevents.drop(columns=['los'])\n",
    "target = df_labevents['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "labevents_data_train, labevents_data_test, labevents_label_train, labevents_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", labevents_data_train.shape, labevents_label_train.shape)\n",
    "print(\"Testing set shape:\", labevents_data_test.shape, labevents_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3742458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "labevents_data_train.to_csv('labevents_data_train.csv', index=False)\n",
    "labevents_data_test.to_csv('labevents_data_test.csv', index=False)\n",
    "\n",
    "labevents_label_train.to_csv('labevents_label_train.csv', index=False)\n",
    "labevents_label_test.to_csv('labevents_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd54b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labevents_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac93c86",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39543660",
   "metadata": {},
   "source": [
    "### microbiologyevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/microbiologyevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_microbio = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_microbio['micro_specimen_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed71851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_microbio['comments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461feda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make days_since_admission using charttime \n",
    "\n",
    "# Convert to datetime\n",
    "df_microbio['charttime'] = pd.to_datetime(df_microbio['charttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_microbio = df_microbio.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_microbio['days_since_admission'] = df_microbio['charttime'] - df_microbio['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_microbio['days_since_admission'] = df_microbio['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_microbio = df_microbio.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3710c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add storetime - charttime feature (call it delay)\n",
    "\n",
    "# Convert to datetime\n",
    "df_microbio['storetime'] = pd.to_datetime(df_microbio['storetime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_microbio['delay'] = df_microbio['storetime'] - df_microbio['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_microbio['delay'] = df_microbio['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffde6ec",
   "metadata": {},
   "source": [
    "Drop: microevent_id, subject_id, chartdate, charttime, test_seq, storedate, storetime, test_name and org_itemid (since info in name), quantity, ab_name, comments, micro_specimen_id (unique identifier for sample as some measurements are made on the same sample)\n",
    "Keep but categorical: order_provider_id, spec_type_desc, dilution_text, dilution_comparison\n",
    "Impute null with 0: order_provider_id, org_itemid, isolate_num, ab_itemid, dilution_value\n",
    "Impute with N/A and then one hot encode: interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b50d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "# spec_itemid , test_itemid\n",
    "df_microbio = df_microbio.drop(columns=['microevent_id','subject_id','chartdate','charttime','test_seq','storedate',\n",
    "                                       'storetime','quantity','comments','ab_itemid',\n",
    "                                       'spec_itemid','test_itemid','org_itemid','micro_specimen_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33adb9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute null with 0: order_provider_id, org_itemid, isolate_num, ab_itemid, dilution_value\n",
    "df_microbio['order_provider_id'] = df_microbio['order_provider_id'].fillna(0)\n",
    "df_microbio['isolate_num'] = df_microbio['isolate_num'].fillna(0)\n",
    "df_microbio['dilution_value'] = df_microbio['dilution_value'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f78ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and then one hot encode: interpretation\n",
    "# encode test_name, ab_name\n",
    "\n",
    "df_microbio['interpretation'] = df_microbio['interpretation'].fillna('N/A')\n",
    "df_microbio['test_name'] = df_microbio['test_name'].fillna('N/A')\n",
    "df_microbio['ab_name'] = df_microbio['ab_name'].fillna('N/A')\n",
    "df_microbio['org_name'] = df_microbio['org_name'].fillna('None')\n",
    "df_microbio = pd.get_dummies(df_microbio, columns=['org_name','interpretation','ab_name','test_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f754dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep but categorical: order_provider_id, spec_type_desc, dilution_text, dilution_comparison\n",
    "df_microbio = pd.get_dummies(df_microbio, columns=['order_provider_id','spec_type_desc','dilution_text',\n",
    "                                                  'dilution_comparison'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c759138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio = df_microbio.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc578f",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24215179",
   "metadata": {},
   "source": [
    "Given information about a microbiology measurement, predict that patient's eventual LOS (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio = df_microbio.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_microbio = df_microbio.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "# df_microbio.to_csv('df_microbio.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a963751",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654c85e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_microbio.drop(columns=['los'])\n",
    "target = df_microbio['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "microbio_data_train, microbio_data_test, microbio_label_train, microbio_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", microbio_data_train.shape, microbio_label_train.shape)\n",
    "print(\"Testing set shape:\", microbio_data_test.shape, microbio_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "# microbio_data_train.to_csv('microbio_data_train.csv', index=False)\n",
    "# microbio_data_test.to_csv('microbio_data_test.csv', index=False)\n",
    "\n",
    "# microbio_label_train.to_csv('microbio_label_train.csv', index=False)\n",
    "# microbio_label_test.to_csv('microbio_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbio_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b50bbd",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be580bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb8cdd",
   "metadata": {},
   "source": [
    "### patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/patients.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_patients = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9922481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients['anchor_age'].value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12203d8a",
   "metadata": {},
   "source": [
    "Drop: anchor_year\n",
    "Encode: gender (M to 0 and F to 1), dod (change all to 1 and nulls to 0)\n",
    "Dummies: anchor_year_group  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7220553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "df_patients = df_patients.drop(columns=['anchor_year','dod']) \n",
    "# Since this is the shifted year and dod is an outcome value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode: gender (M to 0 and F to 1), dod (change all to 1 and nulls to 0)\n",
    "df_patients['gender'] = df_patients['gender'].replace('M', 0)\n",
    "df_patients['gender'] = df_patients['gender'].replace('F', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies: anchor_year_group  \n",
    "df_patients = pd.get_dummies(df_patients, columns=['anchor_year_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e77a07",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f4b58",
   "metadata": {},
   "source": [
    "Based on the patient's gender, age and whether their year group was either 2011-2013 or 2014-2016, predict LOS (take an average based on subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3069492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = df_patients.merge(df_los_subject, on='subject_id', how='left')\n",
    "df_patients = df_patients.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "df_patients.to_csv('df_patients.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042beba",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d674b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_patients.drop(columns=['los'])\n",
    "target = df_patients['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "patients_data_train, patients_data_test, patients_label_train, patients_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", patients_data_train.shape, patients_label_train.shape)\n",
    "print(\"Testing set shape:\", patients_data_test.shape, patients_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "patients_data_train.to_csv('patients_data_train.csv', index=False)\n",
    "patients_data_test.to_csv('patients_data_test.csv', index=False)\n",
    "\n",
    "patients_label_train.to_csv('patients_label_train.csv', index=False)\n",
    "patients_label_test.to_csv('patients_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab5e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patients_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5371472",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f419664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ad2dd",
   "metadata": {},
   "source": [
    "### pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/pharmacy.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_pharmacy = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pharmacy.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cfed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_columns = df_pharmacy.select_dtypes(include=['object'])\n",
    "# print(object_columns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178eef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pharmacy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648db89",
   "metadata": {},
   "source": [
    "drop: subject_id, pharmacy_id, poe_id, starttime, stoptime, entertime, verifiedtime, disp_sched, basal_rate, one_hr_max,\n",
    "expirationdate, fill_quantity\n",
    "Encode: proc_type, status\n",
    "Impute with N/A and encode: infusion_type, sliding_scale, duration_interval, expiration_unit, dispensation, medication, route, frequency\n",
    "Impute with 0: lockout_interval, doses_per_24_hrs, duration, expiration_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoptime-starttime for a duration feature\n",
    "\n",
    "# Convert to datetime\n",
    "df_pharmacy['stoptime'] = pd.to_datetime(df_pharmacy['stoptime'], format='%Y/%m/%d %H:%M')\n",
    "df_pharmacy['starttime'] = pd.to_datetime(df_pharmacy['starttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "\n",
    "df_pharmacy['medication_duration'] = df_pharmacy['stoptime'] - df_pharmacy['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_pharmacy['medication_duration'] = df_pharmacy['medication_duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifiedtime - entertime for verification_delay feature \n",
    "\n",
    "# Convert to datetime\n",
    "df_pharmacy['verifiedtime'] = pd.to_datetime(df_pharmacy['verifiedtime'], format='%Y/%m/%d %H:%M')\n",
    "df_pharmacy['entertime'] = pd.to_datetime(df_pharmacy['entertime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_pharmacy['verification_delay'] = df_pharmacy['verifiedtime'] - df_pharmacy['entertime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_pharmacy['verification_delay'] = df_pharmacy['verification_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_value = [0] \n",
    "\n",
    "# Fill null values with the list\n",
    "df_pharmacy['disp_sched'] = df_pharmacy['disp_sched'].fillna(pd.Series([fill_value]*len(df_pharmacy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f594f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all categories to strings\n",
    "df_pharmacy['disp_sched'] = df_pharmacy['disp_sched'].apply(lambda x: [str(item) for item in x])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "encoded_feature = pd.DataFrame(mlb.fit_transform(df_pharmacy['disp_sched']),\n",
    "                               columns=mlb.classes_,\n",
    "                               index=df_pharmacy.index)\n",
    "\n",
    "df_pharmacy = pd.concat([df_pharmacy, encoded_feature], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ccba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_pharmacy = df_pharmacy.drop(columns=['subject_id','pharmacy_id','poe_id','starttime','stoptime','entertime',\n",
    "                                       'verifiedtime','expirationdate', 'fill_quantity','disp_sched'])\n",
    "# expiration date and fill quantity are all empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode: proc_type, status\n",
    "df_pharmacy = pd.get_dummies(df_pharmacy, columns=['proc_type','status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11172fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_pharmacy['infusion_type'] = df_pharmacy['infusion_type'].fillna('N/A')\n",
    "df_pharmacy['sliding_scale'] = df_pharmacy['sliding_scale'].fillna('N/A')\n",
    "df_pharmacy['duration_interval'] = df_pharmacy['duration_interval'].fillna('N/A')\n",
    "df_pharmacy['expiration_unit'] = df_pharmacy['expiration_unit'].fillna('N/A')\n",
    "df_pharmacy['dispensation'] = df_pharmacy['dispensation'].fillna('N/A')\n",
    "df_pharmacy['medication'] = df_pharmacy['medication'].fillna('N/A')\n",
    "df_pharmacy['route'] = df_pharmacy['route'].fillna('N/A')\n",
    "df_pharmacy['frequency'] = df_pharmacy['frequency'].fillna('N/A')\n",
    "df_pharmacy = pd.get_dummies(df_pharmacy, columns=['infusion_type','sliding_scale','duration_interval','expiration_unit',\n",
    "                                                  'dispensation','medication','route','frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61180b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0: lockout_interval, doses_per_24_hrs, duration, expiration_value\n",
    "df_pharmacy['lockout_interval'] = df_pharmacy['lockout_interval'].fillna(0)\n",
    "df_pharmacy['doses_per_24_hrs'] = df_pharmacy['doses_per_24_hrs'].fillna(0)\n",
    "df_pharmacy['expiration_value'] = df_pharmacy['expiration_value'].fillna(0)\n",
    "df_pharmacy['basal_rate'] = df_pharmacy['basal_rate'].fillna(0)\n",
    "df_pharmacy['one_hr_max'] = df_pharmacy['one_hr_max'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4cdb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pharmacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72643d14",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716afc94",
   "metadata": {},
   "source": [
    "Given information about a particular prescribed medication, predict the LOS for that patient (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pharmacy = df_pharmacy.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_pharmacy = df_pharmacy.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "df_pharmacy.to_csv('df_pharmacy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2f39d",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede12649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_pharmacy.drop(columns=['los'])\n",
    "target = df_pharmacy['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "pharmacy_data_train, pharmacy_data_test, pharmacy_label_train, pharmacy_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", pharmacy_data_train.shape, pharmacy_label_train.shape)\n",
    "print(\"Testing set shape:\", pharmacy_data_test.shape, pharmacy_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "pharmacy_data_train.to_csv('pharmacy_data_train.csv', index=False)\n",
    "pharmacy_data_test.to_csv('pharmacy_data_test.csv', index=False)\n",
    "\n",
    "pharmacy_label_train.to_csv('pharmacy_label_train.csv', index=False)\n",
    "pharmacy_label_test.to_csv('pharmacy_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb1c40c",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1fa18",
   "metadata": {},
   "source": [
    "### poe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/poe.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_poe = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fd0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_columns = df_poe.select_dtypes(include=['object'])\n",
    "# print(object_columns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ac417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_poe['order_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc090da",
   "metadata": {},
   "source": [
    "To drop: poe_id, subject_id, ordertime, discontinue_of_poe_id, discontinued_by_poe_id (all unique), order_status (all inactive)\n",
    "Encode: order_type, transaction_type\n",
    "Impute with N/A and then encode: order_subtype, order_provider_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5dafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a feature of ordertime - admittime for days_since_admission\n",
    "\n",
    "# Convert to datetime\n",
    "df_poe['ordertime'] = pd.to_datetime(df_poe['ordertime'], format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_poe = df_poe.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_poe['days_since_admission'] = df_poe['ordertime'] - df_poe['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_poe['days_since_admission'] = df_poe['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_poe = df_poe.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_poe = df_poe.drop(columns=['poe_id','subject_id','ordertime','discontinue_of_poe_id','discontinued_by_poe_id',\n",
    "                                       'order_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_poe = pd.get_dummies(df_poe, columns=['order_type','transaction_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cbf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_poe['order_subtype'] = df_poe['order_subtype'].fillna('N/A')\n",
    "df_poe['order_provider_id'] = df_poe['order_provider_id'].fillna('N/A')\n",
    "df_poe = pd.get_dummies(df_poe, columns=['order_subtype','order_provider_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf778c",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6c6b8",
   "metadata": {},
   "source": [
    "Given information about a particular order made by a provider, predict the LOS for that patient (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe = df_poe.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_poe = df_poe.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344378fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "df_poe.to_csv('df_poe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ebe71f",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ba4a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_poe.drop(columns=['los'])\n",
    "target = df_poe['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "poe_data_train, poe_data_test, poe_label_train, poe_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", poe_data_train.shape, poe_label_train.shape)\n",
    "print(\"Testing set shape:\", poe_data_test.shape, poe_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb068f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "poe_data_train.to_csv('poe_data_train.csv', index=False)\n",
    "poe_data_test.to_csv('poe_data_test.csv', index=False)\n",
    "\n",
    "poe_label_train.to_csv('poe_label_train.csv', index=False)\n",
    "poe_label_test.to_csv('poe_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d429156",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0adbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bb935",
   "metadata": {},
   "source": [
    "### prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ada77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/prescriptions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_prescriptions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prescriptions['ndc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed596e",
   "metadata": {},
   "source": [
    "Drop na and encode: dose_val_rx, form_val_disp, order_provider_id\n",
    "Drop: subject_id, pharmacy_id, starttime, stoptime, form_rx (mostly null), poe_id\n",
    "Impute with N/A and encode: formulary_drug_cd, gsn, prod_strength, route\n",
    "Encode: drug_type, drug, dose_unit_rx, form_unit_disp\n",
    "Impute with 0: doses_per_24_hrs\n",
    "\n",
    "Drop rows with na\n",
    "\n",
    "order_provider_id\n",
    "Was going to impute with N/A and encode but going to drop as too many features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature of stoptime-starttime called duration \n",
    "\n",
    "# Convert to datetime\n",
    "df_prescriptions['stoptime'] = pd.to_datetime(df_prescriptions['stoptime'], format='%Y/%m/%d %H:%M')\n",
    "df_prescriptions['starttime'] = pd.to_datetime(df_prescriptions['starttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_prescriptions['duration'] = df_prescriptions['stoptime'] - df_prescriptions['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_prescriptions['duration'] = df_prescriptions['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d367cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_prescriptions.dropna(subset=['dose_val_rx', 'form_val_disp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbefe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_prescriptions = df_prescriptions.drop(columns=['subject_id','pharmacy_id','starttime','stoptime','form_rx','poe_id',\n",
    "                                                 'order_provider_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b364333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_prescriptions['formulary_drug_cd'] = df_prescriptions['formulary_drug_cd'].fillna('N/A')\n",
    "df_prescriptions['gsn'] = df_prescriptions['gsn'].fillna('N/A')\n",
    "df_prescriptions['prod_strength'] = df_prescriptions['prod_strength'].fillna('N/A')\n",
    "df_prescriptions['route'] = df_prescriptions['route'].fillna('N/A')\n",
    "\n",
    "# Impute with 0\n",
    "df_prescriptions['ndc'] = df_prescriptions['ndc'].fillna(0)\n",
    "\n",
    "df_prescriptions = pd.get_dummies(df_prescriptions, columns=['formulary_drug_cd','gsn','prod_strength',\n",
    "                                                            'route','drug_type','drug','dose_unit_rx','form_unit_disp',\n",
    "                                                            'dose_val_rx','form_val_disp','ndc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions['doses_per_24_hrs'] = df_prescriptions['doses_per_24_hrs'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d897313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc3934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with null values \n",
    "df_prescriptions = df_prescriptions.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_columns = df_prescriptions.select_dtypes(include=['object'])\n",
    "# print(object_columns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2992df",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8657b83",
   "metadata": {},
   "source": [
    "Given information about a particular prescribed medication, predict the LOS for that patient (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff401bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions = df_prescriptions.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_prescriptions = df_prescriptions.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "# df_prescriptions.to_csv('df_prescriptions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ae675",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab54e80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_prescriptions.drop(columns=['los'])\n",
    "target = df_prescriptions['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "prescriptions_data_train, prescriptions_data_test, prescriptions_label_train, prescriptions_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", prescriptions_data_train.shape, prescriptions_label_train.shape)\n",
    "print(\"Testing set shape:\", prescriptions_data_test.shape, prescriptions_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "prescriptions_data_train.to_csv('prescriptions_data_train.csv', index=False)\n",
    "prescriptions_data_test.to_csv('prescriptions_data_test.csv', index=False)\n",
    "\n",
    "prescriptions_label_train.to_csv('prescriptions_label_train.csv', index=False)\n",
    "prescriptions_label_test.to_csv('prescriptions_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b32ff",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28492dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 4890 to 2874 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24886107",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/jenni/OneDrive/Desktop/IP/\"\n",
    "file = \"prescriptions_data_train.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "prescriptions_data_train = pd.read_csv(full_path)\n",
    "\n",
    "file = \"prescriptions_data_test.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "prescriptions_data_test = pd.read_csv(full_path)\n",
    "\n",
    "file = \"prescriptions_label_train.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "prescriptions_label_train = pd.read_csv(full_path)\n",
    "\n",
    "file = \"prescriptions_label_test.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "prescriptions_label_test = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3341ac",
   "metadata": {},
   "source": [
    "### procedures_icd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/procedures_icd.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_procedures = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545710a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedures['icd_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e620f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_procedures['icd_version'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1e4ae",
   "metadata": {},
   "source": [
    "Drop: subject_id, chartdate\n",
    "Encode: icd_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf864de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a feature called days_since_admission of chartdate - admitdate\n",
    "\n",
    "# Convert to datetime\n",
    "df_procedures['chartdate'] = pd.to_datetime(df_procedures['chartdate'], format='%Y-%m-%d')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_procedures = df_procedures.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "df_procedures['admittime'] = df_procedures['admittime'].dt.date\n",
    "df_procedures['chartdate'] = df_procedures['chartdate'].dt.date\n",
    "\n",
    "df_procedures['days_since_admission'] = df_procedures['chartdate'] - df_procedures['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_procedures['days_since_admission'] = df_procedures['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_procedures = df_procedures.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df385e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_procedures = df_procedures.drop(columns=['subject_id','chartdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622fd8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_procedures = pd.get_dummies(df_procedures, columns=['icd_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a4059",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968699d",
   "metadata": {},
   "source": [
    "Given information about billed procedures for patients during their hospital stay, predict the LOS for that patient (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedures = df_procedures.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_procedures = df_procedures.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c025eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "# df_procedures.to_csv('df_procedures.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4ba50",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb03ed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_procedures.drop(columns=['los'])\n",
    "target = df_procedures['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "procedures_data_train, procedures_data_test, procedures_label_train, procedures_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", procedures_data_train.shape, procedures_label_train.shape)\n",
    "print(\"Testing set shape:\", procedures_data_test.shape, procedures_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "# procedures_data_train.to_csv('procedures_data_train.csv', index=False)\n",
    "# procedures_data_test.to_csv('procedures_data_test.csv', index=False)\n",
    "\n",
    "# procedures_label_train.to_csv('procedures_label_train.csv', index=False)\n",
    "# procedures_label_test.to_csv('procedures_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93521dd1",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 355 to 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6472d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/jenni/OneDrive/Desktop/IP/\"\n",
    "file = \"procedures_data_train.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "procedures_data_train = pd.read_csv(full_path)\n",
    "\n",
    "file = \"procedures_data_test.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "procedures_data_test = pd.read_csv(full_path)\n",
    "\n",
    "file = \"procedures_label_train.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "procedures_label_train = pd.read_csv(full_path)\n",
    "\n",
    "file = \"procedures_label_test.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "procedures_label_test = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc9661",
   "metadata": {},
   "source": [
    "### services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/services.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_services = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_services.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe27366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_services['curr_service'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4c32b",
   "metadata": {},
   "source": [
    "Drop: subject_id, transfertime\n",
    "Impute with N/A and encode: prev_service\n",
    "Encode: curr_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature called days_since_admission using transfertime-admittime \n",
    "\n",
    "# Convert to datetime\n",
    "df_services['transfertime'] = pd.to_datetime(df_services['transfertime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_services = df_services.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_services['days_since_admission'] = df_services['transfertime'] - df_services['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_services['days_since_admission'] = df_services['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_services = df_services.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d765fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_services = df_services.drop(columns=['subject_id','transfertime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a352cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_services['prev_service'] = df_services['prev_service'].fillna('N/A')\n",
    "df_services = pd.get_dummies(df_services, columns=['prev_service','curr_service'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c7f0d",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73696ed",
   "metadata": {},
   "source": [
    "Given information about the hospital service(s) which cared for the patient during their hospitalization, predict their eventual LOS (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_services = df_services.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_services = df_services.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f512d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "# df_services.to_csv('df_services.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea60ac",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b462d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_services.drop(columns=['los'])\n",
    "target = df_services['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "services_data_train, services_data_test, services_label_train, services_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", services_data_train.shape, services_label_train.shape)\n",
    "print(\"Testing set shape:\", services_data_test.shape, services_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf636d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "# services_data_train.to_csv('services_data_train.csv', index=False)\n",
    "# services_data_test.to_csv('services_data_test.csv', index=False)\n",
    "\n",
    "# services_label_train.to_csv('services_label_train.csv', index=False)\n",
    "# services_label_test.to_csv('services_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "services_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e8414",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c0b28",
   "metadata": {},
   "source": [
    "### transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/transfers.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_transfers = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f7ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transfers['intime'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2565229",
   "metadata": {},
   "source": [
    "Drop: subject_id, transfer_id, intime, outtime\n",
    "Encode: eventtype\n",
    "Impute with N/A and encode: careunit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a days_since_admission feature of intime-admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_transfers['intime'] = pd.to_datetime(df_transfers['intime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_transfers = df_transfers.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "df_transfers['days_since_admission'] = df_transfers['intime'] - df_transfers['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_transfers['days_since_admission'] = df_transfers['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_transfers = df_transfers.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature of outtime-intime \n",
    "\n",
    "# Convert to datetime\n",
    "df_transfers['outtime'] = pd.to_datetime(df_transfers['outtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_transfers['duration'] = df_transfers['outtime'] - df_transfers['intime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_transfers['duration'] = df_transfers['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567492b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_transfers = df_transfers.drop(columns=['subject_id','transfer_id','intime','outtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_transfers['careunit'] = df_transfers['careunit'].fillna('N/A')\n",
    "df_transfers = pd.get_dummies(df_transfers, columns=['eventtype','careunit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transfers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01697c70",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cde6d",
   "metadata": {},
   "source": [
    "Given information about patients' unit transfers, predict their eventual LOS (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers = df_transfers.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_transfers = df_transfers.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435415cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "# df_transfers.to_csv('df_transfers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653b3d8",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9f2d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_transfers.drop(columns=['los'])\n",
    "target = df_transfers['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "transfers_data_train, transfers_data_test, transfers_label_train, transfers_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", transfers_data_train.shape, transfers_label_train.shape)\n",
    "print(\"Testing set shape:\", transfers_data_test.shape, transfers_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19195cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "# transfers_data_train.to_csv('transfers_data_train.csv', index=False)\n",
    "# transfers_data_test.to_csv('transfers_data_test.csv', index=False)\n",
    "\n",
    "# transfers_label_train.to_csv('transfers_label_train.csv', index=False)\n",
    "# transfers_label_test.to_csv('transfers_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e47613",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4c106",
   "metadata": {},
   "source": [
    "### chartevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc097ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/chartevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_chart = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chart['itemid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_columns = df_chart.select_dtypes(include=['object'])\n",
    "# print(object_columns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eecde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_chart['warning'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d98b9d",
   "metadata": {},
   "source": [
    "Drop: subject_id, charttime, storetime, stay_id, caregiver_id (the person who documented the data)\n",
    "Encode: value,itemid\n",
    "Impute with 0: valuenum, warning\n",
    "Impute with N/A and encode: valueuom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a days_since_admission feature of charttime-admittime \n",
    "\n",
    "# Convert to datetime\n",
    "df_chart['charttime'] = pd.to_datetime(df_chart['charttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_chart = df_chart.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_chart['days_since_admission'] = df_chart['charttime'] - df_chart['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_chart['days_since_admission'] = df_chart['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_chart = df_chart.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5684379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a delay feature of storetime-charttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_chart['storetime'] = pd.to_datetime(df_chart['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_chart['delay'] = df_chart['storetime'] - df_chart['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_chart['delay'] = df_chart['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ed761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_chart = df_chart.drop(columns=['subject_id','charttime','storetime', 'stay_id','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19afceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_chart['valueuom'] = df_chart['valueuom'].fillna('N/A')\n",
    "df_chart = pd.get_dummies(df_chart, columns=['valueuom','value','itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0\n",
    "df_chart['valuenum'] = df_chart['valuenum'].fillna(0)\n",
    "df_chart['warning'] = df_chart['warning'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_chart.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8429de8a",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a72c2b",
   "metadata": {},
   "source": [
    "Given a piece of charted data during their ICU stay, predict their eventual LOS (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52070eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chart = df_chart.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_chart = df_chart.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ed921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "# uncomment and run if changes are made (it takes ages btw)\n",
    "# df_chart.to_csv('df_chart.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe470a",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ab4dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_chart.drop(columns=['los'])\n",
    "target = df_chart['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "chart_data_train, chart_data_test, chart_label_train, chart_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", chart_data_train.shape, chart_label_train.shape)\n",
    "print(\"Testing set shape:\", chart_data_test.shape, chart_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "# chart_data_train.to_csv('chart_data_train.csv', index=False)\n",
    "# chart_data_test.to_csv('chart_data_test.csv', index=False)\n",
    "\n",
    "# chart_label_train.to_csv('chart_label_train.csv', index=False)\n",
    "# chart_label_test.to_csv('chart_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80079d25",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f492f",
   "metadata": {},
   "source": [
    "### icustays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d51b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/icustays.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_icustays = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icustays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97bf492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icustays['outtime'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b03ca",
   "metadata": {},
   "source": [
    "Drop: subject_id, stay_id, intime, outtime\n",
    "Encode: first_careunit, last_careunit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e68baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a feature called days_since_admission using intime-admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_icustays['intime'] = pd.to_datetime(df_icustays['intime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_icustays = df_icustays.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_icustays['days_since_admission'] = df_icustays['intime'] - df_icustays['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_icustays['days_since_admission'] = df_icustays['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_icustays = df_icustays.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0adc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_icustays = df_icustays.drop(columns=['subject_id','stay_id','intime','outtime'])\n",
    "\n",
    "# Rename los to icu_los\n",
    "df_icustays = df_icustays.rename(columns={'los': 'icu_los'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d02448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_icustays = pd.get_dummies(df_icustays, columns=['first_careunit','last_careunit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462c3c3",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c8f99",
   "metadata": {},
   "source": [
    "Given tracking information for ICU stays including admission and discharge times, predict the patient's eventual LOS (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693404af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icustays = df_icustays.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_icustays = df_icustays.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad4fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "# df_icustays.to_csv('df_icustays.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b9028",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ceb1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_icustays.drop(columns=['los'])\n",
    "target = df_icustays['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "icustays_data_train, icustays_data_test, icustays_label_train, icustays_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", icustays_data_train.shape, icustays_label_train.shape)\n",
    "print(\"Testing set shape:\", icustays_data_test.shape, icustays_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ffe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "# icustays_data_train.to_csv('icustays_data_train.csv', index=False)\n",
    "# icustays_data_test.to_csv('icustays_data_test.csv', index=False)\n",
    "\n",
    "# icustays_label_train.to_csv('icustays_label_train.csv', index=False)\n",
    "# icustays_label_test.to_csv('icustays_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccb624",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1509a270",
   "metadata": {},
   "source": [
    "### ingredientevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b471e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/ingredientevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_ingredient = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a037775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingredient.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54437de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingredient['storetime']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a7f9a",
   "metadata": {},
   "source": [
    "Drop: subject_id, starttime, endtime, storetime, orderid, originalamount, stay_id, caregiver_id\n",
    "Encode: amountuom, statusdescription, itemid\n",
    "Impute with 0: rate\n",
    "Impute with N/A and encode: rateuom, linkorderid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature of endtime-starttime \n",
    "\n",
    "# Convert to datetime\n",
    "df_ingredient['endtime'] = pd.to_datetime(df_ingredient['endtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_ingredient['starttime'] = pd.to_datetime(df_ingredient['starttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "df_ingredient['duration'] = df_ingredient['endtime'] - df_ingredient['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_ingredient['duration'] = df_ingredient['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a recording_delay feature of storetime-endtime\n",
    "\n",
    "# Convert to datetime\n",
    "df_ingredient['storetime'] = pd.to_datetime(df_ingredient['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_ingredient['recording_delay'] = df_ingredient['storetime'] - df_ingredient['endtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_ingredient['recording_delay'] = df_ingredient['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_ingredient = df_ingredient.drop(columns=['subject_id','starttime','endtime','storetime','orderid','originalamount',\n",
    "                                           'stay_id','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_ingredient['rateuom'] = df_ingredient['rateuom'].fillna('N/A')\n",
    "df_ingredient['linkorderid'] = df_ingredient['linkorderid'].fillna('N/A')\n",
    "df_ingredient = pd.get_dummies(df_ingredient, columns=['rateuom','amountuom','statusdescription','itemid','linkorderid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0\n",
    "df_ingredient['rate'] = df_ingredient['rate'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16fb5ec",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4e7f7",
   "metadata": {},
   "source": [
    "Given information on ingredients of continuous or intermittent administrations including nutritional and water content, predict the patient's eventual LOS (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a706fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingredient = df_ingredient.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_ingredient = df_ingredient.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "df_ingredient.to_csv('df_ingredient.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c996b8c",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3166c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_ingredient.drop(columns=['los'])\n",
    "target = df_ingredient['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "ingredient_data_train, ingredient_data_test, ingredient_label_train, ingredient_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", ingredient_data_train.shape, ingredient_label_train.shape)\n",
    "print(\"Testing set shape:\", ingredient_data_test.shape, ingredient_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "ingredient_data_train.to_csv('ingredient_data_train.csv', index=False)\n",
    "ingredient_data_test.to_csv('ingredient_data_test.csv', index=False)\n",
    "\n",
    "ingredient_label_train.to_csv('ingredient_label_train.csv', index=False)\n",
    "ingredient_label_test.to_csv('ingredient_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544db7e6",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beec90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 7727 to 4116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/jenni/OneDrive/Desktop/IP/\"\n",
    "file = \"ingredient_data_train.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "ingredient_data_train = pd.read_csv(full_path)\n",
    "\n",
    "file = \"ingredient_data_test.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "ingredient_data_test = pd.read_csv(full_path)\n",
    "\n",
    "file = \"ingredient_label_train.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "ingredient_label_train = pd.read_csv(full_path)\n",
    "\n",
    "file = \"ingredient_label_test.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "ingredient_label_test = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1b0f6",
   "metadata": {},
   "source": [
    "### inputevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/inputevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_input = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_columns = df_input.select_dtypes(include=['object'])\n",
    "# print(object_columns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662c45c",
   "metadata": {},
   "source": [
    "Drop: subject_id, starttime, endtime, storetime, orderid, linkorderid, continueinnextdept, stay_id, caregiver_id,\n",
    "totalamountuom\n",
    "Encode: amountuom, ordercategoryname, ordercomponenttypedescription, ordercategorydescription, statusdescription, itemid\n",
    "Impute with 0: rate, totalamount\n",
    "Impute with N/A and encode: rateuom, secondaryordercategoryname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature using endtime-starttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_input['endtime'] = pd.to_datetime(df_input['endtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_input['starttime'] = pd.to_datetime(df_input['starttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "df_input['duration'] = df_input['endtime'] - df_input['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_input['duration'] = df_input['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a recording_delay feature using storetime-endtime\n",
    "\n",
    "# Convert to datetime\n",
    "df_input['storetime'] = pd.to_datetime(df_input['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_input['recording_delay'] = df_input['storetime'] - df_input['endtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_input['recording_delay'] = df_input['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_input = df_input.drop(columns=['subject_id','stay_id','starttime','endtime','storetime','orderid','linkorderid',\n",
    "                                  'continueinnextdept','totalamountuom', 'stay_id','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af90068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_input['rateuom'] = df_input['rateuom'].fillna('N/A')\n",
    "df_input['secondaryordercategoryname'] = df_input['secondaryordercategoryname'].fillna('N/A')\n",
    "df_input = pd.get_dummies(df_input, columns=['rateuom','secondaryordercategoryname','amountuom','ordercategoryname',\n",
    "                                            'ordercomponenttypedescription','ordercategorydescription','statusdescription',\n",
    "                                            'itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0\n",
    "df_input['rate'] = df_input['rate'].fillna(0)\n",
    "df_input['totalamount'] = df_input['totalamount'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_input.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf44c7",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1236763",
   "metadata": {},
   "source": [
    "Given information documented regarding continuous infusions or intermittent administrations, predict the patient's eventual LOS (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_input.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_input = df_input.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "df_input.to_csv('df_input.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b05792",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d8149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_input.drop(columns=['los'])\n",
    "target = df_input['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "input_data_train, input_data_test, input_label_train, input_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", input_data_train.shape, input_label_train.shape)\n",
    "print(\"Testing set shape:\", input_data_test.shape, input_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf63a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "input_data_train.to_csv('input_data_train.csv', index=False)\n",
    "input_data_test.to_csv('input_data_test.csv', index=False)\n",
    "\n",
    "input_label_train.to_csv('input_label_train.csv', index=False)\n",
    "input_label_test.to_csv('input_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bf790",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dfb7de",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e206cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc11f4",
   "metadata": {},
   "source": [
    "### outputevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27810950",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/outputevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_output = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e69d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_output['value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709537d6",
   "metadata": {},
   "source": [
    "Drop: subject_id, charttime, storetime, valueuom, stay_id, caregiver_id'\n",
    "Encode: itemid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab39c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a days_since_admission feature using charttime-admittime \n",
    "\n",
    "# Convert to datetime\n",
    "df_output['charttime'] = pd.to_datetime(df_output['charttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_output = df_output.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "df_output['days_since_admission'] = df_output['charttime'] - df_output['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_output['days_since_admission'] = df_output['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_output = df_output.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a recording_delay feature using storetime-charttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_output['storetime'] = pd.to_datetime(df_output['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_output['recording_delay'] = df_output['storetime'] - df_output['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_output['recording_delay'] = df_output['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_output = df_output.drop(columns=['subject_id','stay_id','charttime','storetime','storetime','valueuom','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00509966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode\n",
    "df_output = pd.get_dummies(df_output, columns=['itemid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247b9f5",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5458391",
   "metadata": {},
   "source": [
    "Given information regarding patient outputs including urine, drainage, and so on, predict the patient's eventual LOS (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0678616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_output = df_output.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a9caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "df_output.to_csv('df_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193906d",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a70a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_output.drop(columns=['los'])\n",
    "target = df_output['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "output_data_train, output_data_test, output_label_train, output_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", output_data_train.shape, output_label_train.shape)\n",
    "print(\"Testing set shape:\", output_data_test.shape, output_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8020e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "output_data_train.to_csv('output_data_train.csv', index=False)\n",
    "output_data_test.to_csv('output_data_test.csv', index=False)\n",
    "\n",
    "output_label_train.to_csv('output_label_train.csv', index=False)\n",
    "output_label_test.to_csv('output_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbedaa",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e151a754",
   "metadata": {},
   "source": [
    "### procedureevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb562c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/procedureevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_procedure_events = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedure_events.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_procedure_events['value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9af05",
   "metadata": {},
   "source": [
    "Drop: subject_id, starttime, endtime, storetime, orderid, linkorderid, continueinnextdept, stay_id, caregiver_id\n",
    "Encode: valueuom, ordercategoryname, ordercategorydescription, statusdescription, itemid\n",
    "Impute with N/A and encode: location, locationcategory\n",
    "MAKE DURATION FEATURE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfebd9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature using endtime-starttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_procedure_events['endtime'] = pd.to_datetime(df_procedure_events['endtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_procedure_events['starttime'] = pd.to_datetime(df_procedure_events['starttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "df_procedure_events['duration'] = df_procedure_events['endtime'] - df_procedure_events['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_procedure_events['duration'] = df_procedure_events['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a recording_delay feature using storetime-endtime\n",
    "\n",
    "# Convert to datetime\n",
    "df_procedure_events['storetime'] = pd.to_datetime(df_procedure_events['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_procedure_events['recording_delay'] = df_procedure_events['storetime'] - df_procedure_events['endtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_procedure_events['recording_delay'] = df_procedure_events['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_procedure_events = df_procedure_events.drop(columns=['subject_id','stay_id','starttime','endtime','storetime','orderid',\n",
    "                                                        'linkorderid','continueinnextdept','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_procedure_events['location'] = df_procedure_events['location'].fillna('N/A')\n",
    "df_procedure_events['locationcategory'] = df_procedure_events['locationcategory'].fillna('N/A')\n",
    "df_procedure_events = pd.get_dummies(df_procedure_events, columns=['location','locationcategory','valueuom',\n",
    "                                                                   'ordercategoryname','ordercategorydescription',\n",
    "                                                                   'statusdescription','itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cffb5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedure_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab2509",
   "metadata": {},
   "source": [
    "#### Learner target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc72e3b",
   "metadata": {},
   "source": [
    "Given information regarding patient outputs including urine, drainage, and so on, predict the patient's eventual LOS (based on hadm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedure_events = df_procedure_events.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_procedure_events = df_procedure_events.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ecffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "\n",
    "# uncomment and run if changes are made\n",
    "df_procedure_events.to_csv('df_procedure_events.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76831c55",
   "metadata": {},
   "source": [
    "#### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01069273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_procedure_events.drop(columns=['los'])\n",
    "target = df_procedure_events['los']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "procedure_events_data_train, procedure_events_data_test, procedure_events_label_train, procedure_events_label_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets\n",
    "print(\"Training set shape:\", procedure_events_data_train.shape, procedure_events_label_train.shape)\n",
    "print(\"Testing set shape:\", procedure_events_data_test.shape, procedure_events_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if changes are made\n",
    "\n",
    "procedure_events_data_train.to_csv('procedure_events_data_train.csv', index=False)\n",
    "procedure_events_data_test.to_csv('procedure_events_data_test.csv', index=False)\n",
    "\n",
    "procedure_events_label_train.to_csv('procedure_events_label_train.csv', index=False)\n",
    "procedure_events_label_test.to_csv('procedure_events_label_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcfa7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure_events_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714b243",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a903e3",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
