{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374a49e1",
   "metadata": {},
   "source": [
    "### Training of LOS learners for ensemble - full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607cae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE\n",
    "# might need further dimension reduction because I'm removing a lot of data by filtering out the 20 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c786c1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.24.3\n",
      "Pandas version: 1.5.3\n",
      "Scikit version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "# External libraries for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "#To render graphs within notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib \n",
    "import os\n",
    "\n",
    "# Versions of libraries\n",
    "print(\"Numpy version: {}\".format(np.__version__))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Scikit version: {}\".format(sk.__version__))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f2ed738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dafd5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Project/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e977f4",
   "metadata": {},
   "source": [
    "#### Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9301abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_days(duration_str):\n",
    "    parts = duration_str.split(' days ')  # Split string into form ['22', '20:55:00']\n",
    "    days = float(parts[0])  # Extract number of days and convert to float\n",
    "    time_parts = parts[1].split(':')  # Split time part (hh:mm:ss) ['20', '55', '00']\n",
    "    hours = float(time_parts[0])  # Extract hours and convert to float\n",
    "    minutes = float(time_parts[1])  # Extract minutes and convert to float\n",
    "    seconds = float(time_parts[2])  # Extract seconds and convert to float\n",
    "    total_days = days + (hours / 24) + (minutes / (24 * 60)) + (seconds / (24 * 3600))  # Calculate total days\n",
    "    return total_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd63138",
   "metadata": {},
   "source": [
    "#### Select 20 patients (based on subject_id from patients) to use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b576a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/patients.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_patients = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_patients = df_patients['subject_id'].sample(n=20, random_state=42).tolist()\n",
    "\n",
    "# Any records belonging to these 20 subjects will be removed before training \n",
    "print(evaluation_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95933b",
   "metadata": {},
   "source": [
    "Changed my mind, I want to filter based on hadm_id not subject_id\n",
    "If a table only has subject_id then remove the subjects the evaluation admissions belong to (admissions maps subject_id to hadm_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507786c",
   "metadata": {},
   "source": [
    "#### Select 55 (20% of) admissions to use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3604a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/admissions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_admissions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3136f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27617929, 27553957, 20282368, 27296885, 24980601, 21133938, 25559382, 20611796, 28778757, 28723315, 28998349, 28676446, 29276678, 26842957, 21477991, 25922998, 26706939, 27993466, 28236161, 27259207, 20385771, 24540843, 20900955, 22413744, 27494880, 25103777, 21599196, 21540783, 22585261, 26275841, 22130791, 22490490, 25020332, 29279905, 29483621, 27167814, 25508812, 21607814, 20297618, 29974575, 24912093, 21255400, 29295881, 28829452, 24656677, 29858644, 23488445, 25970245, 22508257, 25742920, 25085565, 22228639, 27660781, 28335091, 27703517]\n"
     ]
    }
   ],
   "source": [
    "evaluation_admissions = df_admissions['hadm_id'].sample(n=55, random_state=42).tolist()\n",
    "\n",
    "# Any records belonging to these admissions will be removed before training \n",
    "print(evaluation_admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ccc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_patients = df_admissions[df_admissions['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ab2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_patients = evaluation_patients['subject_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba40ac",
   "metadata": {},
   "source": [
    "#### Target variable LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1673b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS based on admissions table (target dataframe)\n",
    "\n",
    "file = \"hosp/admissions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_admissions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a037a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions['dischtime'] = pd.to_datetime(df_admissions['dischtime'], format='%d/%m/%Y %H:%M')\n",
    "df_admissions['admittime'] = pd.to_datetime(df_admissions['admittime'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c50aa82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_los_hadm = pd.DataFrame()\n",
    "df_los_subject = pd.DataFrame()\n",
    "\n",
    "df_los_subject['subject_id'] = df_admissions['subject_id']\n",
    "df_los_hadm['hadm_id'] = df_admissions['hadm_id']\n",
    "df_los_hadm['los'] = df_admissions['dischtime']-df_admissions['admittime']\n",
    "df_los_subject['los'] = df_admissions['dischtime']-df_admissions['admittime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a2ead7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24181354</td>\n",
       "      <td>8 days 23:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25926192</td>\n",
       "      <td>7 days 20:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23983182</td>\n",
       "      <td>5 days 17:33:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22942076</td>\n",
       "      <td>1 days 17:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21606243</td>\n",
       "      <td>2 days 02:11:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>24745425</td>\n",
       "      <td>5 days 15:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>22168393</td>\n",
       "      <td>4 days 12:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>27708593</td>\n",
       "      <td>7 days 07:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>23251352</td>\n",
       "      <td>4 days 04:56:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>28108313</td>\n",
       "      <td>2 days 16:10:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hadm_id             los\n",
       "0    24181354 8 days 23:24:00\n",
       "1    25926192 7 days 20:12:00\n",
       "2    23983182 5 days 17:33:00\n",
       "3    22942076 1 days 17:41:00\n",
       "4    21606243 2 days 02:11:00\n",
       "..        ...             ...\n",
       "270  24745425 5 days 15:57:00\n",
       "271  22168393 4 days 12:18:00\n",
       "272  27708593 7 days 07:10:00\n",
       "273  23251352 4 days 04:56:00\n",
       "274  28108313 2 days 16:10:00\n",
       "\n",
       "[275 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_los_hadm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc995c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average LOS for each subject_id\n",
    "df_los_subject = df_los_subject.groupby('subject_id').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d038117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admittime= pd.DataFrame()\n",
    "df_admittime['hadm_id'] = df_admissions['hadm_id']\n",
    "df_admittime['admittime'] = df_admissions['admittime']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677808d",
   "metadata": {},
   "source": [
    "### omr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0135261",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/omr.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_omr = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_omr_training = df_omr[~df_omr['subject_id'].isin(evaluation_patients)]\n",
    "df_omr_evaluation = df_omr[df_omr['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "\n",
    "# Create directory \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_omr_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_omr_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f10f7",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr = df_omr_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e0e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr = df_omr.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb910d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine result_name and seq_num into the column name with result_value from the same row as its value \n",
    "\n",
    "# Function to combine values from columns into a new column \n",
    "def new_columns(row):\n",
    "    return row['result_name'] + '_' + str(row['seq_num'])\n",
    "\n",
    "new_names = df_omr.apply(new_columns, axis=1) # series of names of combinations \n",
    "\n",
    "\n",
    "def add_values(row, colName):\n",
    "    name = row['result_name'] + '_' + str(row['seq_num'])\n",
    "    if str(name) == colName:\n",
    "        return row['result_value']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "for i in range(len(new_names)):\n",
    "    df_omr[new_names[i]] = df_omr.apply(add_values, args=(new_names[i],), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fdb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop seq_num, result_name, result_value\n",
    "df_omr = df_omr.drop(columns=['seq_num', 'result_name', 'result_value'])\n",
    "# sequence number doesn't add any useful info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42289a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr['subject_id'].value_counts()\n",
    "\n",
    "# The patient with the most measurements has 391 so could make it 391 features for everyone but most will have lots of \n",
    "# zeroes\n",
    "# Fine as sparcity represents not taking many measurements which could also be a factor?\n",
    "# Could have number of measurements as an additional feature too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_omr[df_omr['subject_id'] == 10019003]\n",
    "filtered_df\n",
    "\n",
    "# Preserves every measurement made for each subject across all of their stays \n",
    "# Only one entry per row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = df_omr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68eb155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering by date (so each patients measurements are chronological from top to bottom)\n",
    "\n",
    "df_omr = df_omr.sort_values(by=['subject_id', 'chartdate'])\n",
    "\n",
    "df_omr\n",
    "\n",
    "# This preserves for example, increase in weight over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2960241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop chartdate since the time shift is not consistent for each subject \n",
    "df_omr = df_omr.drop(columns=['chartdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_omr = df_omr.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87506a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row for each subject, features for every measurement made on them \n",
    "\n",
    "colNames = df_omr.columns.tolist()\n",
    "colNames.remove('subject_id')\n",
    "\n",
    "x = 0\n",
    "prev_subject = 0\n",
    "\n",
    "\n",
    "for row in range(len(df_omr)):\n",
    "    current_subject = df_omr['subject_id'][row] \n",
    "    if current_subject != prev_subject:\n",
    "        x = 0 # reset x\n",
    "    for i in range(len(colNames)): # for each column\n",
    "        if df_omr.loc[row, colNames[i]] != 0:\n",
    "            if colNames[i] + '_0' not in df_omr_final.columns: # New column name added\n",
    "                x = 0 # reset x\n",
    "            new_name = colNames[i] + '_' + str(x)\n",
    "            if new_name in df_omr_final.columns and (current_subject == prev_subject): # Trying to add another of the same \n",
    "                # measurement for the same patient \n",
    "                x += 1\n",
    "                new_name = colNames[i] + '_' + str(x)\n",
    "            df_omr_final.loc[current_subject, new_name] = df_omr.loc[row, colNames[i]]\n",
    "            df_omr_final = df_omr_final.copy()\n",
    "            break # leave for loop as the rest of the columns will be 0 for this row\n",
    "    prev_subject = current_subject\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr_final.fillna(0, inplace=True)\n",
    "df_omr_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all values to numbers \n",
    "\n",
    "df_omr_final = df_omr_final.astype(str)\n",
    "\n",
    "# Function to convert fraction string to decimal\n",
    "def fraction_to_decimal(fraction_str):\n",
    "    try:\n",
    "        numerator, denominator = map(int, fraction_str.split('/'))\n",
    "        return numerator / denominator\n",
    "    except ValueError:\n",
    "        return fraction_str  # Return unchanged if not a fraction\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "df_omr_final = df_omr_final.applymap(fraction_to_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr_final = df_omr_final.astype(float)\n",
    "# df_omr_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and convert it to a column\n",
    "df_omr_final.reset_index(inplace=True)\n",
    "df_omr_final.rename(columns={'index': 'subject_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames based on the ID column\n",
    "df_omr_final = df_omr_final.merge(df_los_subject, on='subject_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this show?\n",
    "# Each patient (subject_id is the index of the df) has measurements showing type_sequence_date\n",
    "# sequence starts from 1 and it is used when the same measurement was taken more than once in a single day\n",
    "# date starts from 0 and is used when the same measurement for the same patient was taken on a different day\n",
    "# Note that they were NOT taken on the same date for each patient but the bigger the date integer, the later the measurement\n",
    "# was taken, relative to that patient's admission  \n",
    "\n",
    "# Weight (Lbs)_1_0 is the first time the patient was weighed, Weight (Lbs)_3_0 is the third time they were weighed on that \n",
    "# same day as they were first weighed\n",
    "# Weight (Lbs)_1_1 is from a separate (later) date where the patient was weighed again, this is the first measurement \n",
    "# from this day \n",
    "# Any non applicable measurements are imputed with 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61878a30",
   "metadata": {},
   "source": [
    "Decide which ones to keep all measurements of per patient and which to just take the average and keep as one record for patient (that aren’t likely to change):\n",
    "\n",
    "Remove blood pressure sitting, lying and standing as too few samples \n",
    "Take average for height \n",
    "\n",
    "Could probably drop a few of the features that are really empty ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop subject_id\n",
    "df_omr_final = df_omr_final.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613460b",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6fe328",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_omr_final.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_omr_final['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction for data\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 12\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(data)\n",
    "data = svd.transform(data)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "# y_test['los'] = y_test['los'].astype(str)\n",
    "# y_test.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "# y_test.loc[~y_test['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "# y_test['los'] = y_test['los'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca53027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest regression\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_omr = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_omr.fit(data, target)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# # Calculate mean squared error\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# # Plot true vs predicted values\n",
    "# plt.scatter(y_test, y_pred)\n",
    "# plt.xlabel(\"True Values\")\n",
    "# plt.ylabel(\"Predicted Values\")\n",
    "# plt.title(\"True vs Predicted Values (Random Forest Regression)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153efda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "# Create a new directory for the model file\n",
    "output_folder = 'LOS_RF_learners'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_omr.joblib')\n",
    "dump(random_forest_omr, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f1ab3",
   "metadata": {},
   "source": [
    "### Admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9587a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/admissions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_admissions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b1ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_admissions_training = df_admissions[~df_admissions['subject_id'].isin(evaluation_patients)]\n",
    "df_admissions_evaluation = df_admissions[df_admissions['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16727570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_admissions_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_admissions_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18359a82",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e839590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an ed_duration feature for edouttime - edregtime (how long the patient stayed in the emergency department)\n",
    "\n",
    "# Convert to datetime\n",
    "df_admissions['edouttime'] = pd.to_datetime(df_admissions['edouttime'], format='%d/%m/%Y %H:%M')\n",
    "df_admissions['edregtime'] = pd.to_datetime(df_admissions['edregtime'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "df_admissions['ed_duration'] = df_admissions['edouttime'] - df_admissions['edregtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_admissions['ed_duration'] = df_admissions['ed_duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.drop(columns=['subject_id', 'admittime', 'dischtime', 'deathtime', 'hospital_expire_flag'\n",
    "                            , 'edregtime', 'edouttime', 'admit_provider_id','discharge_location'])\n",
    "\n",
    "# discharge_location is an outcome feature, should not be used to predict LOS as not known beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null with N/A and then one hot encode\n",
    "df_admissions['marital_status'] = df_admissions['marital_status'].fillna('N/A')\n",
    "df_admissions = pd.get_dummies(df_admissions, columns=['admission_type', 'admission_location', \n",
    "                                                      'insurance','language', 'marital_status','race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_admissions = df_admissions.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f086b17",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_admissions.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_admissions['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f817e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "\n",
    "data['ed_duration']= data['ed_duration'].astype(str)\n",
    "data['ed_duration']= data['ed_duration'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bbf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_admissions = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_admissions.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd514c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_admissions.joblib')\n",
    "dump(random_forest_admissions, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183dd88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2b8e0c2",
   "metadata": {},
   "source": [
    "### Emar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/emar.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_emar = pd.read_csv(full_path)\n",
    "\n",
    "# records for 65 different patients \n",
    "# 181 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_emar_training = df_emar[~df_emar['subject_id'].isin(evaluation_patients)]\n",
    "df_emar_evaluation = df_emar[df_emar['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_emar_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_emar_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd44b01",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar = df_emar_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfba95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar = df_emar.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f956e23",
   "metadata": {},
   "source": [
    "Impute with N/A and encode: enter_provider_id, medication\n",
    "\n",
    "Drop: subject_id, emar_id, poe_id, pharmacy_id, event_txt, storetime\n",
    "\n",
    "poe_id is an identifier which links administrations in emar to orders in poe and prescriptions\n",
    "storetime is when it was recorded in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b97988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature called delay using scheduletime - charttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_emar['scheduletime'] = pd.to_datetime(df_emar['scheduletime'], format='%Y/%m/%d %H:%M')\n",
    "df_emar['charttime'] = pd.to_datetime(df_emar['charttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_emar['delay'] = df_emar['charttime'] - df_emar['scheduletime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_emar['delay'] = df_emar['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e71f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar = df_emar.drop(columns=['subject_id','emar_id','poe_id','pharmacy_id',\n",
    "                               'event_txt','charttime','scheduletime','storetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4042494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null with N/A and then one hot encode\n",
    "df_emar['enter_provider_id'] = df_emar['enter_provider_id'].fillna('N/A')\n",
    "df_emar['medication'] = df_emar['medication'].fillna('N/A')\n",
    "df_emar = pd.get_dummies(df_emar, columns=['enter_provider_id', 'medication'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c875127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar = df_emar.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_emar = df_emar.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efdfe37",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40356b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_emar.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_emar['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542cd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['delay']= data['delay'].astype(str)\n",
    "data['delay']= data['delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f07163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_emar = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_emar.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d18eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_emar.joblib')\n",
    "dump(random_forest_emar, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa144449",
   "metadata": {},
   "source": [
    "### Emar_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/emar_detail.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_emar_detail = pd.read_csv(full_path,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_emar_detail_training = df_emar_detail[~df_emar_detail['subject_id'].isin(evaluation_patients)]\n",
    "df_emar_detail_evaluation = df_emar_detail[df_emar_detail['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88906ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_emar_detail_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_emar_detail.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342ddf7",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9764712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdaa916",
   "metadata": {},
   "source": [
    "Fields that have lots of null values:\n",
    "reason_for_no_barcode: drop\n",
    "prior_infusion_rate: impute with zeroes\n",
    "infusion_rate: impute with zeroes\n",
    "infusion_rate_adjustment: impute with 'N/A', then one hot encoding\n",
    "infusion_rate_adjustment_amount: impute with zeroes\n",
    "infusion_rate_unit: impute with 'N/A', then one hot encoding\n",
    "infusion_complete: impute with 'N/A', then one hot encoding\n",
    "completion_interval: impute with 0, then ordinal encoding \n",
    "new_iv_bag_hung: impute with N, then binary encoding \n",
    "\n",
    "Text data to remove but maybe consider later:\n",
    "product_description, product_description_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94417ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.drop(columns=['reason_for_no_barcode']) # Too hard to encode, adds not much value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0s\n",
    "df_emar_detail['prior_infusion_rate'] = df_emar_detail['prior_infusion_rate'].fillna(0)\n",
    "df_emar_detail['infusion_rate'] = df_emar_detail['infusion_rate'].fillna(0)\n",
    "df_emar_detail['infusion_rate_adjustment_amount'] = df_emar_detail['infusion_rate_adjustment_amount'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_emar_detail['infusion_rate_adjustment'] = df_emar_detail['infusion_rate_adjustment'].fillna('N/A')\n",
    "df_emar_detail['infusion_rate_unit'] = df_emar_detail['infusion_rate_unit'].fillna('N/A')\n",
    "df_emar_detail['infusion_complete'] = df_emar_detail['infusion_complete'].fillna('N/A')\n",
    "df_emar_detail = pd.get_dummies(df_emar_detail, columns=['infusion_rate_adjustment','infusion_complete',\n",
    "                                                         'infusion_rate_unit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abb0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].fillna(0)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('PRN', 0)\n",
    "#Converting all the intervals to minutes\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 2 hours', 120)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 4 hours', 240)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 1 hour', 60)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 1.5 hours', 90)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 8 hours', 480)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 15 minutes', 15)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 12 hours', 720)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 30 minutes', 30)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 24 hours', 1140)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 1 minutes', 1)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 14 hours', 840)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 7 hours', 420)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 5 hours', 300)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 3 hours', 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['new_iv_bag_hung'] = df_emar_detail['new_iv_bag_hung'].fillna('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding\n",
    "df_emar_detail['new_iv_bag_hung'] = df_emar_detail['new_iv_bag_hung'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and one hot encode:\n",
    "# administration_type\n",
    "# barcode_type\n",
    "# complete_dose_not_given\n",
    "# dose_due_unit\n",
    "# dose_given_unit\n",
    "# will_remainder_of_dose_be_given\n",
    "# product_unit\n",
    "# product_code\n",
    "# route\n",
    "# side\n",
    "# site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009cfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['administration_type'] = df_emar_detail['administration_type'].fillna('N/A')\n",
    "df_emar_detail['barcode_type'] = df_emar_detail['barcode_type'].fillna('N/A')\n",
    "df_emar_detail['complete_dose_not_given'] = df_emar_detail['complete_dose_not_given'].fillna('N/A')\n",
    "df_emar_detail['dose_due_unit'] = df_emar_detail['dose_due_unit'].fillna('N/A')\n",
    "df_emar_detail['dose_given_unit'] = df_emar_detail['dose_given_unit'].fillna('N/A')\n",
    "df_emar_detail['will_remainder_of_dose_be_given'] = df_emar_detail['will_remainder_of_dose_be_given'].fillna('N/A')\n",
    "df_emar_detail['product_unit'] = df_emar_detail['product_unit'].fillna('N/A')\n",
    "df_emar_detail['product_code'] = df_emar_detail['product_code'].fillna('N/A')\n",
    "df_emar_detail['route'] = df_emar_detail['route'].fillna('N/A')\n",
    "df_emar_detail['side'] = df_emar_detail['side'].fillna('N/A')\n",
    "df_emar_detail['site'] = df_emar_detail['site'].fillna('N/A')\n",
    "df_emar_detail = pd.get_dummies(df_emar_detail, columns=['administration_type','barcode_type','complete_dose_not_given',\n",
    "                                                        'dose_due_unit','dose_given_unit',\n",
    "                                                        'will_remainder_of_dose_be_given','product_unit','product_code',\n",
    "                                                        'route','side','site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with zeroes:\n",
    "# dose_due and dose_given, but also need to deal with some of them being ranges\n",
    "# product_amount_given\n",
    "# restart_interval, then ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a159976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['product_amount_given'] = df_emar_detail['product_amount_given'].fillna(0)\n",
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].fillna(0)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].fillna(0)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].astype(str)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5037db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_middle_value(range_string):\n",
    "    if '-' in range_string:\n",
    "        start, end = map(float, range_string.split('-'))\n",
    "        return (start + end) / 2\n",
    "    else:\n",
    "        return range_string\n",
    "\n",
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].apply(find_middle_value)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].apply(find_middle_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('PRN', 0)\n",
    "#Converting all the intervals to minutes\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 2 hours', 120)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 4 hours', 240)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 1 hour', 60)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 30 minutes', 30)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 24 hours', 1140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N and map to binary encoding:\n",
    "# continued_infusion_in_other_location\n",
    "# non_formulary_visual_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['continued_infusion_in_other_location'] = df_emar_detail['continued_infusion_in_other_location'].fillna('N')\n",
    "df_emar_detail['non_formulary_visual_verification'] = df_emar_detail['non_formulary_visual_verification'].fillna('N')\n",
    "# Binary encoding\n",
    "df_emar_detail['continued_infusion_in_other_location'] = df_emar_detail['continued_infusion_in_other_location'].map({'Y': 1, 'N': 0})\n",
    "df_emar_detail['non_formulary_visual_verification'] = df_emar_detail['non_formulary_visual_verification'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7cf41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.drop(columns=['pharmacy_id']) # Contains NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.drop(columns=['emar_id']) # Practically unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace blanks with zero\n",
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].replace('___', 0)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].replace('___', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc48afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].astype(float)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff849a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A or 0\n",
    "# One hot encode the categorical features \n",
    "\n",
    "df_emar_detail['product_description'] = df_emar_detail['product_description'].fillna('N/A')\n",
    "df_emar_detail['product_description_other'] = df_emar_detail['product_description_other'].fillna('N/A')\n",
    "df_emar_detail['parent_field_ordinal'] = df_emar_detail['parent_field_ordinal'].fillna(0)\n",
    "df_emar_detail = pd.get_dummies(df_emar_detail, columns=['product_description_other','product_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.merge(df_los_subject, on='subject_id', how='left')\n",
    "df_emar_detail = df_emar_detail.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b55267",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e5d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_emar_detail.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_emar_detail['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ff044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeae1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_emar_detail = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_emar_detail.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac029b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_emar_detail.joblib')\n",
    "dump(random_forest_emar_detail, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea959e93",
   "metadata": {},
   "source": [
    "### hcpcsevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/hcpcsevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_hcpcsevents = pd.read_csv(full_path)\n",
    "\n",
    "# Contains info for 18 different patients\n",
    "# d_hcpcs has longer descriptions (connected by code) but no other useful info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454541cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_hcpcsevents_training = df_hcpcsevents[~df_hcpcsevents['subject_id'].isin(evaluation_patients)]\n",
    "df_hcpcsevents_evaluation = df_hcpcsevents[df_hcpcsevents['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c60f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_hcpcsevents_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_hcpcsevents_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ace286",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe87144",
   "metadata": {},
   "source": [
    "To drop: subject_id, chartdate, hcpcs_cd (code that links to longer description in d_hcpcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3746266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = df_hcpcsevents_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a249f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = df_hcpcsevents.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c728f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature for days_since_admission using chartdate - admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_hcpcsevents['chartdate'] = pd.to_datetime(df_hcpcsevents['chartdate'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_hcpcsevents = df_hcpcsevents.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# Discard the time part and keep only the date\n",
    "df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_hcpcsevents['days_since_admission'] = df_hcpcsevents['chartdate'] - df_hcpcsevents['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_hcpcsevents['days_since_admission'] = df_hcpcsevents['days_since_admission'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents['days_since_admission'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = df_hcpcsevents.drop(columns=['subject_id','chartdate','hcpcs_cd'])\n",
    "# Not enough samples to include code as after encoding there would be a lot more features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617e9ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = pd.get_dummies(df_hcpcsevents, columns=['short_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdec64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = df_hcpcsevents.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_hcpcsevents = df_hcpcsevents.drop(columns=['hadm_id', 'admittime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9231ca",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fe002",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_hcpcsevents.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_hcpcsevents['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4273d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 13 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f96ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to integers\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].str.split().str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd8a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 9\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(data)\n",
    "data = svd.transform(data)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f8f4b9",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_hcpcsevents = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_hcpcsevents.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce85f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_hcpcsevents.joblib')\n",
    "dump(random_forest_hcpcsevents, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf558c",
   "metadata": {},
   "source": [
    "### labevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0400634",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/labevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_labevents = pd.read_csv(full_path)\n",
    "\n",
    "# Information regarding 252 different admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcfdd3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_labevents_training = df_labevents[~df_labevents['subject_id'].isin(evaluation_patients)]\n",
    "# df_labevents_evaluation = df_labevents[df_labevents['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_labevents_training = df_labevents[~df_labevents['hadm_id'].isin(evaluation_admissions)]\n",
    "df_labevents_evaluation = df_labevents[df_labevents['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb07e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_labevents_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_labevents_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0681f",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d89e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = df_labevents_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab164795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = df_labevents.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1652d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents['value'] = pd.to_numeric(df_labevents['value'], errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75db51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature for days_since_admission using charttime - admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_labevents['charttime'] = pd.to_datetime(df_labevents['charttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_labevents = df_labevents.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_labevents['days_since_admission'] = df_labevents['charttime'] - df_labevents['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_labevents['days_since_admission'] = df_labevents['days_since_admission'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6297f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add storetime - charttime feature called delay\n",
    "\n",
    "# Convert to datetime\n",
    "df_labevents['storetime'] = pd.to_datetime(df_labevents['storetime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_labevents['delay'] = df_labevents['storetime'] - df_labevents['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_labevents['delay'] = df_labevents['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6411f4",
   "metadata": {},
   "source": [
    "Drop: labevent_id, subject_id, order_provider_id (too many Null), charttime, storetime, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ac09832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = df_labevents.drop(columns=['labevent_id','subject_id','order_provider_id','charttime','storetime','comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d171a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For flag make abnormal = 1 and fill Null with 0\n",
    "df_labevents['flag'] = df_labevents['flag'].fillna(0)\n",
    "df_labevents['flag'] = df_labevents['flag'].replace('abnormal', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b4de7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For priority fill Null with N/A and then one hot encode\n",
    "df_labevents['priority'] = df_labevents['priority'].fillna('N/A')\n",
    "df_labevents = pd.get_dummies(df_labevents, columns=['priority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44d13296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = pd.get_dummies(df_labevents, columns=['valueuom','specimen_id','itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e790afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with null values \n",
    "df_labevents = df_labevents.dropna()\n",
    "# Reduced from 107727 rows to 66660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a7e151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = df_labevents.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_labevents = df_labevents.drop(columns=['hadm_id', 'admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e38edffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_labevents.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_labevents['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4f2b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bookmark \n",
    "\n",
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'labevents_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bf33c",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1b6c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['delay']= data['delay'].astype(str)\n",
    "data['delay']= data['delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03bcc0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to integers\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].str.split().str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46621a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_labevents = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_labevents.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d118246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_labevents.joblib')\n",
    "dump(random_forest_labevents, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a115fa4f",
   "metadata": {},
   "source": [
    "### microbiologyevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0748548",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/microbiologyevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_microbio = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761931ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_microbio_training = df_microbio[~df_microbio['subject_id'].isin(evaluation_patients)]\n",
    "# df_microbio_evaluation = df_microbio[df_microbio['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_microbio_training = df_microbio[~df_microbio['hadm_id'].isin(evaluation_admissions)]\n",
    "df_microbio_evaluation = df_microbio[df_microbio['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_microbio_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_microbio_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebc10e",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c24b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio = df_microbio_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio = df_microbio.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make days_since_admission using charttime \n",
    "\n",
    "# Convert to datetime\n",
    "df_microbio['charttime'] = pd.to_datetime(df_microbio['charttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_microbio = df_microbio.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_microbio['days_since_admission'] = df_microbio['charttime'] - df_microbio['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_microbio['days_since_admission'] = df_microbio['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_microbio = df_microbio.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7da974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add storetime - charttime feature (call it delay)\n",
    "\n",
    "# Convert to datetime\n",
    "df_microbio['storetime'] = pd.to_datetime(df_microbio['storetime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_microbio['delay'] = df_microbio['storetime'] - df_microbio['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_microbio['delay'] = df_microbio['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15ea61",
   "metadata": {},
   "source": [
    "Drop: microevent_id, subject_id, chartdate, charttime, test_seq, storedate, storetime, test_name and org_itemid (since info in name), quantity, ab_name, comments, micro_specimen_id (unique identifier for sample as some measurements are made on the same sample)\n",
    "Keep but categorical: order_provider_id, spec_type_desc, dilution_text, dilution_comparison\n",
    "Impute null with 0: order_provider_id, org_itemid, isolate_num, ab_itemid, dilution_value\n",
    "Impute with N/A and then one hot encode: interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33336ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "# spec_itemid , test_itemid\n",
    "df_microbio = df_microbio.drop(columns=['microevent_id','subject_id','chartdate','charttime','test_seq','storedate',\n",
    "                                       'storetime','quantity','comments','ab_itemid',\n",
    "                                       'spec_itemid','test_itemid','org_itemid','micro_specimen_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bf761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute null with 0: order_provider_id, org_itemid, isolate_num, ab_itemid, dilution_value\n",
    "df_microbio['order_provider_id'] = df_microbio['order_provider_id'].fillna(0)\n",
    "df_microbio['isolate_num'] = df_microbio['isolate_num'].fillna(0)\n",
    "df_microbio['dilution_value'] = df_microbio['dilution_value'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8dd1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and then one hot encode: interpretation\n",
    "# encode test_name, ab_name\n",
    "\n",
    "df_microbio['interpretation'] = df_microbio['interpretation'].fillna('N/A')\n",
    "df_microbio['test_name'] = df_microbio['test_name'].fillna('N/A')\n",
    "df_microbio['ab_name'] = df_microbio['ab_name'].fillna('N/A')\n",
    "df_microbio['org_name'] = df_microbio['org_name'].fillna('None')\n",
    "df_microbio = pd.get_dummies(df_microbio, columns=['org_name','interpretation','ab_name','test_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d071a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep but categorical: order_provider_id, spec_type_desc, dilution_text, dilution_comparison\n",
    "df_microbio = pd.get_dummies(df_microbio, columns=['order_provider_id','spec_type_desc','dilution_text',\n",
    "                                                  'dilution_comparison'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4eac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio = df_microbio.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea23bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio = df_microbio.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_microbio = df_microbio.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff8446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_microbio.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_microbio['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ba58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'microbio_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582dfec",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55195ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['delay']= data['delay'].astype(str)\n",
    "data['delay']= data['delay'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_microbio = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_microbio.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9cb85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_microbio.joblib')\n",
    "dump(random_forest_microbio, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98518c",
   "metadata": {},
   "source": [
    "### patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/patients.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_patients = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517eba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_patients_training = df_patients[~df_patients['subject_id'].isin(evaluation_patients)]\n",
    "df_patients_evaluation = df_patients[df_patients['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa920653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_patients_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_patients_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2b03e",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = df_patients_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = df_patients.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b833c",
   "metadata": {},
   "source": [
    "Drop: anchor_year\n",
    "Encode: gender (M to 0 and F to 1), dod (change all to 1 and nulls to 0)\n",
    "Dummies: anchor_year_group  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "df_patients = df_patients.drop(columns=['anchor_year','dod']) \n",
    "# Since this is the shifted year and dod is an outcome value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83110fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode: gender (M to 0 and F to 1), dod (change all to 1 and nulls to 0)\n",
    "df_patients['gender'] = df_patients['gender'].replace('M', 0)\n",
    "df_patients['gender'] = df_patients['gender'].replace('F', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b068e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies: anchor_year_group  \n",
    "df_patients = pd.get_dummies(df_patients, columns=['anchor_year_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fe917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = df_patients.merge(df_los_subject, on='subject_id', how='left')\n",
    "df_patients = df_patients.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_patients.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_patients['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f1e9cc",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462651d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_patients = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_patients.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45b68a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_patients.joblib')\n",
    "dump(random_forest_patients, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b7214d",
   "metadata": {},
   "source": [
    "### pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/pharmacy.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_pharmacy = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70590ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_pharmacy_training = df_pharmacy[~df_pharmacy['subject_id'].isin(evaluation_patients)]\n",
    "df_pharmacy_evaluation = df_pharmacy[df_pharmacy['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_pharmacy_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_pharmacy_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe27b25",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990d555",
   "metadata": {},
   "source": [
    "drop: subject_id, pharmacy_id, poe_id, starttime, stoptime, entertime, verifiedtime, disp_sched, basal_rate, one_hr_max,\n",
    "expirationdate, fill_quantity\n",
    "Encode: proc_type, status\n",
    "Impute with N/A and encode: infusion_type, sliding_scale, duration_interval, expiration_unit, dispensation, medication, route, frequency\n",
    "Impute with 0: lockout_interval, doses_per_24_hrs, duration, expiration_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pharmacy = df_pharmacy_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a92caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pharmacy = df_pharmacy.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e12a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoptime-starttime for a duration feature\n",
    "\n",
    "# Convert to datetime\n",
    "df_pharmacy['stoptime'] = pd.to_datetime(df_pharmacy['stoptime'], format='%Y/%m/%d %H:%M')\n",
    "df_pharmacy['starttime'] = pd.to_datetime(df_pharmacy['starttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "\n",
    "df_pharmacy['medication_duration'] = df_pharmacy['stoptime'] - df_pharmacy['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_pharmacy['medication_duration'] = df_pharmacy['medication_duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f20555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifiedtime - entertime for verification_delay feature \n",
    "\n",
    "# Convert to datetime\n",
    "df_pharmacy['verifiedtime'] = pd.to_datetime(df_pharmacy['verifiedtime'], format='%Y/%m/%d %H:%M')\n",
    "df_pharmacy['entertime'] = pd.to_datetime(df_pharmacy['entertime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_pharmacy['verification_delay'] = df_pharmacy['verifiedtime'] - df_pharmacy['entertime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_pharmacy['verification_delay'] = df_pharmacy['verification_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b539012",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_value = [0] \n",
    "\n",
    "# Fill null values with the list\n",
    "df_pharmacy['disp_sched'] = df_pharmacy['disp_sched'].fillna(pd.Series([fill_value]*len(df_pharmacy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all categories to strings\n",
    "df_pharmacy['disp_sched'] = df_pharmacy['disp_sched'].apply(lambda x: [str(item) for item in x])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "encoded_feature = pd.DataFrame(mlb.fit_transform(df_pharmacy['disp_sched']),\n",
    "                               columns=mlb.classes_,\n",
    "                               index=df_pharmacy.index)\n",
    "\n",
    "df_pharmacy = pd.concat([df_pharmacy, encoded_feature], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af47da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_pharmacy = df_pharmacy.drop(columns=['subject_id','pharmacy_id','poe_id','starttime','stoptime','entertime',\n",
    "                                       'verifiedtime','expirationdate', 'fill_quantity','disp_sched'])\n",
    "# expiration date and fill quantity are all empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83864927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode: proc_type, status\n",
    "df_pharmacy = pd.get_dummies(df_pharmacy, columns=['proc_type','status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6104f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_pharmacy['infusion_type'] = df_pharmacy['infusion_type'].fillna('N/A')\n",
    "df_pharmacy['sliding_scale'] = df_pharmacy['sliding_scale'].fillna('N/A')\n",
    "df_pharmacy['duration_interval'] = df_pharmacy['duration_interval'].fillna('N/A')\n",
    "df_pharmacy['expiration_unit'] = df_pharmacy['expiration_unit'].fillna('N/A')\n",
    "df_pharmacy['dispensation'] = df_pharmacy['dispensation'].fillna('N/A')\n",
    "df_pharmacy['medication'] = df_pharmacy['medication'].fillna('N/A')\n",
    "df_pharmacy['route'] = df_pharmacy['route'].fillna('N/A')\n",
    "df_pharmacy['frequency'] = df_pharmacy['frequency'].fillna('N/A')\n",
    "df_pharmacy = pd.get_dummies(df_pharmacy, columns=['infusion_type','sliding_scale','duration_interval','expiration_unit',\n",
    "                                                  'dispensation','medication','route','frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0: lockout_interval, doses_per_24_hrs, duration, expiration_value\n",
    "df_pharmacy['lockout_interval'] = df_pharmacy['lockout_interval'].fillna(0)\n",
    "df_pharmacy['doses_per_24_hrs'] = df_pharmacy['doses_per_24_hrs'].fillna(0)\n",
    "df_pharmacy['expiration_value'] = df_pharmacy['expiration_value'].fillna(0)\n",
    "df_pharmacy['basal_rate'] = df_pharmacy['basal_rate'].fillna(0)\n",
    "df_pharmacy['one_hr_max'] = df_pharmacy['one_hr_max'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pharmacy = df_pharmacy.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_pharmacy = df_pharmacy.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_pharmacy.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_pharmacy['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd0910",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb4e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "\n",
    "data['medication_duration']= data['medication_duration'].astype(str)\n",
    "data['medication_duration']= data['medication_duration'].apply(convert_to_days)\n",
    "# Convert strings to integers\n",
    "data['verification_delay'] = data['verification_delay'].astype(str)\n",
    "data['verification_delay'] = data['verification_delay'].str.split().str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258064e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7106be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_pharmacy = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_pharmacy.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0b5e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_pharmacy.joblib')\n",
    "dump(random_forest_pharmacy, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6d159",
   "metadata": {},
   "source": [
    "### poe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/poe.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_poe = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_poe_training = df_poe[~df_poe['subject_id'].isin(evaluation_patients)]\n",
    "# df_poe_evaluation = df_poe[df_poe['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_poe_training = df_poe[~df_poe['hadm_id'].isin(evaluation_admissions)]\n",
    "df_poe_evaluation = df_poe[df_poe['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_poe_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_poe_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3715f",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c443ac6",
   "metadata": {},
   "source": [
    "To drop: poe_id, subject_id, ordertime, discontinue_of_poe_id, discontinued_by_poe_id (all unique), order_status (all inactive)\n",
    "Encode: order_type, transaction_type\n",
    "Impute with N/A and then encode: order_subtype, order_provider_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe = df_poe_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe = df_poe.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fbb0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a feature of ordertime - admittime for days_since_admission\n",
    "\n",
    "# Convert to datetime\n",
    "df_poe['ordertime'] = pd.to_datetime(df_poe['ordertime'], format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_poe = df_poe.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_poe['days_since_admission'] = df_poe['ordertime'] - df_poe['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_poe['days_since_admission'] = df_poe['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_poe = df_poe.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ddc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_poe = df_poe.drop(columns=['poe_id','subject_id','ordertime','discontinue_of_poe_id','discontinued_by_poe_id',\n",
    "                                       'order_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768bce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_poe = pd.get_dummies(df_poe, columns=['order_type','transaction_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cdeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_poe['order_subtype'] = df_poe['order_subtype'].fillna('N/A')\n",
    "df_poe['order_provider_id'] = df_poe['order_provider_id'].fillna('N/A')\n",
    "df_poe = pd.get_dummies(df_poe, columns=['order_subtype','order_provider_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292bf145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe = df_poe.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_poe = df_poe.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed77a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_poe.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_poe['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'poe_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c3504",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_poe = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_poe.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e282d35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_poe.joblib')\n",
    "dump(random_forest_poe, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc258098",
   "metadata": {},
   "source": [
    "### poe_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/poe_detail.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_poe_detail = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_poe_detail_training = df_poe_detail[~df_poe_detail['subject_id'].isin(evaluation_patients)]\n",
    "df_poe_detail_evaluation = df_poe_detail[df_poe_detail['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_poe_detail_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_poe_detail_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7c8b0",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22396996",
   "metadata": {},
   "source": [
    "To drop: poe_id\n",
    "Encode: field_name, field_value\n",
    "subject_id for los and then drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe_detail = df_poe_detail_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe_detail = df_poe_detail.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2085bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_poe_detail = df_poe_detail.drop(columns=['poe_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_poe_detail = pd.get_dummies(df_poe_detail, columns=['field_name','field_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67091bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe_detail = df_poe_detail.merge(df_los_subject, on='subject_id', how='left')\n",
    "df_poe_detail = df_poe_detail.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_poe_detail.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_poe_detail['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'poe_detail_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274bd99e",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff955d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c510aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_poe_detail = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_poe_detail.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aaaa05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_poe_detail.joblib')\n",
    "dump(random_forest_poe_detail, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc8383",
   "metadata": {},
   "source": [
    "### chartevents - memory issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/chartevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_chart = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_chart_training = df_chart[~df_chart['subject_id'].isin(evaluation_patients)]\n",
    "# df_chart_evaluation = df_chart[df_chart['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_chart_training = df_chart[~df_chart['hadm_id'].isin(evaluation_admissions)]\n",
    "df_chart_evaluation = df_chart[df_chart['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_chart_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_chart_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad103150",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6dfd0b",
   "metadata": {},
   "source": [
    "Drop: subject_id, charttime, storetime, stay_id, caregiver_id (the person who documented the data)\n",
    "Encode: value,itemid\n",
    "Impute with 0: valuenum, warning\n",
    "Impute with N/A and encode: valueuom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chart = df_chart_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677dabbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chart = df_chart.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a days_since_admission feature of charttime-admittime \n",
    "\n",
    "# Convert to datetime\n",
    "df_chart['charttime'] = pd.to_datetime(df_chart['charttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_chart = df_chart.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_chart['days_since_admission'] = df_chart['charttime'] - df_chart['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_chart['days_since_admission'] = df_chart['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_chart = df_chart.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782df4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a delay feature of storetime-charttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_chart['storetime'] = pd.to_datetime(df_chart['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_chart['delay'] = df_chart['storetime'] - df_chart['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_chart['delay'] = df_chart['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef011046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_chart = df_chart.drop(columns=['subject_id','charttime','storetime', 'stay_id','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28dbbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_chart['valueuom'] = df_chart['valueuom'].fillna('N/A')\n",
    "df_chart = pd.get_dummies(df_chart, columns=['valueuom','value','itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eadfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0\n",
    "df_chart['valuenum'] = df_chart['valuenum'].fillna(0)\n",
    "df_chart['warning'] = df_chart['warning'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5019065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chart = df_chart.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_chart = df_chart.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329bb589",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_chart.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_chart['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7efa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'chart_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85069296",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "df_chart['los'] = df_chart['los'].astype(str)\n",
    "# df_chart.fillna(0, inplace=True)\n",
    "df_chart.loc[~df_chart['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "df_chart['los'] = df_chart['los'].apply(convert_to_days)\n",
    "df_chart['delay']= df_chart['delay'].astype(str)\n",
    "df_chart['delay']= df_chart['delay'].apply(convert_to_days)\n",
    "df_chart['days_since_admission'] = df_chart['days_since_admission'].astype(str)\n",
    "df_chart['days_since_admission'] = df_chart['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66eb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the trained models\n",
    "trained_models = []\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "# # Split the DataFrame into chunks and train the Random Forest model on each chunk\n",
    "# for i in range(0, len(df_chart), chunk_size):\n",
    "#     chunk = df_chart.iloc[i:i+chunk_size]\n",
    "\n",
    "#     # Split the chunk into features and target variable\n",
    "#     X = chunk.drop(columns=['los'])\n",
    "#     y = chunk['los'] \n",
    "\n",
    "#     # Train the on the current chunk\n",
    "#     random_forest_chart.partial_fit(X, y)  \n",
    "\n",
    "# # After processing all chunks, the model will be trained on the entire dataset\n",
    "\n",
    "\n",
    "# Iterate over the DataFrame in chunks and train the Random Forest regressor on each chunk separately\n",
    "for i in range(0, len(df_chart), chunk_size):\n",
    "    chunk = df_chart.iloc[i:i+chunk_size]\n",
    "\n",
    "    # Split the chunk into features and target variable\n",
    "    X_chunk = chunk.drop(columns=['los'])  # Adjust the column name\n",
    "    y_chunk = chunk['los']  # Adjust the column name\n",
    "\n",
    "    # Initialize a new Random Forest regressor for each chunk\n",
    "    random_forest_chart = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)\n",
    "\n",
    "    # Train the Random Forest regressor on the current chunk\n",
    "    random_forest_chart.fit(X_chunk, y_chunk)\n",
    "\n",
    "    # Store the trained model\n",
    "    trained_models.append(random_forest_chart)\n",
    "\n",
    "# After processing all chunks, you can combine the trained models if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef274dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting duration strings to floats\n",
    "\n",
    "# target['los'] = target['los'].astype(str)\n",
    "# target.fillna(0, inplace=True)\n",
    "# target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "# target['los'] = target['los'].apply(convert_to_days)\n",
    "# data['delay']= data['delay'].astype(str)\n",
    "# data['delay']= data['delay'].apply(convert_to_days)\n",
    "# data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "# data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking to load the dataset in smaller batches and process each sequentially, as the full df is 20GB \n",
    "# Model keeps getting updated with new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random forest model\n",
    "\n",
    "# # Convert DataFrame to 1D array using ravel()\n",
    "# target = target.values.ravel()\n",
    "\n",
    "# # Initialize and fit the Random Forest regression model\n",
    "# random_forest_chart = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)\n",
    "# random_forest_chart.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4441b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "# model_file = os.path.join(output_folder, 'random_forest_chart.joblib')\n",
    "# dump(random_forest_chart, model_file)\n",
    "\n",
    "for i, model in enumerate(trained_models):\n",
    "    model_file = os.path.join(output_folder, f\"random_forest_chart{i}.joblib\")\n",
    "    joblib.dump(model, model_file)\n",
    "    \n",
    "    \n",
    "# Makes 519 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65043bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To combine the models\n",
    "\n",
    "# # Combine the predictions from all trained models\n",
    "# def combine_predictions(models, X):\n",
    "#     predictions = np.zeros((X.shape[0], len(models)))\n",
    "#     for i, model in enumerate(models):\n",
    "#         predictions[:, i] = model.predict(X)\n",
    "#     return np.mean(predictions, axis=1)\n",
    "\n",
    "# # Assuming your data for prediction is in a DataFrame named test_df\n",
    "# # You can replace this with your actual DataFrame\n",
    "# # test_df = ...\n",
    "\n",
    "# # Combine predictions from all trained models\n",
    "# combined_predictions = combine_predictions(trained_models, test_df.drop(columns=['target_column']))\n",
    "\n",
    "# # Final prediction is the average of predictions from all models\n",
    "# final_prediction = np.mean(combined_predictions)\n",
    "\n",
    "# print(\"Final prediction:\", final_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d7d4f",
   "metadata": {},
   "source": [
    "### prescriptions - BOOKMARK - still to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/prescriptions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_prescriptions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b59c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_prescriptions_training = df_prescriptions[~df_prescriptions['subject_id'].isin(evaluation_patients)]\n",
    "df_prescriptions_evaluation = df_prescriptions[df_prescriptions['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ac565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_prescriptions_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_prescriptions_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26111474",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f57355",
   "metadata": {},
   "source": [
    "Drop na and encode: dose_val_rx, form_val_disp, order_provider_id\n",
    "Drop: subject_id, pharmacy_id, starttime, stoptime, form_rx (mostly null), poe_id\n",
    "Impute with N/A and encode: formulary_drug_cd, gsn, prod_strength, route\n",
    "Encode: drug_type, drug, dose_unit_rx, form_unit_disp\n",
    "Impute with 0: doses_per_24_hrs\n",
    "\n",
    "Drop rows with na\n",
    "\n",
    "order_provider_id\n",
    "Was going to impute with N/A and encode but going to drop as too many features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbf5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions = df_prescriptions_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde390b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions = df_prescriptions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature of stoptime-starttime called duration \n",
    "\n",
    "# Convert to datetime\n",
    "df_prescriptions['stoptime'] = pd.to_datetime(df_prescriptions['stoptime'], format='%Y/%m/%d %H:%M')\n",
    "df_prescriptions['starttime'] = pd.to_datetime(df_prescriptions['starttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_prescriptions['duration'] = df_prescriptions['stoptime'] - df_prescriptions['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_prescriptions['duration'] = df_prescriptions['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_prescriptions.dropna(subset=['dose_val_rx', 'form_val_disp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_prescriptions = df_prescriptions.drop(columns=['subject_id','pharmacy_id','starttime','stoptime','form_rx','poe_id',\n",
    "                                                 'order_provider_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_prescriptions['formulary_drug_cd'] = df_prescriptions['formulary_drug_cd'].fillna('N/A')\n",
    "df_prescriptions['gsn'] = df_prescriptions['gsn'].fillna('N/A')\n",
    "df_prescriptions['prod_strength'] = df_prescriptions['prod_strength'].fillna('N/A')\n",
    "df_prescriptions['route'] = df_prescriptions['route'].fillna('N/A')\n",
    "\n",
    "# Impute with 0\n",
    "df_prescriptions['ndc'] = df_prescriptions['ndc'].fillna(0)\n",
    "\n",
    "df_prescriptions = pd.get_dummies(df_prescriptions, columns=['formulary_drug_cd','gsn','prod_strength',\n",
    "                                                            'route','drug_type','drug','dose_unit_rx','form_unit_disp',\n",
    "                                                            'dose_val_rx','form_val_disp','ndc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f353de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions['doses_per_24_hrs'] = df_prescriptions['doses_per_24_hrs'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with null values \n",
    "df_prescriptions = df_prescriptions.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions = df_prescriptions.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_prescriptions = df_prescriptions.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a96edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_prescriptions.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_prescriptions['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb833c",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].astype(str)\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764133d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 4890 to 2874 or less\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 2874\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(data)\n",
    "data = svd.transform(data)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b951673",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f04135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_prescriptions = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_prescriptions.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027f789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_prescriptions.joblib')\n",
    "dump(random_forest_prescriptions, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1b676",
   "metadata": {},
   "source": [
    "### procedures_icd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/procedures_icd.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_procedures = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128614d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_procedures_training = df_procedures[~df_procedures['subject_id'].isin(evaluation_patients)]\n",
    "df_procedures_evaluation = df_procedures[df_procedures['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_procedures_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_procedures_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b88915",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1546e2",
   "metadata": {},
   "source": [
    "Drop: subject_id, chartdate\n",
    "Encode: icd_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09787f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedures = df_procedures_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedures = df_procedures.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a feature called days_since_admission of chartdate - admitdate\n",
    "\n",
    "# Convert to datetime\n",
    "df_procedures['chartdate'] = pd.to_datetime(df_procedures['chartdate'], format='%Y-%m-%d')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_procedures = df_procedures.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "df_procedures['admittime'] = df_procedures['admittime'].dt.date\n",
    "df_procedures['chartdate'] = df_procedures['chartdate'].dt.date\n",
    "\n",
    "df_procedures['days_since_admission'] = df_procedures['chartdate'] - df_procedures['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_procedures['days_since_admission'] = df_procedures['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_procedures = df_procedures.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48cd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_procedures = df_procedures.drop(columns=['subject_id','chartdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_procedures = pd.get_dummies(df_procedures, columns=['icd_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedures = df_procedures.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_procedures = df_procedures.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e79cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_procedures.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_procedures['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'procedures_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0851e5a",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].astype(str)\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].str.split().str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e12d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 355 to 115\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 115\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(data)\n",
    "data = svd.transform(data)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb200646",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_procedures = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_procedures.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11290da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_procedures.joblib')\n",
    "dump(random_forest_procedures, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff3d66",
   "metadata": {},
   "source": [
    "### services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcfb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/services.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_services = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_services_training = df_services[~df_services['subject_id'].isin(evaluation_patients)]\n",
    "df_services_evaluation = df_services[df_services['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e653df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_services_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_services_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0aef0",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a351ce",
   "metadata": {},
   "source": [
    "Drop: subject_id, transfertime\n",
    "Impute with N/A and encode: prev_service\n",
    "Encode: curr_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_services = df_services_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_services = df_services.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d6c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature called days_since_admission using transfertime-admittime \n",
    "\n",
    "# Convert to datetime\n",
    "df_services['transfertime'] = pd.to_datetime(df_services['transfertime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_services = df_services.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "df_services['days_since_admission'] = df_services['transfertime'] - df_services['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_services['days_since_admission'] = df_services['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_services = df_services.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88114e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_services = df_services.drop(columns=['subject_id','transfertime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cff571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_services['prev_service'] = df_services['prev_service'].fillna('N/A')\n",
    "df_services = pd.get_dummies(df_services, columns=['prev_service','curr_service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f71192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_services = df_services.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_services = df_services.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_services.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_services['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e788c",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a345db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b24e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_services = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_services.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276468d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_services.joblib')\n",
    "dump(random_forest_services, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e18895",
   "metadata": {},
   "source": [
    "### transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/transfers.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_transfers = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9af29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_transfers_training = df_transfers[~df_transfers['subject_id'].isin(evaluation_patients)]\n",
    "df_transfers_evaluation = df_transfers[df_transfers['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_transfers_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_transfers_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738cd48",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06e96c",
   "metadata": {},
   "source": [
    "Drop: subject_id, transfer_id, intime, outtime\n",
    "Encode: eventtype\n",
    "Impute with N/A and encode: careunit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787262c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers = df_transfers_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d568022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers = df_transfers.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a days_since_admission feature of intime-admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_transfers['intime'] = pd.to_datetime(df_transfers['intime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_transfers = df_transfers.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "df_transfers['days_since_admission'] = df_transfers['intime'] - df_transfers['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_transfers['days_since_admission'] = df_transfers['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_transfers = df_transfers.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83088288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature of outtime-intime \n",
    "\n",
    "# Convert to datetime\n",
    "df_transfers['outtime'] = pd.to_datetime(df_transfers['outtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_transfers['duration'] = df_transfers['outtime'] - df_transfers['intime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_transfers['duration'] = df_transfers['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0916af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_transfers = df_transfers.drop(columns=['subject_id','transfer_id','intime','outtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_transfers['careunit'] = df_transfers['careunit'].fillna('N/A')\n",
    "df_transfers = pd.get_dummies(df_transfers, columns=['eventtype','careunit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers = df_transfers.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_transfers = df_transfers.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fb7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_transfers.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_transfers['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60688639",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'transfers_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf4b44",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238cdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_transfers = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_transfers.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2486e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_transfers.joblib')\n",
    "dump(random_forest_transfers, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c78e31c",
   "metadata": {},
   "source": [
    "### icustays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6dbf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/icustays.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_icustays = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fac1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_icustays_training = df_icustays[~df_icustays['subject_id'].isin(evaluation_patients)]\n",
    "df_icustays_evaluation = df_icustays[df_icustays['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bcfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_icustays_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_icustays_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9f450",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a3492",
   "metadata": {},
   "source": [
    "Drop: subject_id, stay_id, intime, outtime\n",
    "Encode: first_careunit, last_careunit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icustays = df_icustays_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icustays = df_icustays.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a feature called days_since_admission using intime-admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_icustays['intime'] = pd.to_datetime(df_icustays['intime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_icustays = df_icustays.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_icustays['days_since_admission'] = df_icustays['intime'] - df_icustays['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_icustays['days_since_admission'] = df_icustays['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_icustays = df_icustays.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26707941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_icustays = df_icustays.drop(columns=['subject_id','stay_id','intime','outtime'])\n",
    "\n",
    "# Rename los to icu_los\n",
    "df_icustays = df_icustays.rename(columns={'los': 'icu_los'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d564db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_icustays = pd.get_dummies(df_icustays, columns=['first_careunit','last_careunit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icustays = df_icustays.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_icustays = df_icustays.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_services.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_services['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845dd9c6",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_icustays = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_icustays.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b6240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_icustays.joblib')\n",
    "dump(random_forest_icustays, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39dcffd",
   "metadata": {},
   "source": [
    "### ingredientevents - Still to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/ingredientevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_ingredient = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d765746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_ingredient_training = df_ingredient[~df_ingredient['subject_id'].isin(evaluation_patients)]\n",
    "df_ingredient_evaluation = df_ingredient[df_ingredient['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c5a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_ingredient_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_ingredient_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750389ce",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c47b7e3",
   "metadata": {},
   "source": [
    "Drop: subject_id, starttime, endtime, storetime, orderid, originalamount, stay_id, caregiver_id\n",
    "Encode: amountuom, statusdescription, itemid\n",
    "Impute with 0: rate\n",
    "Impute with N/A and encode: rateuom, linkorderid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingredient = df_ingredient_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1305821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingredient = df_ingredient.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature of endtime-starttime \n",
    "\n",
    "# Convert to datetime\n",
    "df_ingredient['endtime'] = pd.to_datetime(df_ingredient['endtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_ingredient['starttime'] = pd.to_datetime(df_ingredient['starttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "df_ingredient['duration'] = df_ingredient['endtime'] - df_ingredient['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_ingredient['duration'] = df_ingredient['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d89914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a recording_delay feature of storetime-endtime\n",
    "\n",
    "# Convert to datetime\n",
    "df_ingredient['storetime'] = pd.to_datetime(df_ingredient['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_ingredient['recording_delay'] = df_ingredient['storetime'] - df_ingredient['endtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_ingredient['recording_delay'] = df_ingredient['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfeee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_ingredient = df_ingredient.drop(columns=['subject_id','starttime','endtime','storetime','orderid','originalamount',\n",
    "                                           'stay_id','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_ingredient['rateuom'] = df_ingredient['rateuom'].fillna('N/A')\n",
    "df_ingredient['linkorderid'] = df_ingredient['linkorderid'].fillna('N/A')\n",
    "df_ingredient = pd.get_dummies(df_ingredient, columns=['rateuom','amountuom','statusdescription','itemid','linkorderid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d16ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0\n",
    "df_ingredient['rate'] = df_ingredient['rate'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingredient = df_ingredient.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_ingredient = df_ingredient.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f125830",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb46d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_ingredient.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_ingredient['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 7727 to 4116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)\n",
    "data['recording_delay']= data['recording_delay'].astype(str)\n",
    "data['recording_delay']= data['recording_delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f57049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 4116\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(data)\n",
    "data = svd.transform(data)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319c7b6",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee05bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_ingredient = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_ingredient.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb621e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_ingredient.joblib')\n",
    "dump(random_forest_ingredient, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec07fa",
   "metadata": {},
   "source": [
    "### inputevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/inputevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_input = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f18672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_input_training = df_input[~df_input['subject_id'].isin(evaluation_patients)]\n",
    "df_input_evaluation = df_input[df_input['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae02e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_input_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_input_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dd8fa",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0430674",
   "metadata": {},
   "source": [
    "Drop: subject_id, starttime, endtime, storetime, orderid, linkorderid, continueinnextdept, stay_id, caregiver_id,\n",
    "totalamountuom\n",
    "Encode: amountuom, ordercategoryname, ordercomponenttypedescription, ordercategorydescription, statusdescription, itemid\n",
    "Impute with 0: rate, totalamount\n",
    "Impute with N/A and encode: rateuom, secondaryordercategoryname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_input_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_input.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bd393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature using endtime-starttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_input['endtime'] = pd.to_datetime(df_input['endtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_input['starttime'] = pd.to_datetime(df_input['starttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "df_input['duration'] = df_input['endtime'] - df_input['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_input['duration'] = df_input['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a recording_delay feature using storetime-endtime\n",
    "\n",
    "# Convert to datetime\n",
    "df_input['storetime'] = pd.to_datetime(df_input['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_input['recording_delay'] = df_input['storetime'] - df_input['endtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_input['recording_delay'] = df_input['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f8a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_input = df_input.drop(columns=['subject_id','stay_id','starttime','endtime','storetime','orderid','linkorderid',\n",
    "                                  'continueinnextdept','totalamountuom', 'stay_id','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497811f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_input['rateuom'] = df_input['rateuom'].fillna('N/A')\n",
    "df_input['secondaryordercategoryname'] = df_input['secondaryordercategoryname'].fillna('N/A')\n",
    "df_input = pd.get_dummies(df_input, columns=['rateuom','secondaryordercategoryname','amountuom','ordercategoryname',\n",
    "                                            'ordercomponenttypedescription','ordercategorydescription','statusdescription',\n",
    "                                            'itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0\n",
    "df_input['rate'] = df_input['rate'].fillna(0)\n",
    "df_input['totalamount'] = df_input['totalamount'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262805de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_input.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad373c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_input.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_input = df_input.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_input.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_input['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c378dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)\n",
    "data['recording_delay']= data['recording_delay'].astype(str)\n",
    "data['recording_delay']= data['recording_delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'input_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac550d",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d949aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_input = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_input.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd30cf9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_input.joblib')\n",
    "dump(random_forest_input, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6e8c2",
   "metadata": {},
   "source": [
    "### outputevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a960789",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/outputevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_output = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f40d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_output_training = df_output[~df_output['subject_id'].isin(evaluation_patients)]\n",
    "df_output_evaluation = df_output[df_output['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715bb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_output_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_output_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9a742",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2b956",
   "metadata": {},
   "source": [
    "Drop: subject_id, charttime, storetime, valueuom, stay_id, caregiver_id'\n",
    "Encode: itemid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d84b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e7fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a days_since_admission feature using charttime-admittime \n",
    "\n",
    "# Convert to datetime\n",
    "df_output['charttime'] = pd.to_datetime(df_output['charttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_output = df_output.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "df_output['days_since_admission'] = df_output['charttime'] - df_output['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_output['days_since_admission'] = df_output['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_output = df_output.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40805f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a recording_delay feature using storetime-charttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_output['storetime'] = pd.to_datetime(df_output['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_output['recording_delay'] = df_output['storetime'] - df_output['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_output['recording_delay'] = df_output['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_output = df_output.drop(columns=['subject_id','stay_id','charttime','storetime','storetime','valueuom','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b421a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode\n",
    "df_output = pd.get_dummies(df_output, columns=['itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_output = df_output.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b7665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_ingredient.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_ingredient['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['recording_delay']= data['recording_delay'].astype(str)\n",
    "data['recording_delay']= data['recording_delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f066e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1b74ed",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ac673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_output = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_output.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c800701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_output.joblib')\n",
    "dump(random_forest_output, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35bf10",
   "metadata": {},
   "source": [
    "### procedureevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/procedureevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_procedure_events = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_procedure_events_training = df_procedure_events[~df_procedure_events['subject_id'].isin(evaluation_patients)]\n",
    "df_procedure_events_evaluation = df_procedure_events[df_procedure_events['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_procedure_events_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_procedure_events_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe76d7",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca455d0",
   "metadata": {},
   "source": [
    "Drop: subject_id, starttime, endtime, storetime, orderid, linkorderid, continueinnextdept, stay_id, caregiver_id\n",
    "Encode: valueuom, ordercategoryname, ordercategorydescription, statusdescription, itemid\n",
    "Impute with N/A and encode: location, locationcategory\n",
    "MAKE DURATION FEATURE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedure_events = df_procedure_events_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedure_events = df_procedure_events.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed212c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature using endtime-starttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_procedure_events['endtime'] = pd.to_datetime(df_procedure_events['endtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_procedure_events['starttime'] = pd.to_datetime(df_procedure_events['starttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "df_procedure_events['duration'] = df_procedure_events['endtime'] - df_procedure_events['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_procedure_events['duration'] = df_procedure_events['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d0cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a recording_delay feature using storetime-endtime\n",
    "\n",
    "# Convert to datetime\n",
    "df_procedure_events['storetime'] = pd.to_datetime(df_procedure_events['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_procedure_events['recording_delay'] = df_procedure_events['storetime'] - df_procedure_events['endtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_procedure_events['recording_delay'] = df_procedure_events['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_procedure_events = df_procedure_events.drop(columns=['subject_id','stay_id','starttime','endtime','storetime','orderid',\n",
    "                                                        'linkorderid','continueinnextdept','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_procedure_events['location'] = df_procedure_events['location'].fillna('N/A')\n",
    "df_procedure_events['locationcategory'] = df_procedure_events['locationcategory'].fillna('N/A')\n",
    "df_procedure_events = pd.get_dummies(df_procedure_events, columns=['location','locationcategory','valueuom',\n",
    "                                                                   'ordercategoryname','ordercategorydescription',\n",
    "                                                                   'statusdescription','itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02353eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedure_events = df_procedure_events.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_procedure_events = df_procedure_events.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_procedure_events.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_procedure_events['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f20bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)\n",
    "data['recording_delay']= data['recording_delay'].astype(str)\n",
    "data['recording_delay']= data['recording_delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'procedure_events_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eca4fd",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60409183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_procedure_events = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_procedure_events.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f044da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_procedure_events.joblib')\n",
    "dump(random_forest_procedure_events, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
