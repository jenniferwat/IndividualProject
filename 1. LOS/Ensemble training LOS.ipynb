{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374a49e1",
   "metadata": {},
   "source": [
    "### Training of LOS learners for ensemble - full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442c9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE\n",
    "# might need further dimension reduction because I'm removing a lot of data by filtering out the 20 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fcad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.24.3\n",
      "Pandas version: 1.5.3\n",
      "Scikit version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "# External libraries for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "#To render graphs within notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib \n",
    "import os\n",
    "\n",
    "# Versions of libraries\n",
    "print(\"Numpy version: {}\".format(np.__version__))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Scikit version: {}\".format(sk.__version__))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588d60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609cc33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Project/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe170a1",
   "metadata": {},
   "source": [
    "#### Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163609a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_days(duration_str):\n",
    "    parts = duration_str.split(' days ')  # Split string into form ['22', '20:55:00']\n",
    "    days = float(parts[0])  # Extract number of days and convert to float\n",
    "    time_parts = parts[1].split(':')  # Split time part (hh:mm:ss) ['20', '55', '00']\n",
    "    hours = float(time_parts[0])  # Extract hours and convert to float\n",
    "    minutes = float(time_parts[1])  # Extract minutes and convert to float\n",
    "    seconds = float(time_parts[2])  # Extract seconds and convert to float\n",
    "    total_days = days + (hours / 24) + (minutes / (24 * 60)) + (seconds / (24 * 3600))  # Calculate total days\n",
    "    return total_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f9cae5",
   "metadata": {},
   "source": [
    "#### Select 20 patients (based on subject_id from patients) to use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3196a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/patients.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_patients = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273972e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10018845, 10011398, 10014354, 10024043, 10035631, 10018328, 10039997, 10004235, 10014078, 10014729, 10010471, 10020187, 10019385, 10021312, 10005817, 10027445, 10026406, 10007818, 10001217, 10016810]\n"
     ]
    }
   ],
   "source": [
    "evaluation_patients = df_patients['subject_id'].sample(n=20, random_state=42).tolist()\n",
    "\n",
    "# Any records belonging to these 20 subjects will be removed before training \n",
    "print(evaluation_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f4d6e7",
   "metadata": {},
   "source": [
    "Changed my mind, I want to filter based on hadm_id not subject_id\n",
    "If a table only has subject_id then remove the subjects the evaluation admissions belong to (admissions maps subject_id to hadm_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86930ed0",
   "metadata": {},
   "source": [
    "#### Select 55 (20% of) admissions to use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7d0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/admissions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_admissions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f89a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27617929, 27553957, 20282368, 27296885, 24980601, 21133938, 25559382, 20611796, 28778757, 28723315, 28998349, 28676446, 29276678, 26842957, 21477991, 25922998, 26706939, 27993466, 28236161, 27259207, 20385771, 24540843, 20900955, 22413744, 27494880, 25103777, 21599196, 21540783, 22585261, 26275841, 22130791, 22490490, 25020332, 29279905, 29483621, 27167814, 25508812, 21607814, 20297618, 29974575, 24912093, 21255400, 29295881, 28829452, 24656677, 29858644, 23488445, 25970245, 22508257, 25742920, 25085565, 22228639, 27660781, 28335091, 27703517]\n"
     ]
    }
   ],
   "source": [
    "evaluation_admissions = df_admissions['hadm_id'].sample(n=55, random_state=42).tolist()\n",
    "\n",
    "# Any records belonging to these admissions will be removed before training \n",
    "print(evaluation_admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb38c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_patients = df_admissions[df_admissions['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e4e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_patients = evaluation_patients['subject_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafebd0",
   "metadata": {},
   "source": [
    "#### Target variable LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df992596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS based on admissions table (target dataframe)\n",
    "\n",
    "file = \"hosp/admissions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_admissions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a5de424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions['dischtime'] = pd.to_datetime(df_admissions['dischtime'], format='%d/%m/%Y %H:%M')\n",
    "df_admissions['admittime'] = pd.to_datetime(df_admissions['admittime'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99faa83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_los_hadm = pd.DataFrame()\n",
    "df_los_subject = pd.DataFrame()\n",
    "\n",
    "df_los_subject['subject_id'] = df_admissions['subject_id']\n",
    "df_los_hadm['hadm_id'] = df_admissions['hadm_id']\n",
    "df_los_hadm['los'] = df_admissions['dischtime']-df_admissions['admittime']\n",
    "df_los_subject['los'] = df_admissions['dischtime']-df_admissions['admittime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "127feeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24181354</td>\n",
       "      <td>8 days 23:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25926192</td>\n",
       "      <td>7 days 20:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23983182</td>\n",
       "      <td>5 days 17:33:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22942076</td>\n",
       "      <td>1 days 17:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21606243</td>\n",
       "      <td>2 days 02:11:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>24745425</td>\n",
       "      <td>5 days 15:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>22168393</td>\n",
       "      <td>4 days 12:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>27708593</td>\n",
       "      <td>7 days 07:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>23251352</td>\n",
       "      <td>4 days 04:56:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>28108313</td>\n",
       "      <td>2 days 16:10:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hadm_id             los\n",
       "0    24181354 8 days 23:24:00\n",
       "1    25926192 7 days 20:12:00\n",
       "2    23983182 5 days 17:33:00\n",
       "3    22942076 1 days 17:41:00\n",
       "4    21606243 2 days 02:11:00\n",
       "..        ...             ...\n",
       "270  24745425 5 days 15:57:00\n",
       "271  22168393 4 days 12:18:00\n",
       "272  27708593 7 days 07:10:00\n",
       "273  23251352 4 days 04:56:00\n",
       "274  28108313 2 days 16:10:00\n",
       "\n",
       "[275 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_los_hadm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9be5074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average LOS for each subject_id\n",
    "df_los_subject = df_los_subject.groupby('subject_id').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2d1a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admittime= pd.DataFrame()\n",
    "df_admittime['hadm_id'] = df_admissions['hadm_id']\n",
    "df_admittime['admittime'] = df_admissions['admittime']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e245cf2",
   "metadata": {},
   "source": [
    "### omr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43290145",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/omr.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_omr = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ed45f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_omr_training = df_omr[~df_omr['subject_id'].isin(evaluation_patients)]\n",
    "df_omr_evaluation = df_omr[df_omr['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef3ece7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "\n",
    "# Create directory \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_omr_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_omr_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5c973",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c75ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr = df_omr_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f985101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr = df_omr.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine result_name and seq_num into the column name with result_value from the same row as its value \n",
    "\n",
    "# Function to combine values from columns into a new column \n",
    "def new_columns(row):\n",
    "    return row['result_name'] + '_' + str(row['seq_num'])\n",
    "\n",
    "new_names = df_omr.apply(new_columns, axis=1) # series of names of combinations \n",
    "\n",
    "\n",
    "def add_values(row, colName):\n",
    "    name = row['result_name'] + '_' + str(row['seq_num'])\n",
    "    if str(name) == colName:\n",
    "        return row['result_value']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "for i in range(len(new_names)):\n",
    "    df_omr[new_names[i]] = df_omr.apply(add_values, args=(new_names[i],), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop seq_num, result_name, result_value\n",
    "df_omr = df_omr.drop(columns=['seq_num', 'result_name', 'result_value'])\n",
    "# sequence number doesn't add any useful info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1866a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr['subject_id'].value_counts()\n",
    "\n",
    "# The patient with the most measurements has 391 so could make it 391 features for everyone but most will have lots of \n",
    "# zeroes\n",
    "# Fine as sparcity represents not taking many measurements which could also be a factor?\n",
    "# Could have number of measurements as an additional feature too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1878fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_omr[df_omr['subject_id'] == 10019003]\n",
    "filtered_df\n",
    "\n",
    "# Preserves every measurement made for each subject across all of their stays \n",
    "# Only one entry per row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ccae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = df_omr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f6b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering by date (so each patients measurements are chronological from top to bottom)\n",
    "\n",
    "df_omr = df_omr.sort_values(by=['subject_id', 'chartdate'])\n",
    "\n",
    "df_omr\n",
    "\n",
    "# This preserves for example, increase in weight over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop chartdate since the time shift is not consistent for each subject \n",
    "df_omr = df_omr.drop(columns=['chartdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de484584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_omr = df_omr.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a188731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e60655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row for each subject, features for every measurement made on them \n",
    "\n",
    "colNames = df_omr.columns.tolist()\n",
    "colNames.remove('subject_id')\n",
    "\n",
    "x = 0\n",
    "prev_subject = 0\n",
    "\n",
    "\n",
    "for row in range(len(df_omr)):\n",
    "    current_subject = df_omr['subject_id'][row] \n",
    "    if current_subject != prev_subject:\n",
    "        x = 0 # reset x\n",
    "    for i in range(len(colNames)): # for each column\n",
    "        if df_omr.loc[row, colNames[i]] != 0:\n",
    "            if colNames[i] + '_0' not in df_omr_final.columns: # New column name added\n",
    "                x = 0 # reset x\n",
    "            new_name = colNames[i] + '_' + str(x)\n",
    "            if new_name in df_omr_final.columns and (current_subject == prev_subject): # Trying to add another of the same \n",
    "                # measurement for the same patient \n",
    "                x += 1\n",
    "                new_name = colNames[i] + '_' + str(x)\n",
    "            df_omr_final.loc[current_subject, new_name] = df_omr.loc[row, colNames[i]]\n",
    "            df_omr_final = df_omr_final.copy()\n",
    "            break # leave for loop as the rest of the columns will be 0 for this row\n",
    "    prev_subject = current_subject\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3dbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr_final.fillna(0, inplace=True)\n",
    "df_omr_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all values to numbers \n",
    "\n",
    "df_omr_final = df_omr_final.astype(str)\n",
    "\n",
    "# Function to convert fraction string to decimal\n",
    "def fraction_to_decimal(fraction_str):\n",
    "    try:\n",
    "        numerator, denominator = map(int, fraction_str.split('/'))\n",
    "        return numerator / denominator\n",
    "    except ValueError:\n",
    "        return fraction_str  # Return unchanged if not a fraction\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "df_omr_final = df_omr_final.applymap(fraction_to_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omr_final = df_omr_final.astype(float)\n",
    "# df_omr_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d2193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and convert it to a column\n",
    "df_omr_final.reset_index(inplace=True)\n",
    "df_omr_final.rename(columns={'index': 'subject_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames based on the ID column\n",
    "df_omr_final = df_omr_final.merge(df_los_subject, on='subject_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c736a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this show?\n",
    "# Each patient (subject_id is the index of the df) has measurements showing type_sequence_date\n",
    "# sequence starts from 1 and it is used when the same measurement was taken more than once in a single day\n",
    "# date starts from 0 and is used when the same measurement for the same patient was taken on a different day\n",
    "# Note that they were NOT taken on the same date for each patient but the bigger the date integer, the later the measurement\n",
    "# was taken, relative to that patient's admission  \n",
    "\n",
    "# Weight (Lbs)_1_0 is the first time the patient was weighed, Weight (Lbs)_3_0 is the third time they were weighed on that \n",
    "# same day as they were first weighed\n",
    "# Weight (Lbs)_1_1 is from a separate (later) date where the patient was weighed again, this is the first measurement \n",
    "# from this day \n",
    "# Any non applicable measurements are imputed with 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ef7f0",
   "metadata": {},
   "source": [
    "Decide which ones to keep all measurements of per patient and which to just take the average and keep as one record for patient (that aren’t likely to change):\n",
    "\n",
    "Remove blood pressure sitting, lying and standing as too few samples \n",
    "Take average for height \n",
    "\n",
    "Could probably drop a few of the features that are really empty ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop subject_id\n",
    "df_omr_final = df_omr_final.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57112916",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_omr_final.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_omr_final['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bc8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction for data\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 12\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(data)\n",
    "data = svd.transform(data)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c79161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "# y_test['los'] = y_test['los'].astype(str)\n",
    "# y_test.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "# y_test.loc[~y_test['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "# y_test['los'] = y_test['los'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1863911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest regression\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_omr = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_omr.fit(data, target)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# # Calculate mean squared error\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# # Plot true vs predicted values\n",
    "# plt.scatter(y_test, y_pred)\n",
    "# plt.xlabel(\"True Values\")\n",
    "# plt.ylabel(\"Predicted Values\")\n",
    "# plt.title(\"True vs Predicted Values (Random Forest Regression)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41079eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "# Create a new directory for the model file\n",
    "output_folder = 'LOS_RF_learners'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_omr.joblib')\n",
    "dump(random_forest_omr, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03635678",
   "metadata": {},
   "source": [
    "### Admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9e295df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/admissions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_admissions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "728c096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_admissions_training = df_admissions[~df_admissions['subject_id'].isin(evaluation_patients)]\n",
    "df_admissions_evaluation = df_admissions[df_admissions['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc6606c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_admissions_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_admissions_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc1036",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43cf5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an ed_duration feature for edouttime - edregtime (how long the patient stayed in the emergency department)\n",
    "\n",
    "# Convert to datetime\n",
    "df_admissions['edouttime'] = pd.to_datetime(df_admissions['edouttime'], format='%d/%m/%Y %H:%M')\n",
    "df_admissions['edregtime'] = pd.to_datetime(df_admissions['edregtime'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "df_admissions['ed_duration'] = df_admissions['edouttime'] - df_admissions['edregtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_admissions['ed_duration'] = df_admissions['ed_duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27efc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.drop(columns=['subject_id', 'admittime', 'dischtime', 'deathtime', 'hospital_expire_flag'\n",
    "                            , 'edregtime', 'edouttime', 'admit_provider_id','discharge_location'])\n",
    "\n",
    "# discharge_location is an outcome feature, should not be used to predict LOS as not known beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null with N/A and then one hot encode\n",
    "df_admissions['marital_status'] = df_admissions['marital_status'].fillna('N/A')\n",
    "df_admissions = pd.get_dummies(df_admissions, columns=['admission_type', 'admission_location', \n",
    "                                                      'insurance','language', 'marital_status','race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8167e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_admissions = df_admissions.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf788c",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_admissions.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_admissions['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "\n",
    "data['ed_duration']= data['ed_duration'].astype(str)\n",
    "data['ed_duration']= data['ed_duration'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_admissions = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_admissions.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e599c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_admissions.joblib')\n",
    "dump(random_forest_admissions, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d27de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7096dad",
   "metadata": {},
   "source": [
    "### Emar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62871fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/emar.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_emar = pd.read_csv(full_path)\n",
    "\n",
    "# records for 65 different patients \n",
    "# 181 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e8131ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_emar_training = df_emar[~df_emar['subject_id'].isin(evaluation_patients)]\n",
    "df_emar_evaluation = df_emar[df_emar['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d14df226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_emar_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_emar_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4f67d4",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f400d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar = df_emar_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e59b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar = df_emar.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0cdc5",
   "metadata": {},
   "source": [
    "Impute with N/A and encode: enter_provider_id, medication\n",
    "\n",
    "Drop: subject_id, emar_id, poe_id, pharmacy_id, event_txt, storetime\n",
    "\n",
    "poe_id is an identifier which links administrations in emar to orders in poe and prescriptions\n",
    "storetime is when it was recorded in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature called delay using scheduletime - charttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_emar['scheduletime'] = pd.to_datetime(df_emar['scheduletime'], format='%Y/%m/%d %H:%M')\n",
    "df_emar['charttime'] = pd.to_datetime(df_emar['charttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_emar['delay'] = df_emar['charttime'] - df_emar['scheduletime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_emar['delay'] = df_emar['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e29afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar = df_emar.drop(columns=['subject_id','emar_id','poe_id','pharmacy_id',\n",
    "                               'event_txt','charttime','scheduletime','storetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null with N/A and then one hot encode\n",
    "df_emar['enter_provider_id'] = df_emar['enter_provider_id'].fillna('N/A')\n",
    "df_emar['medication'] = df_emar['medication'].fillna('N/A')\n",
    "df_emar = pd.get_dummies(df_emar, columns=['enter_provider_id', 'medication'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb60863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar = df_emar.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_emar = df_emar.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3662ee",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_emar.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_emar['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2440409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ac74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['delay']= data['delay'].astype(str)\n",
    "data['delay']= data['delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dee016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_emar = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_emar.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52bd6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_emar.joblib')\n",
    "dump(random_forest_emar, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95462984",
   "metadata": {},
   "source": [
    "### Emar_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94d2c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/emar_detail.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_emar_detail = pd.read_csv(full_path,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b97a3d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_emar_detail_training = df_emar_detail[~df_emar_detail['subject_id'].isin(evaluation_patients)]\n",
    "df_emar_detail_evaluation = df_emar_detail[df_emar_detail['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cfbf134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_emar_detail_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_emar_detail.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224638e",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff71e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a8740",
   "metadata": {},
   "source": [
    "Fields that have lots of null values:\n",
    "reason_for_no_barcode: drop\n",
    "prior_infusion_rate: impute with zeroes\n",
    "infusion_rate: impute with zeroes\n",
    "infusion_rate_adjustment: impute with 'N/A', then one hot encoding\n",
    "infusion_rate_adjustment_amount: impute with zeroes\n",
    "infusion_rate_unit: impute with 'N/A', then one hot encoding\n",
    "infusion_complete: impute with 'N/A', then one hot encoding\n",
    "completion_interval: impute with 0, then ordinal encoding \n",
    "new_iv_bag_hung: impute with N, then binary encoding \n",
    "\n",
    "Text data to remove but maybe consider later:\n",
    "product_description, product_description_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.drop(columns=['reason_for_no_barcode']) # Too hard to encode, adds not much value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0s\n",
    "df_emar_detail['prior_infusion_rate'] = df_emar_detail['prior_infusion_rate'].fillna(0)\n",
    "df_emar_detail['infusion_rate'] = df_emar_detail['infusion_rate'].fillna(0)\n",
    "df_emar_detail['infusion_rate_adjustment_amount'] = df_emar_detail['infusion_rate_adjustment_amount'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_emar_detail['infusion_rate_adjustment'] = df_emar_detail['infusion_rate_adjustment'].fillna('N/A')\n",
    "df_emar_detail['infusion_rate_unit'] = df_emar_detail['infusion_rate_unit'].fillna('N/A')\n",
    "df_emar_detail['infusion_complete'] = df_emar_detail['infusion_complete'].fillna('N/A')\n",
    "df_emar_detail = pd.get_dummies(df_emar_detail, columns=['infusion_rate_adjustment','infusion_complete',\n",
    "                                                         'infusion_rate_unit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].fillna(0)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('PRN', 0)\n",
    "#Converting all the intervals to minutes\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 2 hours', 120)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 4 hours', 240)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 1 hour', 60)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 1.5 hours', 90)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 8 hours', 480)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 15 minutes', 15)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 12 hours', 720)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 30 minutes', 30)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 24 hours', 1140)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 1 minutes', 1)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 14 hours', 840)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 7 hours', 420)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 5 hours', 300)\n",
    "df_emar_detail['completion_interval'] = df_emar_detail['completion_interval'].replace('within 3 hours', 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['new_iv_bag_hung'] = df_emar_detail['new_iv_bag_hung'].fillna('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding\n",
    "df_emar_detail['new_iv_bag_hung'] = df_emar_detail['new_iv_bag_hung'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and one hot encode:\n",
    "# administration_type\n",
    "# barcode_type\n",
    "# complete_dose_not_given\n",
    "# dose_due_unit\n",
    "# dose_given_unit\n",
    "# will_remainder_of_dose_be_given\n",
    "# product_unit\n",
    "# product_code\n",
    "# route\n",
    "# side\n",
    "# site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def15eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['administration_type'] = df_emar_detail['administration_type'].fillna('N/A')\n",
    "df_emar_detail['barcode_type'] = df_emar_detail['barcode_type'].fillna('N/A')\n",
    "df_emar_detail['complete_dose_not_given'] = df_emar_detail['complete_dose_not_given'].fillna('N/A')\n",
    "df_emar_detail['dose_due_unit'] = df_emar_detail['dose_due_unit'].fillna('N/A')\n",
    "df_emar_detail['dose_given_unit'] = df_emar_detail['dose_given_unit'].fillna('N/A')\n",
    "df_emar_detail['will_remainder_of_dose_be_given'] = df_emar_detail['will_remainder_of_dose_be_given'].fillna('N/A')\n",
    "df_emar_detail['product_unit'] = df_emar_detail['product_unit'].fillna('N/A')\n",
    "df_emar_detail['product_code'] = df_emar_detail['product_code'].fillna('N/A')\n",
    "df_emar_detail['route'] = df_emar_detail['route'].fillna('N/A')\n",
    "df_emar_detail['side'] = df_emar_detail['side'].fillna('N/A')\n",
    "df_emar_detail['site'] = df_emar_detail['site'].fillna('N/A')\n",
    "df_emar_detail = pd.get_dummies(df_emar_detail, columns=['administration_type','barcode_type','complete_dose_not_given',\n",
    "                                                        'dose_due_unit','dose_given_unit',\n",
    "                                                        'will_remainder_of_dose_be_given','product_unit','product_code',\n",
    "                                                        'route','side','site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e36be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with zeroes:\n",
    "# dose_due and dose_given, but also need to deal with some of them being ranges\n",
    "# product_amount_given\n",
    "# restart_interval, then ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3812c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['product_amount_given'] = df_emar_detail['product_amount_given'].fillna(0)\n",
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].fillna(0)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].fillna(0)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f746266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].astype(str)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a720d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_middle_value(range_string):\n",
    "    if '-' in range_string:\n",
    "        start, end = map(float, range_string.split('-'))\n",
    "        return (start + end) / 2\n",
    "    else:\n",
    "        return range_string\n",
    "\n",
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].apply(find_middle_value)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].apply(find_middle_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c3bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('PRN', 0)\n",
    "#Converting all the intervals to minutes\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 2 hours', 120)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 4 hours', 240)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 1 hour', 60)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 30 minutes', 30)\n",
    "df_emar_detail['restart_interval'] = df_emar_detail['restart_interval'].replace('within 24 hours', 1140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f35cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N and map to binary encoding:\n",
    "# continued_infusion_in_other_location\n",
    "# non_formulary_visual_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['continued_infusion_in_other_location'] = df_emar_detail['continued_infusion_in_other_location'].fillna('N')\n",
    "df_emar_detail['non_formulary_visual_verification'] = df_emar_detail['non_formulary_visual_verification'].fillna('N')\n",
    "# Binary encoding\n",
    "df_emar_detail['continued_infusion_in_other_location'] = df_emar_detail['continued_infusion_in_other_location'].map({'Y': 1, 'N': 0})\n",
    "df_emar_detail['non_formulary_visual_verification'] = df_emar_detail['non_formulary_visual_verification'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.drop(columns=['pharmacy_id']) # Contains NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be41ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.drop(columns=['emar_id']) # Practically unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace blanks with zero\n",
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].replace('___', 0)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].replace('___', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail['dose_due'] = df_emar_detail['dose_due'].astype(float)\n",
    "df_emar_detail['dose_given'] = df_emar_detail['dose_given'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A or 0\n",
    "# One hot encode the categorical features \n",
    "\n",
    "df_emar_detail['product_description'] = df_emar_detail['product_description'].fillna('N/A')\n",
    "df_emar_detail['product_description_other'] = df_emar_detail['product_description_other'].fillna('N/A')\n",
    "df_emar_detail['parent_field_ordinal'] = df_emar_detail['parent_field_ordinal'].fillna(0)\n",
    "df_emar_detail = pd.get_dummies(df_emar_detail, columns=['product_description_other','product_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emar_detail = df_emar_detail.merge(df_los_subject, on='subject_id', how='left')\n",
    "df_emar_detail = df_emar_detail.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85385e",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_emar_detail.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_emar_detail['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5795468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474033a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_emar_detail = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_emar_detail.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc63f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_emar_detail.joblib')\n",
    "dump(random_forest_emar_detail, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8acb6e",
   "metadata": {},
   "source": [
    "### hcpcsevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7937fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/hcpcsevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_hcpcsevents = pd.read_csv(full_path)\n",
    "\n",
    "# Contains info for 18 different patients\n",
    "# d_hcpcs has longer descriptions (connected by code) but no other useful info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69aba58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_hcpcsevents_training = df_hcpcsevents[~df_hcpcsevents['subject_id'].isin(evaluation_patients)]\n",
    "df_hcpcsevents_evaluation = df_hcpcsevents[df_hcpcsevents['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6002ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_hcpcsevents_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_hcpcsevents_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0897e30",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9976d7",
   "metadata": {},
   "source": [
    "To drop: subject_id, chartdate, hcpcs_cd (code that links to longer description in d_hcpcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = df_hcpcsevents_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a44e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = df_hcpcsevents.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature for days_since_admission using chartdate - admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_hcpcsevents['chartdate'] = pd.to_datetime(df_hcpcsevents['chartdate'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_hcpcsevents = df_hcpcsevents.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# Discard the time part and keep only the date\n",
    "df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_hcpcsevents['days_since_admission'] = df_hcpcsevents['chartdate'] - df_hcpcsevents['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_hcpcsevents['days_since_admission'] = df_hcpcsevents['days_since_admission'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents['days_since_admission'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9da606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = df_hcpcsevents.drop(columns=['subject_id','chartdate','hcpcs_cd'])\n",
    "# Not enough samples to include code as after encoding there would be a lot more features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41749d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = pd.get_dummies(df_hcpcsevents, columns=['short_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a237e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcpcsevents = df_hcpcsevents.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_hcpcsevents = df_hcpcsevents.drop(columns=['hadm_id', 'admittime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3925f",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_hcpcsevents.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_hcpcsevents['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ae4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 13 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to integers\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].str.split().str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 9\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(data)\n",
    "data = svd.transform(data)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b76f3",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_hcpcsevents = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_hcpcsevents.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54a079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_hcpcsevents.joblib')\n",
    "dump(random_forest_hcpcsevents, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bab7c",
   "metadata": {},
   "source": [
    "### labevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc9482e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/labevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_labevents = pd.read_csv(full_path)\n",
    "\n",
    "# Information regarding 252 different admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3c1cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_labevents_training = df_labevents[~df_labevents['subject_id'].isin(evaluation_patients)]\n",
    "# df_labevents_evaluation = df_labevents[df_labevents['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_labevents_training = df_labevents[~df_labevents['hadm_id'].isin(evaluation_admissions)]\n",
    "df_labevents_evaluation = df_labevents[df_labevents['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c48594db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_labevents_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_labevents_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078341d9",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd030234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = df_labevents_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ba94a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = df_labevents.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7ba6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents['value'] = pd.to_numeric(df_labevents['value'], errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "219c6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature for days_since_admission using charttime - admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_labevents['charttime'] = pd.to_datetime(df_labevents['charttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_labevents = df_labevents.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_labevents['days_since_admission'] = df_labevents['charttime'] - df_labevents['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_labevents['days_since_admission'] = df_labevents['days_since_admission'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a8a34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add storetime - charttime feature called delay\n",
    "\n",
    "# Convert to datetime\n",
    "df_labevents['storetime'] = pd.to_datetime(df_labevents['storetime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_labevents['delay'] = df_labevents['storetime'] - df_labevents['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_labevents['delay'] = df_labevents['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04f937",
   "metadata": {},
   "source": [
    "Drop: labevent_id, subject_id, order_provider_id (too many Null), charttime, storetime, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef5a2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = df_labevents.drop(columns=['labevent_id','subject_id','order_provider_id','charttime','storetime','comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6571e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For flag make abnormal = 1 and fill Null with 0\n",
    "df_labevents['flag'] = df_labevents['flag'].fillna(0)\n",
    "df_labevents['flag'] = df_labevents['flag'].replace('abnormal', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48161ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For priority fill Null with N/A and then one hot encode\n",
    "df_labevents['priority'] = df_labevents['priority'].fillna('N/A')\n",
    "df_labevents = pd.get_dummies(df_labevents, columns=['priority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ae4a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = pd.get_dummies(df_labevents, columns=['valueuom','specimen_id','itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ffb59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with null values \n",
    "df_labevents = df_labevents.dropna()\n",
    "# Reduced from 107727 rows to 66660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "450457cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labevents = df_labevents.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_labevents = df_labevents.drop(columns=['hadm_id', 'admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ff6b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_labevents.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_labevents['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0da14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bookmark \n",
    "\n",
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'labevents_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e48073f",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07e9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['delay']= data['delay'].astype(str)\n",
    "data['delay']= data['delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b528046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to integers\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].str.split().str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a541458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_labevents = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_labevents.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0f5e4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOS_RF_learners\\\\random_forest_labevents.joblib']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_labevents.joblib')\n",
    "dump(random_forest_labevents, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f14652",
   "metadata": {},
   "source": [
    "### microbiologyevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0995c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/microbiologyevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_microbio = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f49da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_microbio_training = df_microbio[~df_microbio['subject_id'].isin(evaluation_patients)]\n",
    "# df_microbio_evaluation = df_microbio[df_microbio['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_microbio_training = df_microbio[~df_microbio['hadm_id'].isin(evaluation_admissions)]\n",
    "df_microbio_evaluation = df_microbio[df_microbio['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbf55521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_microbio_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_microbio_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7e47a8",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0cc3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio = df_microbio_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62891ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio = df_microbio.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5a594e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make days_since_admission using charttime \n",
    "\n",
    "# Convert to datetime\n",
    "df_microbio['charttime'] = pd.to_datetime(df_microbio['charttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_microbio = df_microbio.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_microbio['days_since_admission'] = df_microbio['charttime'] - df_microbio['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_microbio['days_since_admission'] = df_microbio['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_microbio = df_microbio.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad8f77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add storetime - charttime feature (call it delay)\n",
    "\n",
    "# Convert to datetime\n",
    "df_microbio['storetime'] = pd.to_datetime(df_microbio['storetime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_microbio['delay'] = df_microbio['storetime'] - df_microbio['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_microbio['delay'] = df_microbio['delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afceb74",
   "metadata": {},
   "source": [
    "Drop: microevent_id, subject_id, chartdate, charttime, test_seq, storedate, storetime, test_name and org_itemid (since info in name), quantity, ab_name, comments, micro_specimen_id (unique identifier for sample as some measurements are made on the same sample)\n",
    "Keep but categorical: order_provider_id, spec_type_desc, dilution_text, dilution_comparison\n",
    "Impute null with 0: order_provider_id, org_itemid, isolate_num, ab_itemid, dilution_value\n",
    "Impute with N/A and then one hot encode: interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "add7226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "# spec_itemid , test_itemid\n",
    "df_microbio = df_microbio.drop(columns=['microevent_id','subject_id','chartdate','charttime','test_seq','storedate',\n",
    "                                       'storetime','quantity','comments','ab_itemid',\n",
    "                                       'spec_itemid','test_itemid','org_itemid','micro_specimen_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "361e4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute null with 0: order_provider_id, org_itemid, isolate_num, ab_itemid, dilution_value\n",
    "df_microbio['order_provider_id'] = df_microbio['order_provider_id'].fillna(0)\n",
    "df_microbio['isolate_num'] = df_microbio['isolate_num'].fillna(0)\n",
    "df_microbio['dilution_value'] = df_microbio['dilution_value'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea7424ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and then one hot encode: interpretation\n",
    "# encode test_name, ab_name\n",
    "\n",
    "df_microbio['interpretation'] = df_microbio['interpretation'].fillna('N/A')\n",
    "df_microbio['test_name'] = df_microbio['test_name'].fillna('N/A')\n",
    "df_microbio['ab_name'] = df_microbio['ab_name'].fillna('N/A')\n",
    "df_microbio['org_name'] = df_microbio['org_name'].fillna('None')\n",
    "df_microbio = pd.get_dummies(df_microbio, columns=['org_name','interpretation','ab_name','test_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29a47772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep but categorical: order_provider_id, spec_type_desc, dilution_text, dilution_comparison\n",
    "df_microbio = pd.get_dummies(df_microbio, columns=['order_provider_id','spec_type_desc','dilution_text',\n",
    "                                                  'dilution_comparison'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0f55fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio = df_microbio.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3c59289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microbio = df_microbio.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_microbio = df_microbio.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5068901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_microbio.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_microbio['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf1e80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'microbio_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d063df3",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d935111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['delay']= data['delay'].astype(str)\n",
    "data['delay']= data['delay'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9872b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_microbio = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_microbio.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe043bf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOS_RF_learners\\\\random_forest_microbio.joblib']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_microbio.joblib')\n",
    "dump(random_forest_microbio, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d4703",
   "metadata": {},
   "source": [
    "### patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76620bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/patients.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_patients = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fea6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_patients_training = df_patients[~df_patients['subject_id'].isin(evaluation_patients)]\n",
    "df_patients_evaluation = df_patients[df_patients['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef5e6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_patients_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_patients_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a48cd0",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = df_patients_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff71678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = df_patients.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371f4a9",
   "metadata": {},
   "source": [
    "Drop: anchor_year\n",
    "Encode: gender (M to 0 and F to 1), dod (change all to 1 and nulls to 0)\n",
    "Dummies: anchor_year_group  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28506159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "df_patients = df_patients.drop(columns=['anchor_year','dod']) \n",
    "# Since this is the shifted year and dod is an outcome value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef887ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode: gender (M to 0 and F to 1), dod (change all to 1 and nulls to 0)\n",
    "df_patients['gender'] = df_patients['gender'].replace('M', 0)\n",
    "df_patients['gender'] = df_patients['gender'].replace('F', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies: anchor_year_group  \n",
    "df_patients = pd.get_dummies(df_patients, columns=['anchor_year_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = df_patients.merge(df_los_subject, on='subject_id', how='left')\n",
    "df_patients = df_patients.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9de563",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_patients.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_patients['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d633698",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354979e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a629d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_patients = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_patients.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb5be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_patients.joblib')\n",
    "dump(random_forest_patients, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc8001",
   "metadata": {},
   "source": [
    "### pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e2ab1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/pharmacy.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_pharmacy = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebb26212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_pharmacy_training = df_pharmacy[~df_pharmacy['subject_id'].isin(evaluation_patients)]\n",
    "df_pharmacy_evaluation = df_pharmacy[df_pharmacy['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12547232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_pharmacy_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_pharmacy_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a597a32",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee4e19",
   "metadata": {},
   "source": [
    "drop: subject_id, pharmacy_id, poe_id, starttime, stoptime, entertime, verifiedtime, disp_sched, basal_rate, one_hr_max,\n",
    "expirationdate, fill_quantity\n",
    "Encode: proc_type, status\n",
    "Impute with N/A and encode: infusion_type, sliding_scale, duration_interval, expiration_unit, dispensation, medication, route, frequency\n",
    "Impute with 0: lockout_interval, doses_per_24_hrs, duration, expiration_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pharmacy = df_pharmacy_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pharmacy = df_pharmacy.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoptime-starttime for a duration feature\n",
    "\n",
    "# Convert to datetime\n",
    "df_pharmacy['stoptime'] = pd.to_datetime(df_pharmacy['stoptime'], format='%Y/%m/%d %H:%M')\n",
    "df_pharmacy['starttime'] = pd.to_datetime(df_pharmacy['starttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "\n",
    "df_pharmacy['medication_duration'] = df_pharmacy['stoptime'] - df_pharmacy['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_pharmacy['medication_duration'] = df_pharmacy['medication_duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ccc058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifiedtime - entertime for verification_delay feature \n",
    "\n",
    "# Convert to datetime\n",
    "df_pharmacy['verifiedtime'] = pd.to_datetime(df_pharmacy['verifiedtime'], format='%Y/%m/%d %H:%M')\n",
    "df_pharmacy['entertime'] = pd.to_datetime(df_pharmacy['entertime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_pharmacy['verification_delay'] = df_pharmacy['verifiedtime'] - df_pharmacy['entertime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_pharmacy['verification_delay'] = df_pharmacy['verification_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_value = [0] \n",
    "\n",
    "# Fill null values with the list\n",
    "df_pharmacy['disp_sched'] = df_pharmacy['disp_sched'].fillna(pd.Series([fill_value]*len(df_pharmacy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all categories to strings\n",
    "df_pharmacy['disp_sched'] = df_pharmacy['disp_sched'].apply(lambda x: [str(item) for item in x])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "encoded_feature = pd.DataFrame(mlb.fit_transform(df_pharmacy['disp_sched']),\n",
    "                               columns=mlb.classes_,\n",
    "                               index=df_pharmacy.index)\n",
    "\n",
    "df_pharmacy = pd.concat([df_pharmacy, encoded_feature], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4476c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_pharmacy = df_pharmacy.drop(columns=['subject_id','pharmacy_id','poe_id','starttime','stoptime','entertime',\n",
    "                                       'verifiedtime','expirationdate', 'fill_quantity','disp_sched'])\n",
    "# expiration date and fill quantity are all empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3834ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode: proc_type, status\n",
    "df_pharmacy = pd.get_dummies(df_pharmacy, columns=['proc_type','status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_pharmacy['infusion_type'] = df_pharmacy['infusion_type'].fillna('N/A')\n",
    "df_pharmacy['sliding_scale'] = df_pharmacy['sliding_scale'].fillna('N/A')\n",
    "df_pharmacy['duration_interval'] = df_pharmacy['duration_interval'].fillna('N/A')\n",
    "df_pharmacy['expiration_unit'] = df_pharmacy['expiration_unit'].fillna('N/A')\n",
    "df_pharmacy['dispensation'] = df_pharmacy['dispensation'].fillna('N/A')\n",
    "df_pharmacy['medication'] = df_pharmacy['medication'].fillna('N/A')\n",
    "df_pharmacy['route'] = df_pharmacy['route'].fillna('N/A')\n",
    "df_pharmacy['frequency'] = df_pharmacy['frequency'].fillna('N/A')\n",
    "df_pharmacy = pd.get_dummies(df_pharmacy, columns=['infusion_type','sliding_scale','duration_interval','expiration_unit',\n",
    "                                                  'dispensation','medication','route','frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b81b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0: lockout_interval, doses_per_24_hrs, duration, expiration_value\n",
    "df_pharmacy['lockout_interval'] = df_pharmacy['lockout_interval'].fillna(0)\n",
    "df_pharmacy['doses_per_24_hrs'] = df_pharmacy['doses_per_24_hrs'].fillna(0)\n",
    "df_pharmacy['expiration_value'] = df_pharmacy['expiration_value'].fillna(0)\n",
    "df_pharmacy['basal_rate'] = df_pharmacy['basal_rate'].fillna(0)\n",
    "df_pharmacy['one_hr_max'] = df_pharmacy['one_hr_max'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a266c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pharmacy = df_pharmacy.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_pharmacy = df_pharmacy.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4680ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_pharmacy.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_pharmacy['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6dad88",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e521a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "\n",
    "data['medication_duration']= data['medication_duration'].astype(str)\n",
    "data['medication_duration']= data['medication_duration'].apply(convert_to_days)\n",
    "# Convert strings to integers\n",
    "data['verification_delay'] = data['verification_delay'].astype(str)\n",
    "data['verification_delay'] = data['verification_delay'].str.split().str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bdab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_pharmacy = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_pharmacy.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56491b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_pharmacy.joblib')\n",
    "dump(random_forest_pharmacy, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca6f4c",
   "metadata": {},
   "source": [
    "### poe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67aa0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/poe.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_poe = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dda8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_poe_training = df_poe[~df_poe['subject_id'].isin(evaluation_patients)]\n",
    "# df_poe_evaluation = df_poe[df_poe['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_poe_training = df_poe[~df_poe['hadm_id'].isin(evaluation_admissions)]\n",
    "df_poe_evaluation = df_poe[df_poe['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37bee779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_poe_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_poe_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25699c39",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c51641",
   "metadata": {},
   "source": [
    "To drop: poe_id, subject_id, ordertime, discontinue_of_poe_id, discontinued_by_poe_id (all unique), order_status (all inactive)\n",
    "Encode: order_type, transaction_type\n",
    "Impute with N/A and then encode: order_subtype, order_provider_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4bc9bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe = df_poe_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aac1f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe = df_poe.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0166f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a feature of ordertime - admittime for days_since_admission\n",
    "\n",
    "# Convert to datetime\n",
    "df_poe['ordertime'] = pd.to_datetime(df_poe['ordertime'], format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_poe = df_poe.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_poe['days_since_admission'] = df_poe['ordertime'] - df_poe['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_poe['days_since_admission'] = df_poe['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_poe = df_poe.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0a79397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_poe = df_poe.drop(columns=['poe_id','subject_id','ordertime','discontinue_of_poe_id','discontinued_by_poe_id',\n",
    "                                       'order_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73c67b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_poe = pd.get_dummies(df_poe, columns=['order_type','transaction_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d7a61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_poe['order_subtype'] = df_poe['order_subtype'].fillna('N/A')\n",
    "df_poe['order_provider_id'] = df_poe['order_provider_id'].fillna('N/A')\n",
    "df_poe = pd.get_dummies(df_poe, columns=['order_subtype','order_provider_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0a8be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe = df_poe.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_poe = df_poe.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "07746ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_poe.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_poe['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77fbc321",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'poe_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba893b",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1143170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36949424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_poe = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_poe.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ae3358cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOS_RF_learners\\\\random_forest_poe.joblib']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_poe.joblib')\n",
    "dump(random_forest_poe, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812577ad",
   "metadata": {},
   "source": [
    "### poe_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9933b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/poe_detail.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_poe_detail = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0620155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_poe_detail_training = df_poe_detail[~df_poe_detail['subject_id'].isin(evaluation_patients)]\n",
    "df_poe_detail_evaluation = df_poe_detail[df_poe_detail['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40613ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_poe_detail_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_poe_detail_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f5bd8",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923562d8",
   "metadata": {},
   "source": [
    "To drop: poe_id\n",
    "Encode: field_name, field_value\n",
    "subject_id for los and then drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad3a8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe_detail = df_poe_detail_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51fb5ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poe_id</th>\n",
       "      <th>poe_seq</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>field_name</th>\n",
       "      <th>field_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011398-23</td>\n",
       "      <td>23</td>\n",
       "      <td>10011398</td>\n",
       "      <td>Admit to</td>\n",
       "      <td>Surgery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011398-103</td>\n",
       "      <td>103</td>\n",
       "      <td>10011398</td>\n",
       "      <td>Transfer to</td>\n",
       "      <td>Surgery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011398-163</td>\n",
       "      <td>163</td>\n",
       "      <td>10011398</td>\n",
       "      <td>Discharge Planning</td>\n",
       "      <td>Finalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011398-109</td>\n",
       "      <td>109</td>\n",
       "      <td>10011398</td>\n",
       "      <td>Tubes &amp; Drains type</td>\n",
       "      <td>Chest tube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011398-35</td>\n",
       "      <td>35</td>\n",
       "      <td>10011398</td>\n",
       "      <td>Tubes &amp; Drains type</td>\n",
       "      <td>Chest tube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3790</th>\n",
       "      <td>10021118-7</td>\n",
       "      <td>7</td>\n",
       "      <td>10021118</td>\n",
       "      <td>Admit category</td>\n",
       "      <td>Admit to inpatient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3791</th>\n",
       "      <td>10021118-24</td>\n",
       "      <td>24</td>\n",
       "      <td>10021118</td>\n",
       "      <td>Code status</td>\n",
       "      <td>Resuscitate (Full code)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3792</th>\n",
       "      <td>10021118-100</td>\n",
       "      <td>100</td>\n",
       "      <td>10021118</td>\n",
       "      <td>Tubes &amp; Drains type</td>\n",
       "      <td>Indwelling urinary catheter (IUC) - Foley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>10021118-227</td>\n",
       "      <td>227</td>\n",
       "      <td>10021118</td>\n",
       "      <td>Tubes &amp; Drains type</td>\n",
       "      <td>Indwelling urinary catheter (IUC) - Foley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>10021118-158</td>\n",
       "      <td>158</td>\n",
       "      <td>10021118</td>\n",
       "      <td>Tubes &amp; Drains type</td>\n",
       "      <td>Indwelling urinary catheter (IUC) - Foley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1642 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            poe_id  poe_seq  subject_id           field_name  \\\n",
       "0      10011398-23       23    10011398             Admit to   \n",
       "1     10011398-103      103    10011398          Transfer to   \n",
       "2     10011398-163      163    10011398   Discharge Planning   \n",
       "3     10011398-109      109    10011398  Tubes & Drains type   \n",
       "4      10011398-35       35    10011398  Tubes & Drains type   \n",
       "...            ...      ...         ...                  ...   \n",
       "3790    10021118-7        7    10021118       Admit category   \n",
       "3791   10021118-24       24    10021118          Code status   \n",
       "3792  10021118-100      100    10021118  Tubes & Drains type   \n",
       "3793  10021118-227      227    10021118  Tubes & Drains type   \n",
       "3794  10021118-158      158    10021118  Tubes & Drains type   \n",
       "\n",
       "                                    field_value  \n",
       "0                                       Surgery  \n",
       "1                                       Surgery  \n",
       "2                                     Finalized  \n",
       "3                                    Chest tube  \n",
       "4                                    Chest tube  \n",
       "...                                         ...  \n",
       "3790                         Admit to inpatient  \n",
       "3791                    Resuscitate (Full code)  \n",
       "3792  Indwelling urinary catheter (IUC) - Foley  \n",
       "3793  Indwelling urinary catheter (IUC) - Foley  \n",
       "3794  Indwelling urinary catheter (IUC) - Foley  \n",
       "\n",
       "[1642 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poe_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3905821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe_detail = df_poe_detail.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7826308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_poe_detail = df_poe_detail.drop(columns=['poe_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b25d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_poe_detail = pd.get_dummies(df_poe_detail, columns=['field_name','field_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bfa99641",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poe_detail = df_poe_detail.merge(df_los_subject, on='subject_id', how='left')\n",
    "df_poe_detail = df_poe_detail.drop(columns=['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "98db213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_poe_detail.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_poe_detail['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7396c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'poe_detail_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fab31",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4cff40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96f8ec8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_poe_detail = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_poe_detail.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "60a09a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOS_RF_learners\\\\random_forest_poe_detail.joblib']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_poe_detail.joblib')\n",
    "dump(random_forest_poe_detail, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ef4a5",
   "metadata": {},
   "source": [
    "### prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8c0f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/prescriptions.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_prescriptions = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78395c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_prescriptions_training = df_prescriptions[~df_prescriptions['subject_id'].isin(evaluation_patients)]\n",
    "df_prescriptions_evaluation = df_prescriptions[df_prescriptions['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d5c917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_prescriptions_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_prescriptions_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbcd69",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399473e2",
   "metadata": {},
   "source": [
    "Drop na and encode: dose_val_rx, form_val_disp, order_provider_id\n",
    "Drop: subject_id, pharmacy_id, starttime, stoptime, form_rx (mostly null), poe_id\n",
    "Impute with N/A and encode: formulary_drug_cd, gsn, prod_strength, route\n",
    "Encode: drug_type, drug, dose_unit_rx, form_unit_disp\n",
    "Impute with 0: doses_per_24_hrs\n",
    "\n",
    "Drop rows with na\n",
    "\n",
    "order_provider_id\n",
    "Was going to impute with N/A and encode but going to drop as too many features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions = df_prescriptions_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c33a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions = df_prescriptions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d57f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature of stoptime-starttime called duration \n",
    "\n",
    "# Convert to datetime\n",
    "df_prescriptions['stoptime'] = pd.to_datetime(df_prescriptions['stoptime'], format='%Y/%m/%d %H:%M')\n",
    "df_prescriptions['starttime'] = pd.to_datetime(df_prescriptions['starttime'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "df_prescriptions['duration'] = df_prescriptions['stoptime'] - df_prescriptions['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_prescriptions['duration'] = df_prescriptions['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_prescriptions.dropna(subset=['dose_val_rx', 'form_val_disp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78231d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_prescriptions = df_prescriptions.drop(columns=['subject_id','pharmacy_id','starttime','stoptime','form_rx','poe_id',\n",
    "                                                 'order_provider_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f2c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_prescriptions['formulary_drug_cd'] = df_prescriptions['formulary_drug_cd'].fillna('N/A')\n",
    "df_prescriptions['gsn'] = df_prescriptions['gsn'].fillna('N/A')\n",
    "df_prescriptions['prod_strength'] = df_prescriptions['prod_strength'].fillna('N/A')\n",
    "df_prescriptions['route'] = df_prescriptions['route'].fillna('N/A')\n",
    "\n",
    "# Impute with 0\n",
    "df_prescriptions['ndc'] = df_prescriptions['ndc'].fillna(0)\n",
    "\n",
    "df_prescriptions = pd.get_dummies(df_prescriptions, columns=['formulary_drug_cd','gsn','prod_strength',\n",
    "                                                            'route','drug_type','drug','dose_unit_rx','form_unit_disp',\n",
    "                                                            'dose_val_rx','form_val_disp','ndc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bdd53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions['doses_per_24_hrs'] = df_prescriptions['doses_per_24_hrs'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with null values \n",
    "df_prescriptions = df_prescriptions.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prescriptions = df_prescriptions.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_prescriptions = df_prescriptions.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8982f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_prescriptions.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_prescriptions['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7374ace",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].astype(str)\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462390e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 4890 to 2874 or less\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 2874\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(data)\n",
    "data = svd.transform(data)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438d712",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_prescriptions = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_prescriptions.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520484b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_prescriptions.joblib')\n",
    "dump(random_forest_prescriptions, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7d1a6",
   "metadata": {},
   "source": [
    "### procedures_icd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75eed6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/procedures_icd.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_procedures = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bf5e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_procedures_training = df_procedures[~df_procedures['subject_id'].isin(evaluation_patients)]\n",
    "# df_procedures_evaluation = df_procedures[df_procedures['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_procedures_training = df_procedures[~df_procedures['hadm_id'].isin(evaluation_admissions)]\n",
    "df_procedures_evaluation = df_procedures[df_procedures['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc0db373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_procedures_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_procedures_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248aa4a",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58afce04",
   "metadata": {},
   "source": [
    "Drop: subject_id, chartdate\n",
    "Encode: icd_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b85427f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedures = df_procedures_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "01252d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedures = df_procedures.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1eb90249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a feature called days_since_admission of chartdate - admitdate\n",
    "\n",
    "# Convert to datetime\n",
    "df_procedures['chartdate'] = pd.to_datetime(df_procedures['chartdate'], format='%Y-%m-%d')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_procedures = df_procedures.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "df_procedures['admittime'] = df_procedures['admittime'].dt.date\n",
    "df_procedures['chartdate'] = df_procedures['chartdate'].dt.date\n",
    "\n",
    "df_procedures['days_since_admission'] = df_procedures['chartdate'] - df_procedures['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_procedures['days_since_admission'] = df_procedures['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_procedures = df_procedures.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ad3738b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_procedures = df_procedures.drop(columns=['subject_id','chartdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "68de0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_procedures = pd.get_dummies(df_procedures, columns=['icd_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e10adea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedures = df_procedures.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_procedures = df_procedures.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "94e43364",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_procedures.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_procedures['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5c4f52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'procedures_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac941b",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8d0d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].astype(str)\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].str.split().str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cc79f136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explained Variance Ratio:\n",
      "[3.45480616e-01 4.44957161e-01 1.93438956e-01 5.16315510e-04\n",
      " 4.92956540e-04 3.96487536e-04 3.67158021e-04 3.36794019e-04\n",
      " 2.83744621e-04 2.58429181e-04 2.52220606e-04 2.29863113e-04\n",
      " 2.29726367e-04 2.19606609e-04 2.01159644e-04 2.01137042e-04\n",
      " 1.93402232e-04 1.84791418e-04 1.72406978e-04 1.71482944e-04\n",
      " 1.69952813e-04 1.60400245e-04 1.43677212e-04 1.43675452e-04\n",
      " 1.43076186e-04 1.42911018e-04 1.34616158e-04 1.14941904e-04\n",
      " 1.14942002e-04 1.14941473e-04 1.14934627e-04 1.14941897e-04\n",
      " 1.13820429e-04 1.13612439e-04 1.04825486e-04 8.62057271e-05\n",
      " 8.62056158e-05 8.62055575e-05 8.62052188e-05 8.62025816e-05\n",
      " 8.62052606e-05 8.62051451e-05 8.62051161e-05 8.62017107e-05\n",
      " 8.62043598e-05 8.41369440e-05 8.08357877e-05 7.47934610e-05\n",
      " 5.74680466e-05 5.74678085e-05 5.74677056e-05 5.74670899e-05\n",
      " 5.74668777e-05 5.74664468e-05 5.74657617e-05 5.74643990e-05\n",
      " 5.74647151e-05 5.74639142e-05 5.74606080e-05 5.74635130e-05\n",
      " 5.74623926e-05 5.74622919e-05 5.74593345e-05 5.74590134e-05\n",
      " 5.74568373e-05 5.74559698e-05 5.74559903e-05 5.74560441e-05\n",
      " 5.74550829e-05 5.74542259e-05 5.74530768e-05 5.74519735e-05\n",
      " 5.74502051e-05 5.74481354e-05 5.74473362e-05 5.74460797e-05\n",
      " 5.74405526e-05 5.74384753e-05 5.74343060e-05 5.74324708e-05\n",
      " 5.74275116e-05 5.74234170e-05 5.74180856e-05 5.74143970e-05\n",
      " 5.74099039e-05 5.74048640e-05 5.73779795e-05 5.73683914e-05\n",
      " 5.73594289e-05 5.73433525e-05 5.73117554e-05 5.72993125e-05\n",
      " 5.72748605e-05 5.70682780e-05 5.27378859e-05 4.91001107e-05\n",
      " 4.48872264e-05 2.87354952e-05 2.87354573e-05 2.87352385e-05\n",
      " 2.87354998e-05 2.87355450e-05 2.87354095e-05 2.87355243e-05\n",
      " 2.87355467e-05 2.87350691e-05 2.87354854e-05 2.87350994e-05\n",
      " 2.87354671e-05 2.87355013e-05 2.87353851e-05 2.87355399e-05\n",
      " 2.87350450e-05 2.87353935e-05 2.87352436e-05]\n",
      "\n",
      " Amount of original variance conserved: 0.9949369678618735\n"
     ]
    }
   ],
   "source": [
    "# Need to reduce from 355 to 115\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 115\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(data)\n",
    "data = svd.transform(data)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e236be",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1b29750f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_procedures = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_procedures.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "72dc2226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOS_RF_learners\\\\random_forest_procedures.joblib']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_procedures.joblib')\n",
    "dump(random_forest_procedures, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966d4f4",
   "metadata": {},
   "source": [
    "### services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82e2f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/services.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_services = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa6ecc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_services_training = df_services[~df_services['subject_id'].isin(evaluation_patients)]\n",
    "df_services_evaluation = df_services[df_services['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7594786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_services_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_services_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f305023",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82967003",
   "metadata": {},
   "source": [
    "Drop: subject_id, transfertime\n",
    "Impute with N/A and encode: prev_service\n",
    "Encode: curr_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed000cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_services = df_services_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_services = df_services.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d987a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature called days_since_admission using transfertime-admittime \n",
    "\n",
    "# Convert to datetime\n",
    "df_services['transfertime'] = pd.to_datetime(df_services['transfertime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_services = df_services.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "df_services['days_since_admission'] = df_services['transfertime'] - df_services['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_services['days_since_admission'] = df_services['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_services = df_services.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_services = df_services.drop(columns=['subject_id','transfertime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bdbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_services['prev_service'] = df_services['prev_service'].fillna('N/A')\n",
    "df_services = pd.get_dummies(df_services, columns=['prev_service','curr_service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_services = df_services.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_services = df_services.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_services.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_services['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf61ea55",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_services = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_services.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430e885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_services.joblib')\n",
    "dump(random_forest_services, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a37716",
   "metadata": {},
   "source": [
    "### transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52e01837",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"hosp/transfers.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_transfers = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f38629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_transfers_training = df_transfers[~df_transfers['subject_id'].isin(evaluation_patients)]\n",
    "# df_transfers_evaluation = df_transfers[df_transfers['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_transfers_training = df_transfers[~df_transfers['hadm_id'].isin(evaluation_admissions)]\n",
    "df_transfers_evaluation = df_transfers[df_transfers['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d86b805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_transfers_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_transfers_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16280115",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d29fff",
   "metadata": {},
   "source": [
    "Drop: subject_id, transfer_id, intime, outtime\n",
    "Encode: eventtype\n",
    "Impute with N/A and encode: careunit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "56a79ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers = df_transfers_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "daa6a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers = df_transfers.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "abdb5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a days_since_admission feature of intime-admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_transfers['intime'] = pd.to_datetime(df_transfers['intime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_transfers = df_transfers.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "df_transfers['days_since_admission'] = df_transfers['intime'] - df_transfers['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_transfers['days_since_admission'] = df_transfers['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_transfers = df_transfers.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dedfed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature of outtime-intime \n",
    "\n",
    "# Convert to datetime\n",
    "df_transfers['outtime'] = pd.to_datetime(df_transfers['outtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_transfers['duration'] = df_transfers['outtime'] - df_transfers['intime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_transfers['duration'] = df_transfers['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "058e3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_transfers = df_transfers.drop(columns=['subject_id','transfer_id','intime','outtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2424f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_transfers['careunit'] = df_transfers['careunit'].fillna('N/A')\n",
    "df_transfers = pd.get_dummies(df_transfers, columns=['eventtype','careunit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a4a8636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers = df_transfers.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_transfers = df_transfers.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "da9d2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_transfers.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_transfers['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "84fb4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'transfers_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87910e79",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ff535463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "439c2b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_transfers = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_transfers.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2e624fb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOS_RF_learners\\\\random_forest_transfers.joblib']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_transfers.joblib')\n",
    "dump(random_forest_transfers, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215facdf",
   "metadata": {},
   "source": [
    "### icustays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0cab142",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/icustays.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_icustays = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ab7f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_icustays_training = df_icustays[~df_icustays['subject_id'].isin(evaluation_patients)]\n",
    "df_icustays_evaluation = df_icustays[df_icustays['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb3ddd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_icustays_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_icustays_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d62b3",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22069b4a",
   "metadata": {},
   "source": [
    "Drop: subject_id, stay_id, intime, outtime\n",
    "Encode: first_careunit, last_careunit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icustays = df_icustays_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icustays = df_icustays.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ed085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a feature called days_since_admission using intime-admittime\n",
    "\n",
    "# Convert to datetime\n",
    "df_icustays['intime'] = pd.to_datetime(df_icustays['intime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_icustays = df_icustays.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "# # Discard the time part and keep only the date\n",
    "# df_hcpcsevents['admittime'] = df_hcpcsevents['admittime'].dt.date\n",
    "# df_hcpcsevents['chartdate'] = df_hcpcsevents['chartdate'].dt.date\n",
    "\n",
    "df_icustays['days_since_admission'] = df_icustays['intime'] - df_icustays['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_icustays['days_since_admission'] = df_icustays['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_icustays = df_icustays.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_icustays = df_icustays.drop(columns=['subject_id','stay_id','intime','outtime'])\n",
    "\n",
    "# Rename los to icu_los\n",
    "df_icustays = df_icustays.rename(columns={'los': 'icu_los'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "df_icustays = pd.get_dummies(df_icustays, columns=['first_careunit','last_careunit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icustays = df_icustays.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_icustays = df_icustays.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_services.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_services['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc53cd",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140aa9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['days_since_admission'] = data['days_since_admission'].astype(str)\n",
    "data['days_since_admission'] = data['days_since_admission'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abddf418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_icustays = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_icustays.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429eec1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_icustays.joblib')\n",
    "dump(random_forest_icustays, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c99644",
   "metadata": {},
   "source": [
    "### ingredientevents - Still to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7f42078",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/ingredientevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_ingredient = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "267e773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_ingredient_training = df_ingredient[~df_ingredient['subject_id'].isin(evaluation_patients)]\n",
    "df_ingredient_evaluation = df_ingredient[df_ingredient['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f59cba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_ingredient_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_ingredient_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91d7a9",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e301ea",
   "metadata": {},
   "source": [
    "Drop: subject_id, starttime, endtime, storetime, orderid, originalamount, stay_id, caregiver_id\n",
    "Encode: amountuom, statusdescription, itemid\n",
    "Impute with 0: rate\n",
    "Impute with N/A and encode: rateuom, linkorderid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingredient = df_ingredient_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingredient = df_ingredient.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b85827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature of endtime-starttime \n",
    "\n",
    "# Convert to datetime\n",
    "df_ingredient['endtime'] = pd.to_datetime(df_ingredient['endtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_ingredient['starttime'] = pd.to_datetime(df_ingredient['starttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "df_ingredient['duration'] = df_ingredient['endtime'] - df_ingredient['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_ingredient['duration'] = df_ingredient['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee26fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a recording_delay feature of storetime-endtime\n",
    "\n",
    "# Convert to datetime\n",
    "df_ingredient['storetime'] = pd.to_datetime(df_ingredient['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_ingredient['recording_delay'] = df_ingredient['storetime'] - df_ingredient['endtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_ingredient['recording_delay'] = df_ingredient['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_ingredient = df_ingredient.drop(columns=['subject_id','starttime','endtime','storetime','orderid','originalamount',\n",
    "                                           'stay_id','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_ingredient['rateuom'] = df_ingredient['rateuom'].fillna('N/A')\n",
    "df_ingredient['linkorderid'] = df_ingredient['linkorderid'].fillna('N/A')\n",
    "df_ingredient = pd.get_dummies(df_ingredient, columns=['rateuom','amountuom','statusdescription','itemid','linkorderid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb156141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0\n",
    "df_ingredient['rate'] = df_ingredient['rate'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfcd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingredient = df_ingredient.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_ingredient = df_ingredient.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6565a4",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75729266",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_ingredient.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_ingredient['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87218c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reduce from 7727 to 4116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)\n",
    "data['recording_delay']= data['recording_delay'].astype(str)\n",
    "data['recording_delay']= data['recording_delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of desired features (components)\n",
    "n_components = 4116\n",
    "\n",
    "# Initialize Truncated SVD with the desired number of components\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Fit the Truncated SVD model to the sparse matrix and transform the data\n",
    "svd.fit(data)\n",
    "data = svd.transform(data)\n",
    "\n",
    "# Get the explained variance ratio (how much variance is explained by each component)\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "# Print the transformed matrix and explained variance ratio\n",
    "# print(\"Transformed Matrix:\")\n",
    "# print(transformed_matrix)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n Amount of original variance conserved:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122a57d",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_ingredient = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_ingredient.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4cba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_ingredient.joblib')\n",
    "dump(random_forest_ingredient, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b2532",
   "metadata": {},
   "source": [
    "### inputevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6631681",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/inputevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_input = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5200f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_input_training = df_input[~df_input['subject_id'].isin(evaluation_patients)]\n",
    "# df_input_evaluation = df_input[df_input['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_input_training = df_input[~df_input['hadm_id'].isin(evaluation_admissions)]\n",
    "df_input_evaluation = df_input[df_input['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad12230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_input_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_input_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febce11",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426b8bd",
   "metadata": {},
   "source": [
    "Drop: subject_id, starttime, endtime, storetime, orderid, linkorderid, continueinnextdept, stay_id, caregiver_id,\n",
    "totalamountuom\n",
    "Encode: amountuom, ordercategoryname, ordercomponenttypedescription, ordercategorydescription, statusdescription, itemid\n",
    "Impute with 0: rate, totalamount\n",
    "Impute with N/A and encode: rateuom, secondaryordercategoryname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "49c63e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_input_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ec97da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_input.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "98793062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature using endtime-starttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_input['endtime'] = pd.to_datetime(df_input['endtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_input['starttime'] = pd.to_datetime(df_input['starttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "df_input['duration'] = df_input['endtime'] - df_input['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_input['duration'] = df_input['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bc5ac4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a recording_delay feature using storetime-endtime\n",
    "\n",
    "# Convert to datetime\n",
    "df_input['storetime'] = pd.to_datetime(df_input['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_input['recording_delay'] = df_input['storetime'] - df_input['endtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_input['recording_delay'] = df_input['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "67e2e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_input = df_input.drop(columns=['subject_id','stay_id','starttime','endtime','storetime','orderid','linkorderid',\n",
    "                                  'continueinnextdept','totalamountuom', 'stay_id','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cc5d6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_input['rateuom'] = df_input['rateuom'].fillna('N/A')\n",
    "df_input['secondaryordercategoryname'] = df_input['secondaryordercategoryname'].fillna('N/A')\n",
    "df_input = pd.get_dummies(df_input, columns=['rateuom','secondaryordercategoryname','amountuom','ordercategoryname',\n",
    "                                            'ordercomponenttypedescription','ordercategorydescription','statusdescription',\n",
    "                                            'itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "53013438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with 0\n",
    "df_input['rate'] = df_input['rate'].fillna(0)\n",
    "df_input['totalamount'] = df_input['totalamount'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "73349a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_input.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e10b8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_input.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_input = df_input.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cf1fa65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_input.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_input['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ec2bda11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)\n",
    "data['recording_delay']= data['recording_delay'].astype(str)\n",
    "data['recording_delay']= data['recording_delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2ecc2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'input_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fcf61",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d6549e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_input = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_input.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ac47c68b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOS_RF_learners\\\\random_forest_input.joblib']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_input.joblib')\n",
    "dump(random_forest_input, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8239ec",
   "metadata": {},
   "source": [
    "### outputevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f6626d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/outputevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_output = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35ddceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "df_output_training = df_output[~df_output['subject_id'].isin(evaluation_patients)]\n",
    "df_output_evaluation = df_output[df_output['subject_id'].isin(evaluation_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75d7c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_output_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_output_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b3253",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ffeddc",
   "metadata": {},
   "source": [
    "Drop: subject_id, charttime, storetime, valueuom, stay_id, caregiver_id'\n",
    "Encode: itemid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a days_since_admission feature using charttime-admittime \n",
    "\n",
    "# Convert to datetime\n",
    "df_output['charttime'] = pd.to_datetime(df_output['charttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add admittime column from other dataframe\n",
    "df_output = df_output.merge(df_admittime, on='hadm_id', how='left')\n",
    "\n",
    "df_output['days_since_admission'] = df_output['charttime'] - df_output['admittime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_output['days_since_admission'] = df_output['days_since_admission'].fillna(pd.Timedelta(0))\n",
    "\n",
    "# Drop the admission time column\n",
    "df_output = df_output.drop(columns=['admittime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d889bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a recording_delay feature using storetime-charttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_output['storetime'] = pd.to_datetime(df_output['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_output['recording_delay'] = df_output['storetime'] - df_output['charttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_output['recording_delay'] = df_output['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_output = df_output.drop(columns=['subject_id','stay_id','charttime','storetime','storetime','valueuom','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bcbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode\n",
    "df_output = pd.get_dummies(df_output, columns=['itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_output = df_output.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_ingredient.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_ingredient['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d138565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['recording_delay']= data['recording_delay'].astype(str)\n",
    "data['recording_delay']= data['recording_delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499cdc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4c8c2",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_output = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_output.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ebbb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_output.joblib')\n",
    "dump(random_forest_output, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099792e",
   "metadata": {},
   "source": [
    "### procedureevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "78dfe54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"icu/procedureevents.csv\"\n",
    "full_path = path + file\n",
    "\n",
    "df_procedure_events = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e62e84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and evaluation sets \n",
    "\n",
    "# df_procedure_events_training = df_procedure_events[~df_procedure_events['subject_id'].isin(evaluation_patients)]\n",
    "# df_procedure_events_evaluation = df_procedure_events[df_procedure_events['subject_id'].isin(evaluation_patients)]\n",
    "\n",
    "df_procedure_events_training = df_procedure_events[~df_procedure_events['hadm_id'].isin(evaluation_admissions)]\n",
    "df_procedure_events_evaluation = df_procedure_events[df_procedure_events['hadm_id'].isin(evaluation_admissions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "19d8c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data for later \n",
    "folder_name = 'EnsembleEvaluationData'\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_name, 'df_procedure_events_evaluation.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "df_procedure_events_evaluation.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d0385d",
   "metadata": {},
   "source": [
    "#### Preprocessing (on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69aa1a",
   "metadata": {},
   "source": [
    "Drop: subject_id, starttime, endtime, storetime, orderid, linkorderid, continueinnextdept, stay_id, caregiver_id\n",
    "Encode: valueuom, ordercategoryname, ordercategorydescription, statusdescription, itemid\n",
    "Impute with N/A and encode: location, locationcategory\n",
    "MAKE DURATION FEATURE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "be7cd06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedure_events = df_procedure_events_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "51a5b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedure_events = df_procedure_events.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "98926c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a duration feature using endtime-starttime\n",
    "\n",
    "# Convert to datetime\n",
    "df_procedure_events['endtime'] = pd.to_datetime(df_procedure_events['endtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_procedure_events['starttime'] = pd.to_datetime(df_procedure_events['starttime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "df_procedure_events['duration'] = df_procedure_events['endtime'] - df_procedure_events['starttime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_procedure_events['duration'] = df_procedure_events['duration'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "73b4716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a recording_delay feature using storetime-endtime\n",
    "\n",
    "# Convert to datetime\n",
    "df_procedure_events['storetime'] = pd.to_datetime(df_procedure_events['storetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_procedure_events['recording_delay'] = df_procedure_events['storetime'] - df_procedure_events['endtime']\n",
    "\n",
    "# Fill any non time values\n",
    "df_procedure_events['recording_delay'] = df_procedure_events['recording_delay'].fillna(pd.Timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "107363f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \n",
    "df_procedure_events = df_procedure_events.drop(columns=['subject_id','stay_id','starttime','endtime','storetime','orderid',\n",
    "                                                        'linkorderid','continueinnextdept','caregiver_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6d07e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with N/A and encode\n",
    "df_procedure_events['location'] = df_procedure_events['location'].fillna('N/A')\n",
    "df_procedure_events['locationcategory'] = df_procedure_events['locationcategory'].fillna('N/A')\n",
    "df_procedure_events = pd.get_dummies(df_procedure_events, columns=['location','locationcategory','valueuom',\n",
    "                                                                   'ordercategoryname','ordercategorydescription',\n",
    "                                                                   'statusdescription','itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "94a09bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procedure_events = df_procedure_events.merge(df_los_hadm, on='hadm_id', how='left')\n",
    "df_procedure_events = df_procedure_events.drop(columns=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2de4d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_procedure_events.drop(columns=['los'])\n",
    "target = pd.DataFrame(df_procedure_events['los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "42bf804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration strings to floats\n",
    "\n",
    "target['los'] = target['los'].astype(str)\n",
    "target.fillna(0, inplace=True)\n",
    "target.loc[~target['los'].str.contains('days'), 'los'] = '0 days 00:00:00'\n",
    "target['los'] = target['los'].apply(convert_to_days)\n",
    "data['duration']= data['duration'].astype(str)\n",
    "data['duration']= data['duration'].apply(convert_to_days)\n",
    "data['recording_delay']= data['recording_delay'].astype(str)\n",
    "data['recording_delay']= data['recording_delay'].apply(convert_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9b9ea6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Convert the array to a DataFrame\n",
    "df_column_names = pd.DataFrame(column_names, columns=['Column Names'])\n",
    "\n",
    "output_folder = 'LOS_RF_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "file_path = os.path.join(output_folder, 'procedure_events_features.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_column_names.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca876a",
   "metadata": {},
   "source": [
    "#### Training the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4eeebdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "\n",
    "# Convert DataFrame to 1D array using ravel()\n",
    "target = target.values.ravel()\n",
    "\n",
    "# Initialize and fit the Random Forest regression model\n",
    "random_forest_procedure_events = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "random_forest_procedure_events.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b8dca13c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOS_RF_learners\\\\random_forest_procedure_events.joblib']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to folder\n",
    "\n",
    "output_folder = 'LOS_RF_learners'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the trained model to a file in the new folder\n",
    "model_file = os.path.join(output_folder, 'random_forest_procedure_events.joblib')\n",
    "dump(random_forest_procedure_events, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
